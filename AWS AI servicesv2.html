<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AWS AI Services Interactive Map</title>
	<script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* General Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Inter', sans-serif; /* Using Inter font */
            border-radius: 8px; /* Applying rounded corners to all elements */
        }

        /* Body Styles */
        body {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            min-height: 100vh;
            padding: 20px;
            color: white;
            display: flex;
            flex-direction: column;
            gap: 30px;
            align-items: center;
        }

        /* Container for overall layout */
        .container {
            max-width: 1400px;
            margin: 0 auto;
            width: 100%;
        }

        /* Main Heading */
        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.8rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            color: #ffd700;
        }

        /* Mindmap Layout (for AWS specific content) */
        .mindmap {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 30px;
            width: 100%; /* Ensure it takes full width of container */
        }

        /* Central Node (AWS AI & ML Services) */
        .central-node {
            background: linear-gradient(45deg, #ff6b35, #f7931e);
            padding: 20px 40px;
            border-radius: 50px;
            font-size: 1.5rem;
            font-weight: bold;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            cursor: pointer;
            transform: scale(1);
            transition: all 0.3s ease;
        }

        .central-node:hover {
            transform: scale(1.05);
            box-shadow: 0 15px 40px rgba(0,0,0,0.4);
        }

        /* Main Branches Grid Layout */
        .main-branches {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            width: 100%;
        }

        /* Individual Branch Styles */
        .branch {
            background: rgba(255,255,255,0.1);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255,255,255,0.2);
            padding: 25px;
            transition: all 0.3s ease;
            cursor: pointer;
        }

        .branch:hover {
            transform: translateY(-5px);
            background: rgba(255,255,255,0.15);
            box-shadow: 0 15px 40px rgba(0,0,0,0.2);
        }

        .branch-header {
            font-size: 1.3rem;
            font-weight: bold;
            margin-bottom: 15px;
            color: #ffd700;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .branch-icon {
            font-size: 1.5rem;
        }

        /* Sub-services (initially hidden) */
        .sub-services {
            display: none;
            animation: fadeIn 0.3s ease;
        }

        .sub-services.active {
            display: block;
        }

        /* Fade-in animation for sub-services */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Individual Service Item Styles */
        .service-item {
            background: rgba(0,0,0,0.2);
            margin: 10px 0;
            padding: 15px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .service-item:hover {
            background: rgba(0,0,0,0.3);
            transform: translateX(10px);
        }

        .service-name {
            font-weight: bold;
            color: #87ceeb;
            margin-bottom: 5px;
        }

        .service-desc {
            font-size: 0.9rem;
            color: #e0e0e0;
            margin-bottom: 10px;
        }

        .features {
            display: block;
            margin-top: 10px;
            padding: 10px;
            background: rgba(0,0,0,0.3);
        }

        /* Feature Tag Styles */
        .feature-tag {
            display: inline-block;
            background: #4CAF50;
            color: white;
            padding: 3px 8px;
            margin: 2px;
            font-size: 0.8rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .feature-tag:hover {
            background: #45a049;
            transform: scale(1.05);
        }

        /* Feature Details (initially hidden, expands on click) */
        .feature-details {
            display: none;
            margin-top: 15px;
            padding: 15px;
            background: rgba(0,0,0,0.4);
            border-left: 4px solid #4CAF50;
        }

        .feature-details.active {
            display: block;
            animation: slideDown 0.3s ease;
        }

        /* Slide-down animation for feature details */
        @keyframes slideDown {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Sub-feature styles within details */
        .sub-feature {
            background: rgba(255,255,255,0.1);
            margin: 8px 0;
            padding: 10px;
            border-left: 3px solid #87ceeb;
        }

        .sub-feature-title {
            font-weight: bold;
            color: #87ceeb;
            margin-bottom: 5px;
        }

        .sub-feature-desc {
            font-size: 0.9rem;
            color: #e0e0e0;
            line-height: 1.4;
        }

        .feature-nav {
            text-align: center;
            margin: 15px 0;
            font-size: 0.9rem;
            color: #ffd700;
        }

        .use-cases {
            margin-top: 10px;
            font-size: 0.85rem;
            color: #ffd700;
        }

        /* Modal Overlay */
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.8);
            animation: fadeIn 0.3s ease;
            overflow-y: auto; /* Allow scrolling for modal content */
        }

        /* Modal Content Box */
        .modal-content {
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            margin: 5% auto;
            padding: 30px;
            width: 90%;
            max-width: 800px;
            max-height: 90vh; /* Max height to prevent overflow */
            overflow-y: auto; /* Ensure scrollability within modal */
            box-shadow: 0 20px 60px rgba(0,0,0,0.5);
        }

        /* Close Button for Modal */
        .close {
            color: #aaa;
            float: right;
            font-size: 28px;
            font-weight: bold;
            cursor: pointer;
        }

        .close:hover {
            color: white;
        }

        /* Sections within the modal */
        .tech-stack, .interdependencies, .config-nav {
            background: rgba(0,0,0,0.2);
            padding: 15px;
            margin: 15px 0;
        }

        .pricing-info {
            background: rgba(52, 152, 219, 0.2);
            padding: 10px;
            margin: 10px 0;
            font-size: 0.9rem;
        }

        .scenario-box {
            background: rgba(46, 204, 113, 0.2);
            padding: 15px;
            margin: 15px 0;
        }
        
        .config-options {
            list-style-type: disc;
            margin-left: 20px;
            padding-left: 0;
        }

        .config-options li {
            margin-bottom: 5px;
        }

        .llm-action-button {
            background-color: #f7931e;
            color: white;
            border: none;
            padding: 8px 12px;
            cursor: pointer;
            font-size: 0.9rem;
            margin-top: 10px;
            margin-right: 10px;
            transition: background-color 0.3s ease;
        }

        .llm-action-button:hover {
            background-color: #ff6b35;
        }

        .llm-output-area {
            background: rgba(0,0,0,0.3);
            padding: 15px;
            margin-top: 20px;
            border: 1px solid rgba(255,255,255,0.2);
            min-height: 50px;
            font-size: 0.95rem;
            line-height: 1.5;
            white-space: pre-wrap;
            word-wrap: break-word;
            display: none; /* Hidden by default */
        }

        .loading-indicator {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255,255,255,0.3);
            border-radius: 50%;
            border-top-color: #ffd700;
            animation: spin 1s ease-in-out infinite;
            -webkit-animation: spin 1s ease-in-out infinite;
            margin-left: 10px;
            vertical-align: middle;
        }

        @keyframes spin {
            to { -webkit-transform: rotate(360deg); }
        }
        @-webkit-animation {
            to { -webkit-transform: rotate(360deg); }
        }

        /* News and Search Sections */
        .news-section { /* Removed search-section styling from here */
            background: rgba(255,255,255,0.1);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255,255,255,0.2);
            padding: 25px;
            color: white;
            width: 100%; /* Take full width of parent container */
            margin-top: 30px; /* Spacing below the main mindmap branches */
        }

        .news-section h2 {
            font-size: 1.8rem;
            font-weight: bold;
            margin-bottom: 20px;
            color: #ffd700;
            text-align: center;
        }

        .news-item {
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid rgba(255,255,255,0.1);
        }

        .news-item:last-child {
            border-bottom: none;
        }

        .news-item h3 {
            font-size: 1.2rem;
            color: #87ceeb;
            margin-bottom: 5px;
        }

        .news-item p {
            font-size: 0.9rem;
            color: #e0e0e0;
        }

        .news-item a {
            color: #4CAF50;
            text-decoration: underline;
            font-size: 0.85rem;
        }

        /* Floating Search Bar Styles */
        .floating-search-container {
            position: fixed; /* Keep it fixed for floating */
            bottom: 20px;
            right: 20px;
            z-index: 1001;
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            padding: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.5);
            transition: all 0.3s ease-in-out;
            opacity: 0.6; /* Transparent when not expanded */
            width: 60px; /* Small default width for the button */
            height: 60px; /* Small default height for the button */
            display: flex;
            justify-content: center;
            align-items: center;
            border: 1px solid rgba(255,255,255,0.2);
            cursor: grab; /* Indicate it's draggable */
        }

        .floating-search-container:hover {
            opacity: 1; /* Fully opaque on hover */
        }

        .floating-search-container.expanded {
            width: 350px; /* Expanded width */
            height: auto; /* Auto height for content */
            opacity: 1; /* Fully opaque when expanded */
            flex-direction: column;
            align-items: flex-start;
            padding: 20px;
            cursor: auto; /* Change cursor back when expanded */
        }

        .search-toggle-button {
            background-color: #f7931e;
            color: white;
            border: none;
            width: 40px; /* Icon size */
            height: 40px; /* Icon size */
            font-size: 1.5rem;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }

        .search-toggle-button:hover {
            background-color: #ff6b35;
        }

        .floating-search-content {
            display: none; /* Hidden by default */
            width: 100%;
        }

        .floating-search-container.expanded .floating-search-content {
            display: block; /* Visible when expanded */
        }

        .floating-search-content .search-bar-container {
            display: flex;
            gap: 10px;
            margin-bottom: 10px;
            flex-direction: column; /* Stack input and button vertically on mobile */
        }
        
        .floating-search-content .search-bar-container input[type="text"] {
            flex-grow: 1;
            padding: 10px;
            border: 1px solid rgba(255,255,255,0.3);
            background-color: rgba(0,0,0,0.3);
            color: white;
            font-size: 1rem;
            width: 100%; /* Full width within expanded container */
        }

        .floating-search-content .search-bar-container button {
            padding: 10px 20px;
            background-color: #4CAF50;
            color: white;
            border: none;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s ease;
            width: 100%; /* Full width within expanded container */
        }

        .floating-search-content .search-bar-container button:hover {
            background-color: #45a049;
        }

        /* Close button for expanded search */
        .floating-search-close {
            color: #aaa;
            align-self: flex-end; /* Position at top right within flex container */
            font-size: 24px;
            font-weight: bold;
            cursor: pointer;
            margin-bottom: 10px;
            display: none; /* Hidden by default, shown when expanded */
        }

        .floating-search-container.expanded .floating-search-close {
            display: block;
        }

        .floating-search-close:hover {
            color: white;
        }

        /* Responsive Adjustments */
        @media (max-width: 768px) {
            .main-branches {
                grid-template-columns: 1fr;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            .modal-content {
                width: 95%;
                margin: 10% auto;
                padding: 20px;
            }
            .floating-search-container.expanded {
                width: 90%; /* Larger on small screens */
                right: 5%;
                bottom: 10%;
            }
            .floating-search-content .search-bar-container {
                flex-direction: column;
            }
            .news-section {
                padding: 15px; /* Reduce padding on smaller screens */
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ AWS AI Services </h1>
	
<!-- Back to Landing Page Button -->
<a href="./index.html" class="fixed top-4 left-4 z-50 bg-emerald-700 text-white py-2 px-4 rounded-lg shadow-lg hover:bg-emerald-800 transition duration-300 ease-in-out text-lg font-bold">
    &larr; Back to Hub
</a>
	
        <div class="mindmap">
            <div class="central-node">
                AWS AI & ML Services
            </div>
	
            <div class="main-branches">
                <div class="branch" onclick="toggleBranch('ml-platforms')">
                    <div class="branch-header">
                        <span class="branch-icon">üèóÔ∏è</span>
                        ML Platforms & Development
                    </div>
                    <div id="ml-platforms" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('sagemaker')">
                            <div class="service-name">Amazon SageMaker</div>
                            <div class="service-desc">Complete ML development platform</div>
                            <div class="use-cases">üí° Model building, training, deployment, MLOps</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('bedrock')">
                            <div class="service-name">Amazon Bedrock</div>
                            <div class="service-desc">Generative AI foundation models</div>
                            <div class="use-cases">üí° LLMs, text generation, chatbots</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('codewhisperer')">
                            <div class="service-name">Amazon CodeWhisperer</div>
                            <div class="service-desc">AI-powered code generation</div>
                            <div class="use-cases">üí° Code completion, security scanning</div>
                        </div>
                    </div>
                </div>

                <div class="branch" onclick="toggleBranch('vision-ai')">
                    <div class="branch-header">
                        <span class="branch-icon">üëÅÔ∏è</span>
                        Computer Vision
                    </div>
                    <div id="vision-ai" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('rekognition')">
                            <div class="service-name">Amazon Rekognition</div>
                            <div class="service-desc">Image and video analysis</div>
                            <div class="use-cases">üí° Face detection, object recognition, content moderation</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('textract')">
                            <div class="service-name">Amazon Textract</div>
                            <div class="service-desc">Document text extraction</div>
                            <div class="use-cases">üí° OCR, form processing, document analysis</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('lookout-vision')">
                            <div class="service-name">Amazon Lookout for Vision</div>
                            <div class="service-desc">Industrial defect detection</div>
                            <div class="use-cases">üí° Quality control, manufacturing inspection</div>
                        </div>
                    </div>
                </div>

                <div class="branch" onclick="toggleBranch('language-ai')">
                    <div class="branch-header">
                        <span class="branch-icon">üí¨</span>
                        Language AI
                    </div>
                    <div id="language-ai" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('comprehend')">
                            <div class="service-name">Amazon Comprehend</div>
                            <div class="service-desc">Natural language processing</div>
                            <div class="use-cases">üí° Sentiment analysis, entity extraction, topic modeling</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('translate')">
                            <div class="service-name">Amazon Translate</div>
                            <div class="service-desc">Neural machine translation</div>
                            <div class="use-cases">üí° Real-time translation, document localization</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('lex')">
                            <div class="service-name">Amazon Lex</div>
                            <div class="service-desc">Conversational AI chatbots</div>
                            <div class="use-cases">üí° Virtual assistants, customer service bots</div>
                        </div>
                    </div>
                </div>

                <div class="branch" onclick="toggleBranch('speech-ai')">
                    <div class="branch-header">
                        <span class="branch-icon">üéµ</span>
                        Speech AI
                    </div>
                    <div id="speech-ai" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('polly')">
                            <div class="service-name">Amazon Polly</div>
                            <div class="service-desc">Text-to-speech service</div>
                            <div class="use-cases">üí° Voice assistants, audiobooks, accessibility</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('transcribe')">
                            <div class="service-name">Amazon Transcribe</div>
                            <div class="service-desc">Speech-to-text conversion</div>
                            <div class="use-cases">üí° Meeting transcription, call analytics</div>
                        </div>
                    </div>
                </div>

                <div class="branch" onclick="toggleBranch('business-ai')">
                    <div class="branch-header">
                        <span class="branch-icon">üìä</span>
                        Business Intelligence AI
                    </div>
                    <div id="business-ai" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('forecast')">
                            <div class="service-name">Amazon Forecast</div>
                            <div class="service-desc">Time-series forecasting</div>
                            <div class="use-cases">üí° Demand planning, financial forecasting</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('personalize')">
                            <div class="service-name">Amazon Personalize</div>
                            <div class="service-desc">ML-powered recommendations</div>
                            <div class="use-cases">üí° Product recommendations, content personalization</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('fraud-detector')">
                            <div class="service-name">Amazon Fraud Detector</div>
                            <div class="service-desc">Fraud detection service</div>
                            <div class="use-cases">üí° Payment fraud, account takeover prevention</div>
                        </div>
                    </div>
                </div>

                <div class="branch" onclick="toggleBranch('infrastructure')">
                    <div class="branch-header">
                        <span class="branch-icon">‚ö°</span>
                        AI Infrastructure
                    </div>
                    <div id="infrastructure" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('ec2-ai')">
                            <div class="service-name">EC2 AI Instances</div>
                            <div class="service-desc">GPU/AI-optimized compute</div>
                            <div class="use-cases">üí° P4, G4, Inf1, Trn1 instances for training/inference</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('lambda-ai')">
                            <div class="service-name">Lambda + AI</div>
                            <div class="service-desc">Serverless AI processing</div>
                            <div class="use-cases">üí° Event-driven AI, real-time inference</div>
                        </div>
                    </div>
                </div>

                <div class="branch" onclick="toggleBranch('security')">
                    <div class="branch-header">
                        <span class="branch-icon">üîí</span>
                        AI Security & Governance
                    </div>
                    <div id="security" class="sub-services">
                        <div class="service-item" onclick="showServiceDetails('iam-ai')">
                            <div class="service-name">IAM for AI Services</div>
                            <div class="service-desc">Identity and access management</div>
                            <div class="use-cases">üí° Role-based access, service-linked roles, fine-grained permissions</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('vpc-security')">
                            <div class="service-name">VPC & Network Security</div>
                            <div class="service-desc">Network isolation and protection</div>
                            <div class="use-cases">üí° Private endpoints, security groups, NACLs</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('encryption')">
                            <div class="service-name">Data Encryption</div>
                            <div class="service-desc">End-to-end data protection</div>
                            <div class="use-cases">üí° KMS, encryption at rest/transit, customer managed keys</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('compliance')">
                            <div class="service-name">Compliance & Governance</div>
                            <div class="service-desc">Regulatory compliance framework</div>
                            <div class="use-cases">üí° GDPR, HIPAA, SOC, PCI DSS, audit trails</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('monitoring')">
                            <div class="service-name">Security Monitoring</div>
                            <div class="service-desc">Threat detection and logging</div>
                            <div class="use-cases">üí° CloudTrail, GuardDuty, Security Hub, Config</div>
                        </div>
                        <div class="service-item" onclick="showServiceDetails('data-protection')">
                            <div class="service-name">Data Protection & Privacy</div>
                            <div class="use-cases">üí° Macie, data classification, PII detection</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- New segment for General AI & ML Technologies & Advancements -->
            <div class="branch" onclick="toggleBranch('ai-advancements')">
                <div class="branch-header">
                    <span class="branch-icon">üí°</span> AI & ML Technologies & Advancements
                </div>
                <div class="sub-services" id="ai-advancements">
                    <div class="service-item">
                        <div class="service-name">Agentic AI Models</div>
                        <div class="service-desc">AI models capable of taking independent actions and making real-time decisions, mimicking human-like autonomy.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Self-driving cars navigating roads and obeying traffic rules.</li>
                                <li>Personal assistants managing schedules and tasks autonomously.</li>
                                <li>AI systems that automate complex multi-step research or business processes.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Agentic AI Models', 'AI models capable of taking independent actions and making real-time decisions, mimicking human-like autonomy.', 'ai-advancements-agentic-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-agentic-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Multimodal AI Systems</div>
                        <div class="service-desc">AI systems that process and integrate multiple types of input simultaneously, such as text, images, audio, and video, leading to more intuitive understanding.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Smart refrigerators recognizing food, suggesting recipes, and updating grocery lists.</li>
                                <li>Virtual assistants understanding voice commands, handwritten notes, and responding with summaries.</li>
                                <li>Robots perceiving environments more like humans for smoother collaboration in industries.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Multimodal AI Systems', 'AI systems that process and integrate multiple types of input simultaneously, such as text, images, audio, and video, leading to more intuitive understanding.', 'ai-advancements-multimodal-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-multimodal-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Generative AI Evolution</div>
                        <div class="service-desc">Advancements in models like LLMs (Large Language Models), Text-to-Image, and Text-to-Video, enabling smarter, faster, and more ethical content creation across various modalities.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Generating high-quality text, images, music, and complex simulations.</li>
                                <li>Automating content creation for marketing, entertainment, and educational industries.</li>
                                <li>AI-powered code generation for developers.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Generative AI Evolution', 'Advancements in models like LLMs (Large Language Models), Text-to-Image, and Text-to-Video, enabling smarter, faster, and more ethical content creation across various modalities.', 'ai-advancements-generative-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-generative-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Privacy-Preserving AI</div>
                        <div class="service-desc">Development of AI tools and techniques that prioritize user privacy and data security through methods like on-device processing, federated learning, and differential privacy.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>AI tools running locally on devices without sending sensitive data to the cloud.</li>
                                <li>Collaborative AI learning from data across multiple devices without direct data sharing.</li>
                                <li>Secure analysis of aggregated user data while protecting individual privacy.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Privacy-Preserving AI', 'Development of AI tools and techniques that prioritize user privacy and data security through methods like on-device processing, federated learning, and differential privacy.', 'ai-advancements-privacy-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-privacy-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Quantum Computing & AI</div>
                        <div class="service-desc">The convergence of quantum computing with AI, leveraging quantum mechanics to solve complex computational problems beyond the capabilities of classical computers, accelerating AI research.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Developing more powerful and efficient AI algorithms for drug discovery and material science.</li>
                                <li>Optimizing complex logistics and financial modeling.</li>
                                <li>Breaking advanced encryption methods and enhancing cybersecurity.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Quantum Computing & AI', 'The convergence of quantum computing with AI, leveraging quantum mechanics to solve complex computational problems beyond the capabilities of classical computers, accelerating AI research.', 'ai-advancements-quantum-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-quantum-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Edge AI</div>
                        <div class="service-desc">Deploying AI models directly on edge devices (e.g., IoT devices, smartphones, cameras) to perform inference locally, reducing latency, improving privacy, and conserving bandwidth.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Real-time anomaly detection in industrial IoT sensors.</li>
                                <li>Facial recognition on smart cameras for security without cloud dependency.</li>
                                <li>Instantaneous voice commands processing on smart home devices.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Edge AI', 'Deploying AI models directly on edge devices (e.g., IoT devices, smartphones, cameras) to perform inference locally, reducing latency, improving privacy, and conserving bandwidth.', 'ai-advancements-edge-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-edge-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">AI in Cybersecurity</div>
                        <div class="service-desc">Utilizing AI and ML techniques to enhance threat detection, automate incident response, and predict potential vulnerabilities in complex IT environments.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Real-time detection of malware, phishing attempts, and ransomware attacks.</li>
                                <li>Behavioral analytics to identify deviations indicating compromised accounts.</li>
                                <li>Automated threat containment and remediation.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('AI in Cybersecurity', 'Utilizing AI and ML techniques to enhance threat detection, automate incident response, and predict potential vulnerabilities in complex IT environments.', 'ai-advancements-cybersecurity-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-cybersecurity-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Personalized Medicine</div>
                        <div class="service-desc">Applying AI to healthcare data to tailor medical treatments, predict disease risks, and discover new drugs based on an individual's genetic makeup, lifestyle, and environment.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Predictive analytics for early identification of at-risk patients.</li>
                                <li>Optimizing drug dosages and treatment plans for individual patients.</li>
                                <li>Accelerating drug discovery by simulating molecular interactions.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Personalized Medicine', 'Applying AI to healthcare data to tailor medical treatments, predict disease risks, and discover new drugs based on an individual\'s genetic makeup, lifestyle, and environment.', 'ai-advancements-medicine-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-medicine-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Advanced Robotics & Human-AI Collaboration</div>
                        <div class="service-desc">Development of more intelligent and adaptable robots capable of performing complex tasks autonomously or collaboratively with humans, especially in unstructured environments.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Robots in logistics and manufacturing for automated assembly and quality control.</li>
                                <li>Surgical robots assisting medical professionals with precision and consistency.</li>
                                <li>Human-robot collaboration in hazardous environments or for repetitive tasks.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Advanced Robotics & Human-AI Collaboration', 'Development of more intelligent and adaptable robots capable of performing complex tasks autonomously or collaboratively with humans, especially in unstructured environments.', 'ai-advancements-robotics-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-robotics-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Digital Twins</div>
                        <div class="service-desc">Creation of virtual replicas of physical objects, processes, or systems, powered by AI to simulate real-world behavior, optimize performance, and predict outcomes.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Optimizing factory operations and predictive maintenance for machinery.</li>
                                <li>Simulating urban environments for smart city planning and traffic management.</li>
                                <li>Developing and testing new product designs in a virtual environment.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Digital Twins', 'Creation of virtual replicas of physical objects, processes, or systems, powered by AI to simulate real-world behavior, optimize performance, and predict outcomes.', 'ai-advancements-digital-twins-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-digital-twins-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                    <div class="service-item">
                        <div class="service-name">Responsible AI & AI Ethics</div>
                        <div class="service-desc">Focus on developing AI systems that are fair, transparent, accountable, and safe, addressing biases, ensuring privacy, and establishing governance frameworks for ethical deployment.</div>
                        <div class="use-cases">
                            <p class="use-cases-title">Use Cases:</p>
                            <ul>
                                <li>Bias detection and mitigation in hiring algorithms or loan applications.</li>
                                <li>Ensuring transparency in AI decision-making processes (explainable AI).</li>
                                <li>Establishing regulatory frameworks and industry standards for AI development.</li>
                            </ul>
                        </div>
                        <button class="llm-action-button" onclick="generateTechExplanation('Responsible AI & AI Ethics', 'Focus on developing AI systems that are fair, transparent, accountable, and safe, addressing biases, ensuring privacy, and establishing governance frameworks for ethical deployment.', 'ai-advancements-responsible-ai-llm-output', this)">Explain This Tech ‚ú®</button>
                        <div id="ai-advancements-responsible-ai-llm-output" class="llm-output-area" style="display: none;"></div>
                    </div>
                </div>
            </div>

            <!-- News Letter Section (moved to be within the main content flow) -->
            <div class="news-section">
                <h2>AWS AI & ML News Highlights</h2>
                <ul id="news-list" style="list-style: none; padding: 0;">
                    <!-- News items will be loaded here by JavaScript -->
                    <li><div class="loading-indicator"></div> Loading latest news...</li>
                </ul>
            </div>
        </div>
    </div>

    <!-- Service Details Modal -->
    <div id="serviceModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeModal()">&times;</span>
            <div id="modalContent"></div>
        </div>
    </div>

    <!-- Floating AI Search Bar -->
    <div id="floatingAISearch" class="floating-search-container">
        <button class="search-toggle-button" onclick="toggleFloatingSearch()">
            <span id="searchIcon">üîç</span>
        </button>
        <div class="floating-search-content">
            <span class="floating-search-close" onclick="toggleFloatingSearch()">&times;</span>
            <div class="search-bar-container">
                <input type="text" id="ai-search-input" placeholder="Ask Gemini AI anything about AWS AI services or advancements...">
                <button id="ai-search-button" onclick="performAISearch()">Search with AI ‚ú®</button>
            </div>
            <div id="ai-search-output" class="llm-output-area">
                <!-- Search results will appear here -->
            </div>
        </div>
    </div>

    <script>
        // Data structure containing details for each AWS AI service
        const serviceData = {
            sagemaker: {
                name: "Amazon SageMaker",
                description: "Fully managed machine learning platform for building, training, and deploying ML models at scale",
                features: ["Studio IDE", "AutoML", "Model Registry", "Pipelines", "Endpoints", "Ground Truth", "Clarify", "Feature Store"],
                featureDetails: {
                    "Studio IDE": {
                        subFeatures: [
                            { title: "Jupyter Notebooks", desc: "Fully managed Jupyter environment with pre-configured kernels for popular ML frameworks" },
                            { title: "Code Editor", desc: "VS Code-like interface with Git integration, debugging, and collaborative features" },
                            { title: "Data Wrangler", desc: "Visual interface for data preparation with 300+ built-in transformations" },
                            { title: "Model Debugger", desc: "Real-time monitoring and debugging of training jobs with automatic anomaly detection" }
                        ],
                        navigation: "SageMaker Console -> SageMaker Studio -> Open Studio",
                        options: [
                            "Choose instance types for Notebooks (e.g., ml.t3.medium, ml.m5.xlarge)",
                            "Select pre-built or custom Docker images for kernels",
                            "Configure Git repositories for version control"
                        ]
                    },
                    "AutoML": {
                        subFeatures: [
                            { title: "Autopilot", desc: "Automatically builds, trains, and tunes ML models with full visibility into the process" },
                            { title: "Auto Data Quality", desc: "Automatically detects and suggests fixes for data quality issues" },
                            { title: "Feature Engineering", desc: "Automated feature selection and engineering based on data patterns" },
                            { title: "Algorithm Selection", desc: "Intelligent selection of best algorithms based on data characteristics" }
                        ],
                        navigation: "SageMaker Console -> Autopilot -> Create Autopilot experiment",
                        options: [
                            "Specify target attribute for prediction",
                            "Choose data source (S3, Redshift, etc.)",
                            "Select between 'Ensemble' or 'Explainability' mode for model selection",
                            "Set completion criteria (e.g., max candidates, max runtime)"
                        ]
                    },
                    "Model Registry": {
                        subFeatures: [
                            { title: "Version Control", desc: "Track model versions, metadata, and lineage across the ML lifecycle" },
                            { title: "Model Approval", desc: "Workflow-based model approval process with automated testing gates" },
                            { title: "A/B Testing", desc: "Built-in capabilities for model comparison and champion/challenger testing" },
                            { title: "Model Monitoring", desc: "Continuous monitoring of model performance and data drift detection" }
                        ],
                        navigation: "SageMaker Console -> Model Registry -> Create model package group",
                        options: [
                            "Define model package group properties (name, description)",
                            "Integrate with SageMaker Pipelines for automated registration",
                            "Configure approval statuses (e.g., Pending, Approved, Rejected)",
                            "Set up CloudWatch alarms for model performance metrics"
                        ]
                    },
                    "Pipelines": {
                        subFeatures: [
                            { title: "Workflow Orchestration", desc: "Visual pipeline builder with drag-and-drop interface for ML workflows" },
                            { title: "Step Functions Integration", desc: "Native integration with AWS Step Functions for complex workflow management" },
                            { title: "Conditional Logic", desc: "Support for conditional steps, parallel processing, and error handling" },
                            { title: "Pipeline Templates", desc: "Pre-built templates for common ML use cases and industry patterns" }
                        ],
                        navigation: "SageMaker Console -> SageMaker Pipelines -> Create pipeline",
                        options: [
                            "Define pipeline steps (Processing, Training, Model creation, Register model, Deploy model)",
                            "Configure step dependencies and inputs/outputs",
                            "Specify instance types and resource requirements for each step",
                            "Set up failure handling and retry logic"
                        ]
                    },
                    "Endpoints": {
                        subFeatures: [
                            { title: "Real-time Inference", desc: "Low-latency endpoints for real-time predictions with auto-scaling" },
                            { title: "Batch Transform", desc: "Efficient batch processing for large-scale inference jobs" },
                            { title: "Multi-Model Endpoints", desc: "Host multiple models on a single endpoint for cost optimization" },
                            { title: "Serverless Inference", desc: "Pay-per-request inference without managing infrastructure" }
                        ],
                        navigation: "SageMaker Console -> Inference -> Endpoints -> Create endpoint",
                        options: [
                            "Choose inference type (Real-time, Batch, Serverless, Async)",
                            "Select instance type and count for real-time endpoints",
                            "Configure auto-scaling policies based on utilization or latency",
                            "Specify S3 input/output paths for batch transform jobs"
                        ]
                    },
                    "Ground Truth": {
                        subFeatures: [
                            { title: "Human Labeling", desc: "Managed workforce for high-quality data labeling with built-in quality controls" },
                            { title: "Active Learning", desc: "ML-assisted labeling that reduces manual effort by up to 70%" },
                            { title: "Custom Workflows", desc: "Configurable labeling workflows for complex annotation tasks" },
                            { title: "Quality Assurance", desc: "Multi-level review process with inter-annotator agreement metrics" }
                        ],
                        navigation: "SageMaker Console -> Ground Truth -> Labeling jobs -> Create labeling job",
                        options: [
                            "Choose task type (e.g., Image classification, Object detection, Text classification)",
                            "Select workforce (Private, Vendor, Public/Mechanical Turk)",
                            "Upload data (S3) and specify output location",
                            "Configure labeling instructions and review processes"
                        ]
                    },
                    "Clarify": {
                        subFeatures: [
                            { title: "Bias Detection", desc: "Pre-training and post-training bias detection across multiple fairness metrics" },
                            { title: "Model Explainability", desc: "SHAP-based feature importance and local explanations for individual predictions" },
                            { title: "Fairness Monitoring", desc: "Continuous monitoring of model fairness in production deployments" },
                            { title: "Bias Mitigation", desc: "Recommendations and techniques for reducing identified bias in models" }
                        ],
                        navigation: "SageMaker Console -> SageMaker Clarify -> Bias/Explainability jobs -> Create",
                        options: [
                            "Specify dataset for bias analysis or model for explainability",
                            "Define sensitive attributes (e.g., age, gender)",
                            "Choose fairness metrics (e.g., disparate impact, equal opportunity)",
                            "Select explainability methods (e.g., SHAP, LIME)"
                        ]
                    },
                    "Feature Store": {
                        subFeatures: [
                            { title: "Feature Repository", desc: "Centralized repository for storing, discovering, and sharing ML features" },
                            { title: "Real-time Features", desc: "Low-latency feature serving for real-time ML applications" },
                            { title: "Feature Lineage", desc: "Track feature creation, transformation, and usage across models" },
                            { title: "Time Travel", desc: "Access historical feature values for training and backtesting" }
                        ],
                        navigation: "SageMaker Console -> Feature Store -> Create feature group",
                        options: [
                            "Define feature group schema (feature names, data types)",
                            "Choose online (low-latency) and/or offline (batch) store",
                            "Configure data ingestion source (e.g., S3, Kinesis)",
                            "Set up data retention policies"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: Primary data storage for training data, model artifacts, and inference results.",
                    "**EC2**: SageMaker leverages EC2 instances for training and hosting. Choosing the right instance type (e.g., GPU instances) is crucial for performance.",
                    "**CloudWatch**: For monitoring training jobs, endpoint performance, and MLOps pipelines.",
                    "**IAM**: Essential for defining permissions for SageMaker to access other AWS services and for users to interact with SageMaker.",
                    "**Lambda**: Can be used to trigger SageMaker jobs or process inference results serverlessly.",
                    "**Glue**: For ETL and data preparation before feeding data into SageMaker.",
                    "**Step Functions**: For orchestrating complex, multi-step ML workflows with SageMaker Pipelines."
                ],
                techStack: ["Jupyter Notebooks", "TensorFlow", "PyTorch", "Scikit-learn", "XGBoost", "Hugging Face"],
                pricing: "Pay-as-you-go: Training ($0.05-$24.48/hour), Hosting ($0.05-$24.48/hour), depending on instance type and region. Data labeling, Data Wrangler, Feature Store, and Clarify have separate pricing based on usage.",
                scenarios: [
                    "E-commerce recommendation engine with real-time inference",
                    "Healthcare image classification for medical diagnosis",
                    "Financial fraud detection with automated model retraining",
                    "Supply chain demand forecasting with MLOps pipeline"
                ]
            },
            bedrock: {
                name: "Amazon Bedrock",
                description: "Fully managed service providing access to foundation models from leading AI companies",
                features: ["Claude (Anthropic)", "Titan (Amazon)", "Jurassic (AI21)", "Command (Cohere)", "Stable Diffusion", "Fine-tuning", "RAG"],
                featureDetails: {
                    "Claude (Anthropic)": {
                        subFeatures: [
                            { title: "Claude 3 Opus", desc: "Most capable model for complex reasoning, analysis, and creative tasks" },
                            { title: "Claude 3 Sonnet", desc: "Balanced performance and speed for most business applications" },
                            { title: "Claude 3 Haiku", desc: "Fastest model for high-volume, low-latency use cases" },
                            { title: "Constitutional AI", desc: "Built-in safety measures and alignment with human values" }
                        ],
                        navigation: "Bedrock Console -> Playground (Text/Chat/Image) -> Select model",
                        options: [
                            "Choose model version (e.g., Claude 3 Opus, Sonnet, Haiku)",
                            "Adjust inference parameters (temperature, top_p, max_tokens_to_sample)",
                            "Enable streaming responses for real-time interaction"
                        ]
                    },
                    "Titan (Amazon)": {
                        subFeatures: [
                            { title: "Titan Text", desc: "Multilingual text generation optimized for summarization and Q&A" },
                            { title: "Titan Embeddings", desc: "High-quality text embeddings for search and recommendation systems" },
                            { title: "Titan Image Generator", desc: "Text-to-image generation with built-in safety filters" },
                            { title: "Titan Multimodal", desc: "Process both text and images in a single model call" }
                        ],
                        navigation: "Bedrock Console -> Playground (Text/Chat/Image) -> Select model",
                        options: [
                            "Choose Titan model type (Text, Embeddings, Image Generator)",
                            "For text, adjust length and creativity parameters",
                            "For embeddings, specify embedding dimension (e.g., 1536, 12288)"
                        ]
                    },
                    "Jurassic (AI21)": {
                        subFeatures: [
                            { title: "Jumbo/Grande Models", desc: "Large language models capable of advanced text generation, summarization, and question answering." },
                            { title: "Contextual Answers", desc: "Designed to provide highly relevant answers based on provided context." }
                        ],
                        navigation: "Bedrock Console -> Playground (Text/Chat) -> Select model",
                        options: [
                            "Select Jurassic model series (e.g., Jumbo, Grande)",
                            "Configure response length and creativity parameters."
                        ]
                    },
                    "Command (Cohere)": {
                        subFeatures: [
                            { title: "Command Model", desc: "Powerful LLM optimized for conversational AI, summarization, and content generation." },
                            { title: "Embed Model", desc: "Generates high-quality embeddings for search, clustering, and retrieval-augmented generation (RAG)." }
                        ],
                        navigation: "Bedrock Console -> Playground (Text/Chat) -> Select model",
                        options: [
                            "Choose Command model type (e.g., Command, Embed)",
                            "Adjust max tokens and temperature for text generation."
                        ]
                    },
                    "Stable Diffusion": {
                        subFeatures: [
                            { title: "Image Generation", desc: "Generates high-quality images from text descriptions (text-to-image)." },
                            { title: "Image Editing", desc: "Offers capabilities for image-to-image transformations and inpainting/outpainting." }
                        ],
                        navigation: "Bedrock Console -> Playground (Image) -> Select model",
                        options: [
                            "Provide detailed text prompts for image generation.",
                            "Experiment with different artistic styles and negative prompts."
                        ]
                    },
                    "Fine-tuning": {
                        subFeatures: [
                            { title: "Custom Model Training", desc: "Fine-tune foundation models on your specific domain data" },
                            { title: "Parameter Efficient", desc: "LoRA and other efficient fine-tuning techniques to reduce costs" },
                            { title: "Model Versioning", desc: "Track and manage different versions of your fine-tuned models" },
                            { title: "Evaluation Metrics", desc: "Built-in evaluation tools to measure fine-tuning effectiveness" }
                        ],
                        navigation: "Bedrock Console -> Custom models -> Create training job",
                        options: [
                            "Select base foundation model to fine-tune",
                            "Provide S3 path to training data (JSONL format)",
                            "Configure hyperparameters for training (e.g., epochs, batch size)",
                            "Set up validation data for evaluation"
                        ]
                    },
                    "RAG": {
                        subFeatures: [
                            { title: "Knowledge Bases", desc: "Connect models to your private data sources and documents" },
                            { title: "Vector Search", desc: "Semantic search across your knowledge base for relevant context" },
                            { title: "Source Attribution", desc: "Track which documents contributed to model responses" },
                            { title: "Real-time Updates", desc: "Keep knowledge bases synchronized with changing data sources" }
                        ],
                        navigation: "Bedrock Console -> Knowledge bases -> Create knowledge base",
                        options: [
                            "Choose data source (S3, SharePoint, Confluence, etc.)",
                            "Select embedding model (e.g., Titan Embeddings)",
                            "Choose vector database (e.g., Amazon OpenSearch Service, Pinecone, Redis Enterprise)",
                            "Configure chunking strategy for documents"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing training data for fine-tuning and documents for RAG knowledge bases.",
                    "**OpenSearch Service / Pinecone / Redis Enterprise**: For vector databases used in RAG (Retrieval Augmented Generation).",
                    "**Lambda**: To invoke Bedrock models programmatically and integrate with other applications.",
                    "**API Gateway**: To expose Bedrock-powered applications as secure APIs.",
                    "**IAM**: For controlling access to Bedrock and its underlying models."
                ],
                techStack: ["REST APIs", "SDKs (Python, JavaScript)", "LangChain integration", "Vector databases"],
                pricing: "On-demand: $0.0008-$0.024 per 1K input tokens, $0.0024-$0.072 per 1K output tokens (varies by model). Fine-tuning and provisioned throughput have separate pricing.",
                scenarios: [
                    "Conversational AI chatbot for customer support",
                    "Content generation for marketing campaigns",
                    "Code generation and documentation assistance", 
                    "Document summarization and Q&A systems"
                ]
            },
            codewhisperer: {
                name: "Amazon CodeWhisperer",
                description: "AI-powered code generation service that provides real-time code recommendations",
                features: ["Code Completion", "Code Generation", "Security Scans", "Reference Tracking"],
                featureDetails: {
                    "Code Completion": {
                        subFeatures: [
                            { title: "Line Completion", desc: "Suggests the rest of the current line of code" },
                            { title: "Full Function Completion", desc: "Generates entire functions based on comments or surrounding code" },
                            { title: "Context-Aware Suggestions", desc: "Recommendations tailored to your existing codebase and coding style" }
                        ],
                        navigation: "IDE Plugin (VS Code, JetBrains, AWS Cloud9)",
                        options: [
                            "Enable/disable auto-suggestions",
                            "Configure suggestion trigger settings",
                            "Connect to AWS Builder ID or IAM Identity Center for enterprise features"
                        ]
                    },
                    "Code Generation": {
                        subFeatures: [
                            { title: "Comment-to-Code", desc: "Generates code snippets from natural language comments" },
                            { title: "Test Case Generation", desc: "Creates unit test boilerplate for functions" },
                            { title: "Docstring Generation", desc: "Automatically generates documentation strings for your code" }
                        ],
                        navigation: "IDE Plugin (VS Code, JetBrains, AWS Cloud9)",
                        options: [
                            "Select preferred language for code generation",
                            "Adjust verbosity of generated code"
                        ]
                    },
                    "Security Scans": {
                        subFeatures: [
                            { title: "Vulnerability Detection", desc: "Identifies hard-to-find security vulnerabilities in generated and existing code" },
                            { title: "Remediation Suggestions", desc: "Provides actionable recommendations to fix identified security issues" },
                            { title: "Pre-commit Scans", desc: "Integrate scans into your CI/CD pipeline for proactive security" }
                        ],
                        navigation: "IDE Plugin (VS Code, JetBrains, AWS Cloud9) -> CodeWhisperer panel -> Security Scan",
                        options: [
                            "Configure scan scope (current file, project)",
                            "Set up automated security scans on commit or push (via Git hooks)"
                        ]
                    },
                    "Reference Tracking": {
                        subFeatures: [
                            { title: "Attribution", desc: "Flags code suggestions that resemble public open-source training data" },
                            { title: "License Information", desc: "Provides links to relevant open-source licenses for attributed code" },
                            { title: "Filtering", desc: "Option to filter out suggestions that include code with public references" }
                        ],
                        navigation: "IDE Plugin (VS Code, JetBrains, AWS Cloud9) -> CodeWhisperer settings",
                        options: [
                            "Enable/disable reference suggestions",
                            "Configure preferred open-source licenses for suggestions"
                        ]
                    }
                },
                interdependencies: [
                    "**IDE Integration**: Requires plugins for VS Code, JetBrains (IntelliJ, PyCharm, etc.), or AWS Cloud9.",
                    "**AWS Builder ID / IAM Identity Center**: For authentication and enterprise-level features.",
                    "**S3**: Potentially for storing custom code context or enterprise-specific models (advanced use cases)."
                ],
                techStack: ["Python", "Java", "JavaScript", "TypeScript", "C#", "Go", "Rust", "PHP", "SQL", "Scala", "Kotlin", "Ruby", "YAML", "JSON", "CloudFormation"],
                pricing: "Free for individual developers. Professional tier with advanced features and commercial use is priced per user per month (check AWS pricing page for current rates).",
                scenarios: [
                    "Accelerating developer productivity by auto-completing code",
                    "Identifying and fixing security vulnerabilities early in the development cycle",
                    "Onboarding new developers faster with context-aware suggestions",
                    "Generating boilerplate code for common tasks and functions"
                ]
            },
            rekognition: {
                name: "Amazon Rekognition",
                description: "Deep learning-based image and video analysis service",
                features: ["Face Detection", "Object Recognition", "Scene Detection", "Text in Images", "Content Moderation", "Celebrity Recognition", "Custom Labels"],
                featureDetails: {
                    "Face Detection": {
                        subFeatures: [
                            { title: "Face Analysis", desc: "Detect age range, gender, emotions, and facial attributes with confidence scores" },
                            { title: "Face Comparison", desc: "Compare faces across images to determine if they're the same person" },
                            { title: "Face Search", desc: "Search for known faces in collections of millions of faces" },
                            { title: "Live Video Analysis", desc: "Real-time face detection and recognition in video streams" }
                        ],
                        navigation: "Rekognition Console -> Face Analysis / Face Comparison / Face Search (or API calls)",
                        options: [
                            "Choose detection attributes (e.g., ALL, DEFAULT)",
                            "Set similarity threshold for face comparison/search",
                            "Configure streaming video sources (e.g., Kinesis Video Streams)"
                        ]
                    },
                    "Object Recognition": {
                        subFeatures: [
                            { title: "Object Detection", desc: "Identifies thousands of objects and scenes with bounding box coordinates" },
                            { title: "Activity Recognition", desc: "Detect activities and actions in videos like running, dancing, or cooking" },
                            { title: "Brand Detection", desc: "Identify logos and brand symbols in images and videos" },
                            { title: "Custom Labels", desc: "Train custom models to detect specific objects relevant to your business" }
                        ],
                        navigation: "Rekognition Console -> Detect Labels / Detect Activities / Detect Moderation Labels (or API calls)",
                        options: [
                            "Adjust confidence threshold for label detection",
                            "Specify categories for object/scene filtering",
                            "For Custom Labels, provide training images (S3) and labels"
                        ]
                    },
                    "Content Moderation": {
                        subFeatures: [
                            { title: "Inappropriate Content", desc: "Detect explicit or suggestive content with confidence levels" },
                            { title: "Violence Detection", desc: "Identify violent or disturbing content in images and videos" },
                            { title: "Moderation Labels", desc: "Provides hierarchical labels (e.g., 'Explicit Nudity', 'Graphic Violence')" },
                            { title: "Timestamping", desc: "Timestamps for detected unsafe content in videos" }
                        ],
                        navigation: "Rekognition Console -> Content Moderation -> Create analysis job (or API calls)",
                        options: [
                            "Choose image or video analysis",
                            "Set minimum confidence levels for moderation labels",
                            "Receive notifications on moderation results (SNS)"
                        ]
                    },
                    "Text in Images": {
                        subFeatures: [
                            { title: "Text Detection", desc: "Detects and extracts text from images and videos" },
                            { title: "Language Support", desc: "Supports multiple languages for text detection" },
                            { title: "Bounding Box", desc: "Provides bounding box coordinates for each detected word or line" }
                        ],
                        navigation: "Rekognition Console -> Detect Text (or API calls)",
                        options: [
                            "Specify image or video input",
                            "Set text detection confidence threshold"
                        ]
                    },
                    "Celebrity Recognition": {
                        subFeatures: [
                            { title: "Known Celebrity Database", desc: "Recognizes tens of thousands of celebrities in images and videos" },
                            { title: "Confidence Score", desc: "Provides a confidence score for each recognized celebrity" },
                            { title: "Bounding Box", desc: "Bounding box around the recognized celebrity's face" }
                        ],
                        navigation: "Rekognition Console -> Recognize Celebrities (or API calls)",
                        options: [
                            "Specify image or video input"
                        ]
                    },
                    "Custom Labels": {
                        subFeatures: [
                            { title: "Domain-specific Object Detection", desc: "Train Rekognition to identify custom objects, scenes, or concepts unique to your business" },
                            { title: "No ML Expertise Required", desc: "Simplifies custom model training with a visual interface" },
                            { title: "Batch and Real-time Inference", desc: "Deploy custom models for both batch processing and real-time inference" }
                        ],
                        navigation: "Rekognition Console -> Custom Labels -> Projects -> Create project",
                        options: [
                            "Upload training images (S3) and label them using the console or Ground Truth",
                            "Train a custom model",
                            "Deploy the custom model as an endpoint for inference"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing input images/videos and output analysis results.",
                    "**Kinesis Video Streams**: For real-time video analysis.",
                    "**Lambda**: To trigger Rekognition analysis on new content uploads.",
                    "**DynamoDB**: To store metadata or results from Rekognition analysis.",
                    "**CloudWatch**: For monitoring Rekognition usage and alarms.",
                    "**IAM**: For managing permissions for Rekognition to access resources."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "Image/Video formats (JPEG, PNG, MP4, MOV)", "JSON output"],
                pricing: "Pay-per-image/video for analysis. Features like face comparison, content moderation, and custom labels have separate pricing. Real-time video analysis is priced per minute.",
                scenarios: [
                    "Automated content moderation for user-generated content platforms",
                    "Building smart security cameras with face detection and object recognition",
                    "Analyzing video content for brand mentions and insights",
                    "Organizing photo and video libraries with intelligent tagging"
                ]
            },
            textract: {
                name: "Amazon Textract",
                description: "Machine learning service that automatically extracts text, handwriting, and data from scanned documents",
                features: ["Document Text Detection", "Form Extraction", "Table Extraction", "Identity Documents", "Query-based Extraction"],
                featureDetails: {
                    "Document Text Detection": {
                        subFeatures: [
                            { title: "Printed Text", desc: "Accurately extracts printed text from virtually any document" },
                            { title: "Handwriting Detection", desc: "Detects and extracts handwritten text with high accuracy" },
                            { title: "Layout Information", desc: "Provides information about the layout of the document, including lines and words" }
                        ],
                        navigation: "Textract Console -> Document analysis -> Upload document (or API calls: DetectDocumentText)",
                        options: [
                            "Specify input document (image or PDF)",
                            "Choose whether to detect text or analyze document for forms/tables"
                        ]
                    },
                    "Form Extraction": {
                        subFeatures: [
                            { title: "Key-Value Pairs", desc: "Extracts data from forms as key-value pairs (e.g., 'Name': 'John Doe')" },
                            { title: "Structured Data", desc: "Automatically identifies and extracts structured data from various document types" },
                            { title: "Confidence Scores", desc: "Provides confidence scores for each extracted key-value pair" }
                        ],
                        navigation: "Textract Console -> Document analysis -> Upload document -> Analyze (or API calls: AnalyzeDocument with 'FORMS')",
                        options: [
                            "Specify input document (image or PDF)",
                            "Choose to analyze document for forms"
                        ]
                    },
                    "Table Extraction": {
                        subFeatures: [
                            { title: "Structured Tables", desc: "Extracts data from tables, preserving the row and column structure" },
                            { title: "Merged Cells", desc: "Handles complex table structures, including merged cells" },
                            { title: "Pagination", desc: "Extracts data from multi-page tables" }
                        ],
                        navigation: "Textract Console -> Document analysis -> Upload document -> Analyze (or API calls: AnalyzeDocument with 'TABLES')",
                        options: [
                            "Specify input document (image or PDF)",
                            "Choose to analyze document for tables"
                        ]
                    },
                    "Identity Documents": {
                        subFeatures: [
                            { title: "Passport/Driver's License", desc: "Extracts data from passports and U.S. driver's licenses" },
                            { title: "Pre-trained Model", desc: "Uses a pre-trained ML model specifically for identity documents" },
                            { title: "Structured Response", desc: "Provides a structured JSON response with extracted fields" }
                        ],
                        navigation: "Textract Console -> Analyze Identity Documents -> Upload document (or API calls: AnalyzeID)",
                        options: [
                            "Specify input image of an identity document"
                        ]
                    },
                    "Query-based Extraction": {
                        subFeatures: [
                            { title: "Natural Language Queries", desc: "Extracts specific information by asking natural language questions" },
                            { title: "Contextual Understanding", desc: "Understands the context of the document to answer questions accurately" },
                            { title: "Any Document Layout", desc: "Works across different document layouts without templates" }
                        ],
                        navigation: "Textract Console -> Analyze document -> Upload document -> Use queries (or API calls: AnalyzeDocument with 'QUERIES')",
                        options: [
                            "Specify input document (image or PDF)",
                            "Provide a list of natural language queries (e.g., 'What is the total amount?', 'What is the customer name?')"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing input documents (images, PDFs) and output JSON/text results.",
                    "**Lambda**: To trigger Textract processing on new document uploads.",
                    "**DynamoDB**: To store extracted data for downstream applications.",
                    "**Comprehend**: For further NLP on extracted text (e.g., sentiment analysis on customer feedback forms).",
                    "**SNS / SQS**: For notifications on asynchronous document processing jobs.",
                    "**CloudWatch**: For monitoring Textract usage and job status."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "Document formats (JPEG, PNG, PDF)", "JSON output"],
                pricing: "Pay-per-page for text detection, form extraction, table extraction, and query-based extraction. Identity document analysis has separate pricing.",
                scenarios: [
                    "Automating invoice processing and data entry",
                    "Digitizing medical records for easy search and analysis",
                    "Extracting data from government forms for regulatory compliance",
                    "Building a searchable archive of scanned historical documents"
                ]
            },
            'lookout-vision': {
                name: "Amazon Lookout for Vision",
                description: "Machine learning service that uses computer vision to spot defects and anomalies in industrial products",
                features: ["Image Anomaly Detection", "Model Training", "Real-time Inference", "Batch Inference"],
                featureDetails: {
                    "Image Anomaly Detection": {
                        subFeatures: [
                            { title: "Unsupervised Learning", desc: "Detects anomalies without requiring labeled defect images for training" },
                            { title: "Pixel-level Localization", desc: "Identifies the exact location of defects within an image" },
                            { title: "High Accuracy", desc: "Designed for high-precision detection of subtle manufacturing defects" }
                        ],
                        navigation: "Lookout for Vision Console -> Projects -> Models -> Start model (or API calls: DetectAnomalies)",
                        options: [
                            "Specify input image (JPEG, PNG)",
                            "Get anomaly score and pixel-level anomaly map"
                        ]
                    },
                    "Model Training": {
                        subFeatures: [
                            { title: "Low-code Training", desc: "Train custom models with a minimal amount of 'good' images and no coding required" },
                            { title: "Automated Data Augmentation", desc: "Automatically generates variations of your training images to improve model robustness" },
                            { title: "Model Versioning", desc: "Manage different versions of your trained models" }
                        ],
                        navigation: "Lookout for Vision Console -> Projects -> Models -> Train new model",
                        options: [
                            "Upload a dataset of 'normal' (non-defective) images (S3)",
                            "Optionally upload a small set of 'anomalous' images for improved detection"
                        ]
                    },
                    "Real-time Inference": {
                        subFeatures: [
                            { title: "Low-latency Endpoints", desc: "Deploy models as real-time endpoints for immediate defect detection on production lines" },
                            { title: "Edge Deployment", desc: "Deploy models to edge devices for on-premises inference" },
                            { title: "API Integration", desc: "Integrate defect detection into manufacturing systems via API" }
                        ],
                        navigation: "Lookout for Vision Console -> Projects -> Models -> Deploy model (or API calls: StartModel)",
                        options: [
                            "Choose instance type for real-time endpoint",
                            "Monitor endpoint status and performance"
                        ]
                    },
                    "Batch Inference": {
                        subFeatures: [
                            { title: "Offline Processing", desc: "Analyze large batches of images for defects offline" },
                            { title: "S3 Integration", desc: "Process images directly from S3 buckets" },
                            { title: "Asynchronous Jobs", desc: "Run long-running defect detection jobs without real-time constraints" }
                        ],
                        navigation: "Lookout for Vision Console -> Projects -> Models -> Create inference scheduling job (or API calls: StartAnomaliesDetection)",
                        options: [
                            "Specify S3 input and output locations for batch images",
                            "Choose the trained model version for inference"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing training images and inference results.",
                    "**Kinesis Video Streams**: For real-time image capture from industrial cameras.",
                    "**Lambda**: To trigger inference on new images or process detection results.",
                    "**CloudWatch**: For monitoring model performance and service usage.",
                    "**Greengrass**: For deploying models to edge devices for local inference."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "Image formats (JPEG, PNG)"],
                pricing: "Pay-per-hour for model training and per-image for inference (real-time and batch). Edge deployment has separate pricing.",
                scenarios: [
                    "Automated quality inspection on manufacturing assembly lines (e.g., checking for scratches, dents, misalignments)",
                    "Detecting anomalies in electronic components and circuit boards",
                    "Inspecting textiles for fabric defects",
                    "Monitoring food production for quality control"
                ]
            },
            comprehend: {
                name: "Amazon Comprehend",
                description: "Natural-language processing (NLP) service that uses machine learning to find insights and relationships in text",
                features: ["Sentiment Analysis", "Entity Recognition", "Keyphrase Extraction", "Language Detection", "Topic Modeling", "Targeted Sentiment", "Events Detection", "Custom NLP"],
                featureDetails: {
                    "Entity Recognition": {
                        subFeatures: [
                            { title: "Named Entity Recognition (NER)", desc: "Identifies people, places, organizations, dates, and other named entities" },
                            { title: "Protected Health Information (PHI) Detection", desc: "Specifically identifies and redacts PHI in medical text" },
                            { title: "Personal Identifiable Information (PII) Detection", desc: "Detects and redacts PII like names, addresses, and credit card numbers" },
                            { title: "Custom Entity Recognition", desc: "Train custom models to identify entities specific to your domain" }
                        ],
                        navigation: "Comprehend Console -> Analyze (Entities) / Customization -> Custom entity recognizers (or API calls)",
                        options: [
                            "Choose language for analysis",
                            "Specify entity types to detect",
                            "For custom entities, provide training documents and entity lists (S3)"
                        ]
                    },
                    "Sentiment Analysis": {
                        subFeatures: [
                            { title: "Positive, Negative, Neutral, Mixed", desc: "Determines the overall sentiment of a text" },
                            { title: "Sentiment Scores", desc: "Provides confidence scores for each sentiment type" },
                            { title: "Asynchronous Analysis", desc: "Process large batches of documents for sentiment" }
                        ],
                        navigation: "Comprehend Console -> Analyze (Sentiment) (or API calls: DetectSentiment)",
                        options: [
                            "Choose language for analysis",
                            "Select output format (JSON)"
                        ]
                    },
                    "Keyphrase Extraction": {
                        subFeatures: [
                            { title: "Important Phrases", desc: "Identifies the main topics and key phrases in a text" },
                            { title: "Confidence Scores", desc: "Returns confidence scores for each extracted keyphrase" },
                            { title: "Ranked Keyphrases", desc: "Provides keyphrases in order of relevance" }
                        ],
                        navigation: "Comprehend Console -> Analyze (Key phrases) (or API calls: DetectKeyPhrases)",
                        options: [
                            "Choose language for analysis",
                            "Filter by minimum confidence score"
                        ]
                    },
                    "Language Detection": {
                        subFeatures: [
                            { title: "Dominant Language", desc: "Identifies the primary language of a text from over 100 languages" },
                            { title: "Confidence Score", desc: "Provides a confidence score for the detected language" },
                            { title: "Language Codes", desc: "Returns standard ISO 639-1 language codes" }
                        ],
                        navigation: "Comprehend Console -> Analyze (Language) (or API calls: DetectDominantLanguage)",
                        options: [
                            "No specific configuration options beyond input text"
                        ]
                    },
                    "Topic Modeling": {
                        subFeatures: [
                            { title: "Automatic Topic Discovery", desc: "Automatically identifies underlying themes and topics in a collection of documents" },
                            { title: "Document-Topic Assignments", desc: "Assigns documents to detected topics with scores" },
                            { title: "Keyword Associations", desc: "Provides a list of keywords associated with each topic" }
                        ],
                        navigation: "Comprehend Console -> Analysis jobs -> Create topic modeling job (or API calls: StartTopicsDetectionJob)",
                        options: [
                            "Specify S3 input bucket for documents (text files)",
                            "Define number of topics to detect (optional)",
                            "Set minimum number of documents per topic",
                            "Schedule recurring topic modeling jobs"
                        ]
                    },
                    "Custom NLP": {
                        subFeatures: [
                            { title: "Custom Classifiers", desc: "Train custom models to categorize documents into your own defined labels" },
                            { title: "Custom Entity Recognizers", desc: "Train custom models to identify entities specific to your domain" },
                            { title: "Automated Model Training", desc: "Simplified training process requiring minimal ML expertise" },
                            { title: "Model Versioning", desc: "Manage and deploy different versions of your custom NLP models" }
                        ],
                        navigation: "Comprehend Console -> Customization -> Custom classifiers / Custom entity recognizers",
                        options: [
                            "Provide training data (text documents with labels/annotations) in S3",
                            "Define labels for classification or entity types for recognition",
                            "Monitor model training progress and evaluation metrics"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing input documents for analysis and output results.",
                    "**Lambda**: To trigger Comprehend analysis and process results serverlessly.",
                    "**Kinesis (Data Streams/Firehose)**: For real-time ingestion of text for analysis.",
                    "**Textract**: For extracting text from documents before sending to Comprehend for NLP.",
                    "**SNS / SQS**: For notifications on asynchronous job completion.",
                    "**DynamoDB**: To store extracted entities, sentiment scores, or topic assignments."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java, .NET)", "JSON output"],
                pricing: "Pay-per-unit for text processed (e.g., $0.00005/unit for sentiment/entity detection, 1 unit = 100 characters). Custom models have separate training and inference pricing.",
                scenarios: [
                    "Analyzing customer feedback from surveys, social media, and call center transcripts",
                    "Automating document classification for legal or financial departments",
                    "Redacting sensitive information (PII/PHI) from documents for compliance",
                    "Building a content recommendation system based on document topics"
                ]
            },
            translate: {
                name: "Amazon Translate",
                description: "Neural machine translation service for fast, high-quality, and affordable language translation",
                features: ["Real-time Translation", "Batch Translation", "Custom Terminology", "Active Custom Translation"],
                featureDetails: {
                    "Real-time Translation": {
                        subFeatures: [
                            { title: "Low-latency Translation", desc: "Translate text instantly for conversational applications" },
                            { title: "Multiple Language Pairs", desc: "Supports translation between many language pairs" },
                            { title: "HTML Translation", desc: "Preserves HTML tags during translation for web content" }
                        ],
                        navigation: "Translate Console -> Real-time translation (or API calls: TranslateText)",
                        options: [
                            "Specify source and target languages",
                            "Provide plain text or HTML input"
                        ]
                    },
                    "Batch Translation": {
                        subFeatures: [
                            { title: "Large Document Translation", desc: "Translate entire documents (PDF, Word, Excel, Plain Text, HTML) stored in S3" },
                            { title: "Asynchronous Processing", desc: "Processes large volumes of data in the background" },
                            { title: "Multilingual Documents", desc: "Automatically detects source language for each document in a batch" }
                        ],
                        navigation: "Translate Console -> Batch translation (or API calls: StartTextTranslationJob)",
                        options: [
                            "Specify S3 input and output locations",
                            "Choose source and target languages (or auto-detect)",
                            "Enable custom terminology or Active Custom Translation"
                        ]
                    },
                    "Custom Terminology": {
                        subFeatures: [
                            { title: "Glossary Creation", desc: "Define custom terms and their translations to ensure consistent brand messaging" },
                            { title: "Domain-specific Translations", desc: "Ensure specific industry terms are translated accurately" },
                            { title: "Import/Export", desc: "Manage terminology lists in CSV format" }
                        ],
                        navigation: "Translate Console -> Custom terminology -> Create terminology",
                        options: [
                            "Upload a CSV file with source and target terms",
                            "Associate terminology with translation jobs"
                        ]
                    },
                    "Active Custom Translation": {
                        subFeatures: [
                            { title: "Customization with Parallel Data", desc: "Train a custom translation model using your own parallel data (source-target pairs)" },
                            { title: "Improved Accuracy", desc: "Achieve higher translation quality for domain-specific content" },
                            { title: "Continuous Improvement", desc: "Update your custom models with new data to refine translations" }
                        ],
                        navigation: "Translate Console -> Active Custom Translation -> Create parallel data -> Create custom translation job",
                        options: [
                            "Upload parallel data (aligned source and target language segments) to S3",
                            "Monitor custom model training progress",
                            "Apply custom models to real-time or batch translation jobs"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing input documents for batch translation and custom terminology files.",
                    "**Lambda**: To trigger translation on new content or to integrate with other services.",
                    "**Comprehend**: Can be used to detect the source language before translation or to analyze translated text.",
                    "**Lex / Polly**: To build multilingual conversational interfaces by combining translation with speech or chatbot services.",
                    "**SNS / SQS**: For receiving notifications on completion of batch translation jobs."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "Input/Output: Plain text, HTML, CSV (for terminology), PDF, DOCX, XLSX"],
                pricing: "Pay-per-character for real-time translation. Batch translation is priced per character. Custom Terminology and Active Custom Translation have separate pricing for training and usage.",
                scenarios: [
                    "Globalizing website content for international audiences",
                    "Translating customer support chats in real-time",
                    "Localizing legal documents and contracts",
                    "Enabling multilingual communication in enterprise applications"
                ]
            },
            lex: {
                name: "Amazon Lex",
                description: "Service for building conversational interfaces (chatbots, voice assistants) powered by AI",
                features: ["Automatic Speech Recognition (ASR)", "Natural Language Understanding (NLU)", "Multi-turn Conversations", "Context Management", "Integrations"],
                featureDetails: {
                    "Automatic Speech Recognition (ASR)": {
                        subFeatures: [
                            { title: "High Accuracy Speech-to-Text", desc: "Converts spoken language into text for processing" },
                            { title: "Custom Vocabularies", desc: "Improve recognition accuracy for domain-specific terms" },
                            { title: "Streaming ASR", desc: "Real-time speech transcription for dynamic interactions" }
                        ],
                        navigation: "Lex Console -> Bots -> Select bot -> Intents -> Slot types (for custom vocab)",
                        options: [
                            "Configure input audio format (e.g., PCM, Opus)",
                            "Enable custom vocabularies for improved recognition of specific terms"
                        ]
                    },
                    "Natural Language Understanding (NLU)": {
                        subFeatures: [
                            { title: "Intent Recognition", desc: "Determines the user's goal based on their input" },
                            { title: "Slot Extraction", desc: "Extracts specific pieces of information (slots) needed to fulfill an intent" },
                            { title: "Utterance Training", desc: "Define example phrases (utterances) for each intent" }
                        ],
                        navigation: "Lex Console -> Bots -> Select bot -> Intents / Slot types",
                        options: [
                            "Define intents, sample utterances, and slots",
                            "Configure slot types (built-in or custom)",
                            "Set up validation rules for slot values"
                        ]
                    },
                    "Multi-turn Conversations": {
                        subFeatures: [
                            { title: "Context Management", desc: "Maintains conversational state across turns to remember user preferences" },
                            { title: "Follow-up Prompts", desc: "Guides the user through a conversation flow to gather necessary information" },
                            { title: "Session Attributes", desc: "Store temporary data for the duration of a conversation" }
                        ],
                        navigation: "Lex Console -> Bots -> Select bot -> Intents -> Fulfillment (or integrate with Lambda)",
                        options: [
                            "Design conversation flows using prompts and confirmations",
                            "Implement conditional logic using Lambda functions for dynamic responses"
                        ]
                    },
                    "Integrations": {
                        subFeatures: [
                            { title: "Lambda Functions", desc: "Connects to Lambda for complex business logic and fulfillment" },
                            { title: "Amazon Connect", desc: "Build intelligent IVR (Interactive Voice Response) systems" },
                            { title: "Third-party Messaging", desc: "Integrate with Facebook Messenger, Slack, Twilio, and more" },
                            { title: "Web/Mobile SDKs", desc: "Embed Lex chatbots into web and mobile applications" }
                        ],
                        navigation: "Lex Console -> Bots -> Select bot -> Channels",
                        options: [
                            "Configure IAM roles for Lambda invocation",
                            "Set up specific channel integrations with API keys and webhooks"
                        ]
                    }
                },
                interdependencies: [
                    "**Lambda**: Essential for fulfilling intents, performing custom logic, and integrating with backend systems.",
                    "**Polly**: For generating natural-sounding speech responses for voicebots.",
                    "**Transcribe**: Can be used for advanced speech-to-text scenarios or analytics on conversations.",
                    "**CloudWatch**: For monitoring bot performance, errors, and user interactions.",
                    "**S3**: For storing voice recordings or chat logs for analysis.",
                    "**Amazon Connect**: For contact center automation and intelligent routing."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java, Node.js)", "JSON for bot configuration"],
                pricing: "Pay-per-request: $0.004 per voice request, $0.00075 per text request. Free tier available.",
                scenarios: [
                    "Automated customer service chatbots for FAQs and order status",
                    "Voice-enabled IVR systems for call centers",
                    "Internal helpdesk bots for employee support",
                    "Personalized shopping assistants"
                ]
            },
            polly: {
                name: "Amazon Polly",
                description: "Text-to-speech service that turns text into lifelike speech",
                features: ["Standard Voices", "Neural Voices", "Speech Marks", "SSML Support", "Long-form Synthesis"],
                featureDetails: {
                    "Standard Voices": {
                        subFeatures: [
                            { title: "Wide Language Support", desc: "Offers a variety of voices across many languages" },
                            { title: "General Purpose", desc: "Suitable for common applications requiring synthesized speech" },
                            { title: "Cost-Effective", desc: "Lower cost compared to Neural Voices" }
                        ],
                        navigation: "Polly Console -> Text-to-speech (or API calls: SynthesizeSpeech)",
                        options: [
                            "Choose from a wide range of standard male and female voices",
                            "Select desired language"
                        ]
                    },
                    "Neural Voices": {
                        subFeatures: [
                            { title: "Lifelike Speech", desc: "Utilizes deep learning to produce highly natural and expressive speech" },
                            { title: "Conversational Style", desc: "Optimized for conversational AI and long-form content" },
                            { title: "Newscaster Style", desc: "Specific voice styles for broadcast media" }
                        ],
                        navigation: "Polly Console -> Text-to-speech (or API calls: SynthesizeSpeech with Engine='neural')",
                        options: [
                            "Choose from available Neural voices (e.g., 'Matthew', 'Joanna', 'Amy')",
                            "Select speech style (e.g., 'conversational', 'newscaster')"
                        ]
                    },
                    "Speech Marks": {
                        subFeatures: [
                            { title: "Metadata for Speech", desc: "Provides timing information for sentences, words, and phonemes" },
                            { title: "Lip-syncing", desc: "Enables precise synchronization with animations for avatars" },
                            { title: "Highlighting", desc: "Allows highlighting text as it's spoken" }
                        ],
                        navigation: "Polly Console -> Text-to-speech -> Advanced settings -> Speech marks (or API calls: SynthesizeSpeech with OutputFormat='json' and SpeechMarkTypes)",
                        options: [
                            "Specify SpeechMark types to generate (e.g., 'sentence', 'word', 'viseme')"
                        ]
                    },
                    "SSML Support": {
                        subFeatures: [
                            { title: "Speech Synthesis Markup Language", desc: "Control speech aspects like pronunciation, volume, pitch, and speaking rate" },
                            { title: "Breaks and Pauses", desc: "Add pauses and control the duration of breaks in speech" },
                            { title: "Emphasis and Pronunciation", desc: "Emphasize words or provide phonetic spellings for complex terms" }
                        ],
                        navigation: "Polly Console -> Text-to-speech -> SSML (or API calls: SynthesizeSpeech with SSML text)",
                        options: [
                            "Embed SSML tags directly in your input text (e.g., `<speak>Hello <break time='1s'/> world.</speak>`)"
                        ]
                    },
                    "Long-form Synthesis": {
                        subFeatures: [
                            { title: "Asynchronous Processing", desc: "Synthesize long blocks of text (up to 100,000 characters per request)" },
                            { title: "Output to S3", desc: "Store generated audio files directly in an S3 bucket" },
                            { title: "Chapter Markers", desc: "Create audiobooks with chapters and other structural elements" }
                        ],
                        navigation: "Polly Console -> Long-form synthesis (or API calls: StartSpeechSynthesisTask)",
                        options: [
                            "Specify input text from S3 or direct input",
                            "Choose S3 output bucket and file format (MP3, OGG, PCM)",
                            "Set up SNS notifications for job completion"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing output audio files, especially for long-form synthesis.",
                    "**Lambda**: To trigger Polly synthesis from other applications or events.",
                    "**Lex**: For providing voice output in conversational AI applications.",
                    "**Transcribe**: Can be used to generate text from audio that is then passed to Polly for re-synthesis.",
                    "**CloudWatch**: For monitoring API calls and service usage.",
                    "**SNS**: For receiving notifications on long-form synthesis job completion."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "SSML (Speech Synthesis Markup Language)", "Audio formats (MP3, OGG, PCM)"],
                pricing: "Pay-per-character for speech generated. Neural voices are more expensive than Standard voices. Long-form synthesis has separate pricing.",
                scenarios: [
                    "Creating audio versions of articles, blogs, and documents (audiobooks)",
                    "Developing voice user interfaces for mobile and web applications",
                    "Generating natural-sounding responses for chatbots and virtual assistants",
                    "Producing voiceovers for videos and presentations"
                ]
            },
            transcribe: {
                name: "Amazon Transcribe",
                description: "Automatic speech recognition (ASR) service that converts speech to text",
                features: ["Standard Transcription", "Medical Transcription", "Custom Vocabularies", "Custom Language Models (CLMs)", "Speaker Diarization", "Channel Identification", "Content Redaction"],
                featureDetails: {
                    "Standard Transcription": {
                        subFeatures: [
                            { title: "High Accuracy", desc: "Converts audio to text for various use cases" },
                            { title: "Multiple Languages", desc: "Supports over 30 languages" },
                            { title: "Timestamping", desc: "Provides timestamps for each word for easy navigation" },
                            { title: "Punctuation", desc: "Automatically adds punctuation to transcribed text" }
                        ],
                        navigation: "Transcribe Console -> Transcription jobs -> Create job (or API calls: StartTranscriptionJob)",
                        options: [
                            "Choose input audio file from S3 (MP3, WAV, FLAC, AMR, OGG, WebM)",
                            "Select source language",
                            "Specify output S3 bucket"
                        ]
                    },
                    "Medical Transcription": {
                        subFeatures: [
                            { title: "Medical Terminology", desc: "Optimized for accurate transcription of medical conversations" },
                            { title: "Speaker Labeling", desc: "Differentiates between clinicians and patients" },
                            { title: "HIPAA-Eligible", desc: "Meets compliance requirements for protected health information" }
                        ],
                        navigation: "Transcribe Console -> Medical transcription jobs -> Create job (or API calls: StartMedicalTranscriptionJob)",
                        options: [
                            "Choose medical specialty (e.g., general, radiology)",
                            "Select type of conversation (e.g., dictation, conversation)"
                        ]
                    },
                    "Custom Vocabularies": {
                        subFeatures: [
                            { title: "Improved Accuracy", desc: "Enhance transcription of domain-specific words and phrases" },
                            { title: "Pronunciation Guidance", desc: "Provide phonetic pronunciations for unique terms" },
                            { title: "Vocabulary Filtering", desc: "Filter out unwanted words or phrases from transcriptions" }
                        ],
                        navigation: "Transcribe Console -> Custom vocabularies -> Create vocabulary",
                        options: [
                            "Upload a text file with custom words and phrases (one per line)",
                            "Specify pronunciation using IPA or Sounds Like"
                        ]
                    },
                    "Custom Language Models (CLMs)": {
                        subFeatures: [
                            { title: "Domain-specific Models", desc: "Train a custom language model using your own text data" },
                            { title: "Higher Accuracy for Niche Audio", desc: "Significantly improve transcription accuracy for specific audio contexts" },
                            { title: "Adapts to Accent and Speech Patterns", desc: "Tailors the model to unique speech characteristics" }
                        ],
                        navigation: "Transcribe Console -> Custom language models -> Create model",
                        options: [
                            "Provide S3 path to training data (plain text, one sentence per line)",
                            "Choose base language model",
                            "Monitor model training progress"
                        ]
                    },
                    "Speaker Diarization": {
                        subFeatures: [
                            { title: "Identify Speakers", desc: "Distinguishes and labels individual speakers in an audio file" },
                            { title: "Speaker Timestamps", desc: "Provides timestamps for each speaker's turn" },
                            { title: "Up to 10 Speakers", desc: "Can identify up to 10 unique speakers in a single audio file" }
                        ],
                        navigation: "Transcribe Console -> Transcription jobs -> Create job -> Speaker diarization (or API calls: StartTranscriptionJob with 'ShowSpeakerLabels')",
                        options: [
                            "Enable speaker diarization option during job creation"
                        ]
                    },
                    "Content Redaction": {
                        subFeatures: [
                            { title: "PII Redaction", desc: "Automatically removes or masks personally identifiable information (PII) from transcripts" },
                            { title: "Built-in Categories", desc: "Redact common PII types like names, addresses, credit card numbers" },
                            { title: "Custom Redaction", desc: "Define custom patterns for redaction (e.g., internal IDs)" }
                        ],
                        navigation: "Transcribe Console -> Transcription jobs -> Create job -> Content redaction (or API calls: StartTranscriptionJob with 'ContentRedaction')",
                        options: [
                            "Enable content redaction and specify PII entity types to redact",
                            "Choose redaction output type (e.g., 'PII_REDACTED', 'REPLACE_WITH_PII_ENTITY_TYPE')"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing input audio/video files and output transcripts.",
                    "**Lambda**: To trigger transcription on new audio uploads or process transcription results.",
                    "**Comprehend**: For further NLP analysis (sentiment, entities) on the transcribed text.",
                    "**Lex**: For providing the ASR capability in conversational AI applications.",
                    "**Kinesis Video Streams**: For real-time transcription of live audio/video streams.",
                    "**SNS / SQS**: For receiving notifications on transcription job completion."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "Audio formats (MP3, WAV, FLAC, AMR, OGG, WebM)", "JSON output"],
                pricing: "Pay-per-second for audio processed. Medical transcription, custom vocabularies, and custom language models have separate pricing. Real-time streaming is also priced per second.",
                scenarios: [
                    "Transcribing customer service calls for quality assurance and analytics",
                    "Converting meeting recordings into searchable text documents",
                    "Generating subtitles and captions for video content",
                    "Enabling voice search in applications",
                    "Automating medical dictation for healthcare providers"
                ]
            },
            forecast: {
                name: "Amazon Forecast",
                description: "Fully managed service that uses machine learning to deliver highly accurate forecasts",
                features: ["Automatic ML Models", "Hierarchical Forecasting", "What-if Analysis", "Cold Start Forecasting"],
                featureDetails: {
                    "Automatic ML Models": {
                        subFeatures: [
                            { title: "Automated Model Selection", desc: "Automatically selects the best forecasting algorithm based on your data" },
                            { title: "Hyperparameter Optimization", desc: "Tunes model parameters for optimal performance" },
                            { title: "Ensemble Models", desc: "Combines multiple models for improved accuracy" }
                        ],
                        navigation: "Forecast Console -> Datasets -> Create dataset group -> Create predictor",
                        options: [
                            "Choose 'AutoML' for automatic model selection or select specific algorithms (e.g., ARIMA, DeepAR+)",
                            "Define forecast horizon (e.g., next 7 days, 3 months)"
                        ]
                    },
                    "Hierarchical Forecasting": {
                        subFeatures: [
                            { title: "Multi-level Aggregation", desc: "Forecasts at different granularities (e.g., product, store, region)" },
                            { title: "Consistency Across Levels", desc: "Ensures forecasts are consistent from top to bottom" },
                            { title: "Group Forecasts", desc: "Generate forecasts for specific groups or categories" }
                        ],
                        navigation: "Forecast Console -> Datasets -> Create dataset group -> Create predictor -> Hierarchical Forecasting",
                        options: [
                            "Define hierarchy levels in your dataset (e.g., region > store > item)",
                            "Choose reconciliation method for consistency"
                        ]
                    },
                    "What-if Analysis": {
                        subFeatures: [
                            { title: "Scenario Modeling", desc: "Simulate different business scenarios (e.g., price changes, promotions)" },
                            { title: "Impact Assessment", desc: "Understand the potential impact of future events on your forecasts" },
                            { title: "Interactive Visualizations", desc: "Visualize the impact of 'what-if' scenarios" }
                        ],
                        navigation: "Forecast Console -> Forecasts -> Create what-if forecast",
                        options: [
                            "Define modifications to input time series (e.g., increase promotion by 10%)",
                            "Compare different scenarios side-by-side"
                        ]
                    },
                    "Cold Start Forecasting": {
                        subFeatures: [
                            { title: "New Item Forecasting", desc: "Generates forecasts for new products or locations with limited historical data" },
                            { title: "Similar Item Grouping", desc: "Leverages patterns from similar existing items for better initial forecasts" },
                            { title: "Demand Attribution", desc: "Allocates demand for new products based on relevant features" }
                        ],
                        navigation: "Forecast Console -> Datasets -> Create dataset group -> Create predictor (handled automatically)",
                        options: [
                            "Ensure your dataset includes relevant item metadata for cold start (e.g., category, brand)"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing input time-series data and output forecasts.",
                    "**Lambda**: To automate data ingestion into Forecast or trigger forecasting jobs.",
                    "**Glue**: For ETL and data preparation before feeding into Forecast.",
                    "**CloudWatch**: For monitoring forecasting job status and performance.",
                    "**QuickSight**: For visualizing and sharing Forecast results with business users."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "CSV/JSON input/output"],
                pricing: "Pay-per-gigabyte for data stored, per-hour for training and prediction. Backtesting also incurs a cost.",
                scenarios: [
                    "Retail demand planning for inventory optimization",
                    "Financial forecasting for revenue and budget planning",
                    "Resource utilization forecasting in cloud environments",
                    "Workforce planning based on future demand"
                ]
            },
            personalize: {
                name: "Amazon Personalize",
                description: "Machine learning service for developers to build real-time personalized recommendations for customers",
                features: ["User Segmentation", "Real-time Recommendations", "Batch Recommendations", "Experimentation & A/B Testing", "Cold Start Handling"],
                featureDetails: {
                    "User Segmentation": {
                        subFeatures: [
                            { title: "Automatic Segmentation", desc: "Groups users with similar preferences and behaviors" },
                            { title: "Content-based Segmentation", desc: "Segments users based on the types of items they interact with" },
                            { title: "Behavioral Segmentation", desc: "Segments users based on their purchasing or Browse history" }
                        ],
                        navigation: "Personalize Console -> Campaigns -> Select campaign -> View recommendations (or API calls)",
                        options: [
                            "Not directly configurable as a standalone feature; it's an outcome of model training."
                        ]
                    },
                    "Real-time Recommendations": {
                        subFeatures: [
                            { title: "Low-latency API", desc: "Delivers recommendations instantly as users interact with your application" },
                            { title: "User-Item Interaction", desc: "Recommends items based on a user's current activity and past behavior" },
                            { title: "Item-to-Item Similarity", desc: "Suggests items similar to those a user has viewed or purchased" }
                        ],
                        navigation: "Personalize Console -> Campaigns -> Create campaign (or API calls: GetRecommendations)",
                        options: [
                            "Choose recipe (algorithm) for your recommendation model (e.g., User-Personalization, Item-to-item sim)",
                            "Set up item attributes to include in recommendations (e.g., category, price)"
                        ]
                    },
                    "Batch Recommendations": {
                        subFeatures: [
                            { title: "Offline Generation", desc: "Generates recommendations for a large set of users or items periodically" },
                            { title: "Output to S3", desc: "Stores recommendation lists in an S3 bucket for batch processing" },
                            { title: "Offline Personalization", desc: "Useful for email campaigns, push notifications, or nightly updates" }
                        ],
                        navigation: "Personalize Console -> Batch inference jobs -> Create new job",
                        options: [
                            "Specify S3 input (user/item data) and output locations",
                            "Choose the trained solution version to use for inference"
                        ]
                    },
                    "Experimentation & A/B Testing": {
                        subFeatures: [
                            { title: "A/B Testing Framework", desc: "Test different recommendation strategies or models against each other" },
                            { title: "Performance Metrics", desc: "Track key metrics like click-through rate, conversion rate, and revenue" },
                            { title: "Controlled Rollouts", desc: "Gradually roll out new recommendation models to a subset of users" },
                        ],
                        navigation: "Personalize Console -> Campaigns -> Create campaign -> A/B Test",
                        options: [
                            "Define traffic splits for different recommendation strategies",
                            "Monitor real-time metrics in CloudWatch for comparison"
                        ]
                    },
                    "Cold Start Handling": {
                        subFeatures: [
                            { title: "New User Recommendations", desc: "Provides initial recommendations for users with no interaction history" },
                            { title: "New Item Recommendations", desc: "Recommends newly added items even without interaction data" },
                            { title: "Contextual Recommendations", desc: "Leverages attributes of users and items to generate recommendations" }
                        ],
                        navigation: "Personalize Console -> Datasets -> Dataset group -> Create schema (covered by design)",
                        options: [
                            "Ensure your item and user datasets include rich metadata (e.g., categories, descriptions, demographics)"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing user-item interaction data, item metadata, and user metadata for training and batch inference.",
                    "**Lambda**: To trigger recommendation requests or update datasets.",
                    "**Kinesis (Data Firehose/Streams)**: For real-time ingestion of user interaction data.",
                    "**CloudWatch**: For monitoring campaign performance and service metrics.",
                    "**DynamoDB**: To store user profiles or item catalogs for quick lookup."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "CSV/JSON input for datasets"],
                pricing: "Pay-per-GB for data stored, per-hour for training, and per-recommendation for real-time inference (TPS-based for campaigns). Batch inference is priced per recommendation.",
                scenarios: [
                    "Personalized product recommendations on e-commerce websites",
                    "Tailored content suggestions for streaming services",
                    "Customized news feeds for media companies",
                    "Dynamic pricing based on user segments"
                ]
            },
            'fraud-detector': {
                name: "Amazon Fraud Detector",
                description: "Fully managed service that uses machine learning to identify potentially fraudulent online activities",
                features: ["Fraud Model Training", "Real-time Fraud Predictions", "Rules Engine", "Event-based Detection"],
                featureDetails: {
                    "Fraud Model Training": {
                        subFeatures: [
                            { title: "Pre-built ML Models", desc: "Leverages Amazon's experience in fraud detection to build high-accuracy models" },
                            { title: "Custom Model Training", desc: "Train models using your own historical fraud data" },
                            { title: "Low-code/No-code", desc: "Simplifies the process of building and deploying fraud detection models" }
                        ],
                        navigation: "Fraud Detector Console -> Models -> Create model",
                        options: [
                            "Specify event type (e.g., signup, payment)",
                            "Provide S3 path to historical fraud data (CSV)",
                            "Choose model type (e.g., Online Fraud Insights, Transaction Fraud Insights)"
                        ]
                    },
                    "Real-time Fraud Predictions": {
                        subFeatures: [
                            { title: "Low-latency Scores", desc: "Generates fraud scores for incoming events in milliseconds" },
                            { title: "Dynamic Responses", desc: "Take immediate actions like blocking transactions or adding extra verification steps" },
                            { title: "API Integration", desc: "Easy integration with existing applications via API" }
                        ],
                        navigation: "Fraud Detector Console -> Detectors -> Select detector -> Get fraud prediction (or API calls: GetEventPrediction)",
                        options: [
                            "Pass event variables (e.g., IP address, email, payment method) for real-time evaluation"
                        ]
                    },
                    "Rules Engine": {
                        subFeatures: [
                            { title: "Custom Rules", desc: "Define business logic rules to augment ML model decisions" },
                            { title: "Rule Orchestration", desc: "Combine ML model scores with custom rules for granular control" },
                            { title: "Outcomes", desc: "Define actions to take based on fraud detection results (e.g., 'approve', 'review', 'reject')" }
                        ],
                        navigation: "Fraud Detector Console -> Detectors -> Select detector -> Rules",
                        options: [
                            "Create rules using event variables, model scores, and lists",
                            "Define outcomes associated with each rule (e.g., 'approve_transaction', 'send_for_review')",
                            "Set rule execution order"
                        ]
                    },
                    "Event-based Detection": {
                        subFeatures: [
                            { title: "Define Event Types", desc: "Configure different types of events you want to monitor for fraud (e.g., signup, login, payment)" },
                            { title: "Associated Entities", desc: "Track entities involved in events (e.g., customers, payment instruments)" },
                            { title: "Variables", desc: "Define data points associated with each event (e.g., IP address, email address)" }
                        ],
                        navigation: "Fraud Detector Console -> Event types / Entities / Variables",
                        options: [
                            "Define required and optional variables for each event type",
                            "Map variables to data sources and data types"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing historical fraud data for model training and batch fraud prediction results.",
                    "**Lambda**: For invoking Fraud Detector for real-time predictions and for implementing custom actions based on outcomes.",
                    "**API Gateway**: To expose an endpoint for ingesting events for fraud detection.",
                    "**CloudWatch**: For monitoring fraud detection metrics and alerts.",
                    "**Kinesis (Data Firehose/Streams)**: For ingesting real-time event data into Fraud Detector.",
                    "**DynamoDB**: To store event data or fraud decision logs."
                ],
                techStack: ["REST APIs", "SDKs (Python, Java)", "JSON for event input"],
                pricing: "Pay-per-prediction for online fraud detection. Training and model hosting are also priced based on usage.",
                scenarios: [
                    "Detecting fraudulent online payments and chargebacks",
                    "Preventing account takeovers and fake account registrations",
                    "Identifying promotional abuse or policy violations",
                    "Screening for synthetic identity fraud"
                ]
            },
            'ec2-ai': {
                name: "EC2 AI Instances",
                description: "Amazon EC2 instances optimized for machine learning workloads, featuring GPUs and custom AWS ML chips",
                features: ["GPU Instances", "Inferentia Instances", "Trainium Instances", "Elastic Inference"],
                featureDetails: {
                    "GPU Instances": {
                        subFeatures: [
                            { title: "P-series (e.g., P4d, P3)", desc: "High-performance NVIDIA GPUs for large-scale distributed training" },
                            { title: "G-series (e.g., G5, G4dn)", desc: "Cost-effective NVIDIA GPUs for training, inference, and graphics workloads" },
                            { title: "CUDA and cuDNN", desc: "Optimized for deep learning frameworks using NVIDIA's acceleration libraries" }
                        ],
                        navigation: "EC2 Console -> Instances -> Launch instances -> Choose AMI -> Instance Type (filter by 'GPU')",
                        options: [
                            "Select instance family (P, G)",
                            "Choose specific instance size (e.g., p4d.24xlarge, g4dn.xlarge)",
                            "Select GPU-optimized AMIs (e.g., Deep Learning AMI)"
                        ]
                    },
                    "Inferentia Instances": {
                        subFeatures: [
                            { title: "Inf1 instances", desc: "Powered by AWS Inferentia chips for high-performance, low-cost ML inference" },
                            { title: "Cost Optimization", desc: "Designed to significantly reduce inference costs compared to GPUs" },
                            { title: "Framework Support", desc: "Supports popular ML frameworks like TensorFlow, PyTorch, and ONNX" }
                        ],
                        navigation: "EC2 Console -> Instances -> Launch instances -> Choose AMI -> Instance Type (filter by 'Inferentia')",
                        options: [
                            "Choose Inf1 instance sizes (e.g., inf1.xlarge, inf1.24xlarge)",
                            "Optimize models for Inferentia using SageMaker Neo or AWS Neuron SDK"
                        ]
                    },
                    "Trainium Instances": {
                        subFeatures: [
                            { title: "Trn1 instances", desc: "Powered by AWS Trainium chips for high-performance, low-cost deep learning training" },
                            { title: "Massive Scale", desc: "Designed for petabyte-scale datasets and trillion-parameter models" },
                            { title: "Distributed Training", desc: "Optimized for distributed training with high-bandwidth networking" }
                        ],
                        navigation: "EC2 Console -> Instances -> Launch instances -> Choose AMI -> Instance Type (filter by 'Trainium')",
                        options: [
                            "Choose Trn1 instance sizes (e.g., trn1.2xlarge, trn1.32xlarge)",
                            "Utilize AWS Neuron SDK for model compilation and training"
                        ]
                    },
                    "Elastic Inference": {
                        subFeatures: [
                            { title: "Attachable Accelerators", desc: "Add GPU-powered inference acceleration to any EC2 instance type" },
                            { title: "Cost-Effective Inference", desc: "Optimize costs by adding only the necessary amount of GPU power" },
                            { title: "Flexible Deployment", desc: "Decouple compute and acceleration resources" }
                        ],
                        navigation: "EC2 Console -> Instances -> Launch instances -> Configure instance details -> Elastic Inference",
                        options: [
                            "Choose the EI accelerator type (e.g., eia1.medium, eia2.large)",
                            "Attach EI accelerators to compatible EC2 instances (e.g., M5, C5, R5)"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: For storing training data, model artifacts, and checkpointing during training.",
                    "**EBS**: For high-performance storage attached to instances.",
                    "**ECR**: For storing custom Docker images for ML environments.",
                    "**VPC**: For network configuration and security of instances.",
                    "**IAM**: For managing permissions for instances to access other AWS services.",
                    "**SageMaker**: Often uses EC2 instances under the hood for managed training and hosting.",
                    "**CloudWatch**: For monitoring instance performance and resource utilization."
                ],
                techStack: ["Linux/Windows AMIs", "Deep Learning AMIs", "Docker", "NVIDIA CUDA/cuDNN", "AWS Neuron SDK"],
                pricing: "Pay-per-hour for instance usage. Pricing varies significantly by instance type, region, and whether On-Demand, Reserved Instances, or Spot Instances are used.",
                scenarios: [
                    "Training large deep learning models (e.g., for computer vision, NLP)",
                    "Running high-volume real-time inference endpoints",
                    "Developing and testing new ML algorithms",
                    "Processing large datasets with ML libraries"
                ]
            },
            'lambda-ai': {
                name: "Lambda + AI",
                description: "Serverless compute service that lets you run code without provisioning or managing servers, ideal for event-driven AI applications",
                features: ["Event-driven Inference", "Pre/Post-processing", "Model Deployment", "Asynchronous Workflows"],
                featureDetails: {
                    "Event-driven Inference": {
                        subFeatures: [
                            { title: "Real-time Processing", desc: "Trigger AI inference on new data uploads (e.g., images to S3, messages to SQS)" },
                            { title: "Scalable Inference", desc: "Automatically scales to handle spikes in inference requests" },
                            { title: "Cost-Effective", desc: "Pay only for the compute time consumed when your code runs" }
                        ],
                        navigation: "Lambda Console -> Functions -> Create function -> Add triggers",
                        options: [
                            "Configure triggers (S3, SQS, Kinesis, API Gateway, DynamoDB Streams)",
                            "Specify runtime (Python, Node.js, Java, .NET, Go, Ruby)",
                            "Allocate memory and timeout settings"
                        ]
                    },
                    "Pre/Post-processing": {
                        subFeatures: [
                            { title: "Data Transformation", desc: "Prepare input data for AI models (e.g., resize images, tokenize text)" },
                            { title: "Result Formatting", desc: "Process and format AI model outputs for downstream applications" },
                            { title: "Feature Engineering", desc: "Create new features from raw data before inference" }
                        ],
                        navigation: "Lambda Console -> Functions -> Select function -> Code",
                        options: [
                            "Write custom code in your preferred runtime language",
                            "Utilize AWS SDKs to interact with AI services"
                        ]
                    },
                    "Model Deployment": {
                        subFeatures: [
                            { title: "Lightweight Models", desc: "Deploy small ML models (e.g., ONNX, TensorFlow Lite) directly within Lambda" },
                            { title: "Container Images", desc: "Package larger models within container images for Lambda deployment" },
                            { title: "Layer Support", desc: "Include ML libraries (e.g., NumPy, Scikit-learn, TensorFlow) as Lambda layers" },
                        ],
                        navigation: "Lambda Console -> Functions -> Create function -> Select 'Container image' or configure 'Layers'",
                        options: [
                            "For container images, specify ECR repository and tag",
                            "For layers, add necessary ML libraries or custom code dependencies"
                        ]
                    },
                    "Asynchronous Workflows": {
                        subFeatures: [
                            { title: "Queued Processing", desc: "Process events asynchronously using SQS or SNS for robust workflows" },
                            { title: "Step Functions Orchestration", desc: "Combine Lambda with Step Functions for complex, multi-step AI pipelines" },
                            { title: "Dead-letter Queues", desc: "Handle failed invocations gracefully" }
                        ],
                        navigation: "Lambda Console -> Functions -> Select function -> Configuration -> Asynchronous invocation",
                        options: [
                            "Configure destination for successful/failed invocations (e.g., SQS, SNS)",
                            "Set up retry attempts and maximum event age"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: Common trigger for Lambda functions, especially for new object uploads (e.g., images, documents).",
                    "**API Gateway**: To expose Lambda functions as RESTful APIs for real-time inference.",
                    "**SQS / SNS**: For queuing events and asynchronous processing, and for notifications.",
                    "**DynamoDB**: For storing application state or lightweight data related to AI inferences.",
                    "**SageMaker**: Lambda can invoke SageMaker endpoints for inference or trigger SageMaker jobs.",
                    "**All AWS AI Services**: Lambda acts as an orchestrator and integration point for almost all AWS AI services.",
                    "**CloudWatch**: For monitoring Lambda function invocations, errors, and logs."
                ],
                techStack: ["Python", "Node.js", "Java", ".NET", "Go", "Ruby", "Custom Runtimes", "Container Images"],
                pricing: "Pay-per-request and per-GB-second of compute time. Free tier available.",
                scenarios: [
                    "Image classification on new image uploads to S3",
                    "Real-time sentiment analysis of chat messages",
                    "Triggering document processing with Textract when a PDF is uploaded",
                    "Generating personalized content using Personalize upon user interaction events"
                ]
            },
            'iam-ai': {
                name: "IAM for AI Services",
                description: "AWS Identity and Access Management (IAM) controls who can access your AWS resources and what they can do with them, crucial for securing AI workloads",
                features: ["Role-based Access Control (RBAC)", "Fine-grained Permissions", "Service-linked Roles", "Identity Federation"],
                featureDetails: {
                    "Role-based Access Control (RBAC)": {
                        subFeatures: [
                            { title: "IAM Roles", desc: "Grant temporary permissions to entities (users, applications, services) without long-term credentials" },
                            { title: "Trust Policies", desc: "Define which entities can assume a role" },
                            { title: "Permissions Policies", desc: "Specify what actions can be performed on which resources" }
                        ],
                        navigation: "IAM Console -> Roles -> Create role",
                        options: [
                            "Choose trusted entity (e.g., 'AWS service', 'Web identity', 'SAML 2.0 federation')",
                            "Attach permission policies (e.g., AmazonS3ReadOnlyAccess, AmazonSageMakerFullAccess)"
                        ]
                    },
                    "Fine-grained Permissions": {
                        subFeatures: [
                            { title: "Resource-level Permissions", desc: "Control access to specific SageMaker models, S3 buckets, or Comprehend jobs" },
                            { title: "Condition Keys", desc: "Add conditions to policies (e.g., allow access only from specific IP addresses)" },
                            { title: "Tags for Access Control", desc: "Use resource tags to define access policies dynamically" }
                        ],
                        navigation: "IAM Console -> Policies -> Create policy (visual editor or JSON)",
                        options: [
                            "Specify ARN (Amazon Resource Name) for specific resources",
                            "Use 'Condition' block in JSON policies (e.g., `aws:SourceIp`)",
                            "Implement tag-based access control strategies"
                        ]
                    },
                    "Service-linked Roles": {
                        subFeatures: [
                            { title: "Predefined by AWS", desc: "Roles pre-configured with permissions required by an AWS service to call other AWS services on your behalf" },
                            { title: "Automated Creation", desc: "Automatically created when you enable or configure certain AWS services" },
                            { title: "Streamlined Setup", desc: "Simplifies granting necessary permissions without manual configuration" }
                        ],
                        navigation: "IAM Console -> Roles (filter by 'Service-linked roles')",
                        options: [
                            "Understand the permissions granted by service-linked roles (read-only access to policies)"
                        ]
                    },
                    "Identity Federation": {
                        subFeatures: [
                            { title: "SSO Integration", desc: "Integrate with corporate directories (e.g., Active Directory) or identity providers (e.g., Okta, PingOne)" },
                            { title: "SAML 2.0 Support", desc: "Use SAML for federated access to AWS accounts" },
                            { title: "Web Identity Federation", desc: "Enable users to sign in with public identity providers (e.g., Login with Amazon, Facebook, Google)" }
                        ],
                        navigation: "IAM Console -> Identity providers -> Add provider",
                        options: [
                            "Configure trust relationships with your IdP",
                            "Map IdP attributes to IAM roles"
                        ]
                    }
                },
                interdependencies: [
                    "**All AWS AI Services**: IAM is fundamental for securing access to every AWS service, including all AI/ML offerings.",
                    "**Organizations**: For centrally managing permissions across multiple AWS accounts.",
                    "**CloudTrail**: To log all API calls made to AWS services, including those governed by IAM policies, for auditing and compliance.",
                    "**Resource Access Manager (RAM)**: For sharing resources across AWS accounts, which also relies on IAM for permissions."
                ],
                techStack: ["IAM Policies (JSON)", "AWS CLI", "AWS SDKs", "SAML 2.0"],
                pricing: "No direct cost for IAM. You only pay for the AWS resources that your users and applications access.",
                scenarios: [
                    "Granting data scientists specific permissions to train models in SageMaker without access to production databases",
                    "Controlling which S3 buckets a Textract job can read from or write to",
                    "Federating access for enterprise users to the AWS console and specific AI services",
                    "Ensuring only authorized applications can call AI service APIs"
                ]
            },
            'vpc-security': {
                name: "VPC & Network Security",
                description: "Amazon Virtual Private Cloud (VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define, crucial for secure AI deployments",
                features: ["Private Subnets", "Security Groups", "Network ACLs", "VPC Endpoints", "Direct Connect/VPN"],
                featureDetails: {
                    "Private Subnets": {
                        subFeatures: [
                            { title: "Network Isolation", desc: "Isolate your AI resources (e.g., SageMaker endpoints, EC2 instances) from the public internet" },
                            { title: "Controlled Egress", desc: "Route outbound traffic through NAT gateways or proxy servers" },
                            { title: "Enhanced Security", desc: "Minimizes attack surface for sensitive AI workloads" }
                        ],
                        navigation: "VPC Console -> Subnets -> Create subnet",
                        options: [
                            "Define CIDR blocks for subnets",
                            "Associate with route tables for internet access or private routing",
                            "Place sensitive AI workloads (training, inference) in private subnets"
                        ]
                    },
                    "Security Groups": {
                        subFeatures: [
                            { title: "Virtual Firewalls", desc: "Act as stateful firewalls for EC2 instances and other resources at the instance level" },
                            { title: "Inbound/Outbound Rules", desc: "Control traffic based on IP address, port, and protocol" },
                            { title: "References", desc: "Reference other security groups for easier management" }
                        ],
                        navigation: "EC2 Console -> Security Groups -> Create security group",
                        options: [
                            "Define inbound rules (e.g., allow SSH from specific IPs, allow HTTPS from anywhere)",
                            "Define outbound rules (e.g., allow all outbound to internet, restrict to specific services)"
                        ]
                    },
                    "Network ACLs": {
                        subFeatures: [
                            { title: "Stateless Packet Filters", desc: "Optional layer of security at the subnet level, inspecting both inbound and outbound traffic" },
                            { title: "Numbered Rules", desc: "Rules are evaluated in order, and the first matching rule is applied" },
                            { title: "Deny Rules", desc: "Can explicitly deny traffic, unlike security groups" }
                        ],
                        navigation: "VPC Console -> Network ACLs -> Create network ACL",
                        options: [
                            "Define inbound and outbound rules with rule numbers (lower numbers are evaluated first)",
                            "Associate ACLs with specific subnets"
                        ]
                    },
                    "VPC Endpoints": {
                        subFeatures: [
                            { title: "Private Connectivity", desc: "Connect your VPC to AWS services (e.g., S3, SageMaker, Comprehend) privately without using the public internet" },
                            { title: "Interface Endpoints (Powered by PrivateLink)", desc: "Elastic Network Interfaces (ENIs) within your VPC for service access" },
                            { title: "Gateway Endpoints", desc: "Gateway to S3 and DynamoDB from your VPC" }
                        ],
                        navigation: "VPC Console -> Endpoints -> Create endpoint",
                        options: [
                            "Choose service name (e.g., com.amazonaws.region.sagemaker.api)",
                            "Select VPC and subnets for the endpoint",
                            "Attach security groups and endpoint policies for granular control"
                        ]
                    },
                    "Direct Connect/VPN": {
                        subFeatures: [
                            { title: "Private Network Connection", desc: "Establish a dedicated network connection from your on-premises data center to AWS" },
                            { title: "Site-to-Site VPN", desc: "Securely connect your on-premises network to your VPC over IPsec VPN tunnels" },
                            { title: "Hybrid Cloud AI", desc: "Securely extend your on-premises ML workloads to AWS" }
                        ],
                        navigation: "Direct Connect Console / VPC Console -> Site-to-Site VPN",
                        options: [
                            "Order a Direct Connect connection or configure VPN connection details (Customer Gateway, Virtual Private Gateway)",
                            "Set up routing and firewall rules for on-premises to VPC communication"
                        ]
                    }
                },
                interdependencies: [
                    "**EC2 / SageMaker / Lambda**: All compute resources for AI workloads often run within a VPC.",
                    "**S3 / DynamoDB**: VPC Endpoints provide secure, private access to these data storage services.",
                    "**Route 53**: For DNS resolution within your VPC and for resolving private endpoints.",
                    "**CloudTrail / CloudWatch**: For logging network events and monitoring traffic flow.",
                    "**IAM**: For controlling who can create and manage VPC resources and their configurations."
                ],
                techStack: ["VPC (CIDR, Subnets, Route Tables)", "Security Groups", "Network ACLs", "AWS PrivateLink"],
                pricing: "No direct cost for VPC. You pay for associated resources like NAT Gateways, VPN connections, VPC Endpoints, and data transfer.",
                scenarios: [
                    "Securing SageMaker training jobs and inference endpoints within a private network",
                    "Ensuring sensitive data processed by Textract or Comprehend never traverses the public internet",
                    "Building hybrid AI architectures with on-premises data and cloud-based ML models",
                    "Implementing granular network access control for AI development environments"
                ]
            },
            encryption: {
                name: "Data Encryption",
                description: "Protecting data at rest and in transit using cryptographic methods, essential for securing sensitive AI data and models",
                features: ["Encryption at Rest", "Encryption in Transit", "AWS Key Management Service (KMS)", "Client-Side Encryption"],
                featureDetails: {
                    "Encryption at Rest": {
                        subFeatures: [
                            { title: "S3 Server-Side Encryption (SSE)", desc: "Encrypts data stored in S3 using S3-managed keys (SSE-S3), KMS keys (SSE-KMS), or customer-provided keys (SSE-C)" },
                            { title: "EBS Encryption", desc: "Encrypts data volumes attached to EC2 instances" },
                            { title: "Database Encryption", desc: "Encrypts data in managed databases like RDS and DynamoDB" }
                        ],
                        navigation: "S3 Console -> Bucket properties -> Default encryption / EBS Console -> Volumes -> Create volume (encryption option) / RDS Console -> Create database (encryption option)",
                        options: [
                            "Choose encryption key type (AWS managed, KMS, customer-provided)",
                            "Enable default encryption for S3 buckets",
                            "Encrypt EBS volumes at creation or copy to encrypt existing ones"
                        ]
                    },
                    "Encryption in Transit": {
                        subFeatures: [
                            { title: "TLS/SSL", desc: "Encrypts data exchanged between clients and AWS services using Transport Layer Security" },
                            { title: "VPC Endpoints", desc: "Ensures traffic remains within the AWS network, reducing exposure" },
                            { title: "Client-side HTTPS", desc: "Use HTTPS endpoints for all API calls to AWS services" }
                        ],
                        navigation: "Automatically handled by AWS API endpoints (ensure HTTPS is used)",
                        options: [
                            "Always use HTTPS for all AWS SDK and CLI interactions",
                            "Ensure your client applications are configured to use TLS 1.2 or higher"
                        ]
                    },
                    "AWS Key Management Service (KMS)": {
                        subFeatures: [
                            { title: "Managed Encryption Keys", desc: "Create and manage cryptographic keys used across AWS services" },
                            { title: "Customer Managed Keys (CMKs)", desc: "Full control over key policies, rotation, and usage" },
                            { title: "AWS Managed Keys", desc: "Default keys managed by AWS for various services" },
                            { title: "Hardware Security Modules (HSMs)", desc: "Keys protected by FIPS 140-2 validated HSMs" }
                        ],
                        navigation: "KMS Console -> Customer managed keys -> Create key",
                        options: [
                            "Choose key type (Symmetric, Asymmetric)",
                            "Define key usage (Encrypt and decrypt, Sign and verify)",
                            "Set key policy for access control (IAM users, roles)",
                            "Configure automatic key rotation"
                        ]
                    },
                    "Client-Side Encryption": {
                        subFeatures: [
                            { title: "Encrypt Before Upload", desc: "Encrypt data on the client side before sending to AWS services (e.g., S3)" },
                            { title: "SDK Support", desc: "AWS SDKs provide utilities for client-side encryption and decryption" },
                            { title: "Additional Control", desc: "Offers an extra layer of security beyond server-side encryption" }
                        ],
                        navigation: "Implemented in your application code using AWS SDKs or encryption libraries",
                        options: [
                            "Choose client-side master key (CMK or external key)",
                            "Select encryption algorithm (e.g., AES-256)"
                        ]
                    }
                },
                interdependencies: [
                    "**S3 / EBS / RDS / DynamoDB / SageMaker**: Almost all AWS data storage and ML services integrate with KMS for encryption at rest.",
                    "**IAM**: Essential for controlling who can access and use KMS keys.",
                    "**CloudTrail**: Logs all KMS API calls for auditing key usage.",
                    "**CloudHSM**: For customers requiring dedicated hardware security modules for their keys (integrates with KMS)."
                ],
                techStack: ["AWS KMS", "TLS/SSL", "AWS SDKs", "S3 Encryption"],
                pricing: "KMS is priced per key per month and per API request for key operations. Data transfer charges may apply for cross-region encryption.",
                scenarios: [
                    "Encrypting sensitive training data stored in S3 for SageMaker models",
                    "Protecting customer information extracted by Textract before storing in a database",
                    "Securing communication channels between a Lambda function and a Rekognition endpoint",
                    "Ensuring compliance with data privacy regulations like GDPR and HIPAA"
                ]
            },
            compliance: {
                name: "Compliance & Governance",
                description: "Adhering to regulatory standards and internal policies, crucial for trustworthy AI deployments in regulated industries",
                features: ["Certifications & Attestations", "Data Residency", "Audit Trails", "Responsible AI Governance"],
                featureDetails: {
                    "Certifications & Attestations": {
                        subFeatures: [
                            { title: "GDPR Compliance", desc: "AWS services are built to help customers meet GDPR requirements for data protection" },
                            { title: "HIPAA Eligibility", desc: "Services like SageMaker, Comprehend Medical are HIPAA-eligible for Protected Health Information (PHI)" },
                            { title: "SOC Reports", desc: "Regularly audited by third parties for security, availability, and confidentiality" },
                            { title: "PCI DSS Compliance", desc: "Adheres to Payment Card Industry Data Security Standard for handling payment card data" }
                        ],
                        navigation: "AWS Artifact Console -> Access reports -> Compliance reports",
                        options: [
                            "Review relevant compliance reports and certifications for your industry",
                            "Implement shared responsibility model guidelines"
                        ]
                    },
                    "Data Residency": {
                        subFeatures: [
                            { title: "Region Selection", desc: "Choose specific AWS regions to ensure data remains within geographical boundaries" },
                            { title: "Data Locality", desc: "Maintain data processing and storage in a chosen region for regulatory requirements" },
                            { title: "Data Transfer Restrictions", desc: "Prevent unauthorized cross-region data transfers" }
                        ],
                        navigation: "When creating resources (S3 buckets, EC2 instances, SageMaker domains), choose the desired AWS Region",
                        options: [
                            "Configure S3 bucket policies to restrict cross-region replication",
                            "Utilize VPC endpoints to keep traffic private within a region"
                        ]
                    },
                    "Audit Trails": {
                        subFeatures: [
                            { title: "AWS CloudTrail", desc: "Records all API calls and events made in your AWS account for auditing and governance" },
                            { title: "Config Rules", desc: "Automatically assesses, audits, and evaluates the configurations of your AWS resources" },
                            { title: "Security Hub", desc: "Centralized view of your security and compliance status" }
                        ],
                        navigation: "CloudTrail Console -> Event history / CloudTrail Lake / Config Console -> Rules",
                        options: [
                            "Enable CloudTrail for all regions and global services",
                            "Configure S3 bucket for log storage and KMS encryption",
                            "Set up CloudWatch Alarms for specific API events (e.g., unauthorized access)"
                        ]
                    },
                    "Responsible AI Governance": {
                        subFeatures: [
                            { title: "SageMaker Clarify", desc: "Detects bias and explains predictions in ML models" },
                            { title: "Best Practices", desc: "AWS provides guidance on responsible AI development and deployment" },
                            { title: "Transparency and Accountability", desc: "Tools to understand and document AI system behavior" }
                        ],
                        navigation: "SageMaker Console -> SageMaker Clarify",
                        options: [
                            "Regularly run bias and explainability analyses on your models",
                            "Document model training data, features, and evaluation metrics",
                            "Establish clear human oversight processes for AI-driven decisions"
                        ]
                    }
                },
                interdependencies: [
                    "**All AWS Services**: Compliance practices apply broadly across all AWS services.",
                    "**IAM**: For defining policies that enforce compliance, such as data access restrictions.",
                    "**CloudWatch**: For monitoring compliance-related metrics and alerts.",
                    "**AWS Audit Manager**: For automating audit evidence collection.",
                    "**AWS Security Hub**: For a consolidated view of compliance status across services.",
                    "**AWS Config**: For continuous monitoring and assessment of resource configurations against compliance rules."
                ],
                techStack: ["AWS CloudTrail", "AWS Config", "AWS Security Hub", "AWS Audit Manager"],
                pricing: "Pricing varies by service (e.g., CloudTrail for log storage, Config for rule evaluations).",
                scenarios: [
                    "Ensuring healthcare AI applications comply with HIPAA regulations",
                    "Maintaining GDPR compliance for customer data processed by AI services",
                    "Auditing all changes made to SageMaker models and training jobs",
                    "Implementing responsible AI practices to ensure fairness and transparency in ML predictions"
                ]
            },
            monitoring: {
                name: "Security Monitoring",
                description: "Continuously collecting and analyzing data from your AWS environment to detect potential security threats and anomalies in AI workloads",
                features: ["CloudTrail", "GuardDuty", "Security Hub", "Config", "VPC Flow Logs"],
                featureDetails: {
                    "CloudTrail": {
                        subFeatures: [
                            { title: "API Call Logging", desc: "Records all API calls and events in your AWS account" },
                            { title: "Event History", desc: "View, search, and download recent AWS account activity" },
                            { title: "CloudTrail Lake", desc: "For immutable storage and advanced analytics of audit logs" },
                            { title: "Integrations", desc: "Send logs to CloudWatch Logs or S3 for further analysis" }
                        ],
                        navigation: "CloudTrail Console -> Event history / Trails -> Create trail",
                        options: [
                            "Enable logging for all regions and global services",
                            "Configure S3 bucket for log storage and KMS encryption",
                            "Set up CloudWatch Alarms for specific API events (e.g., unauthorized access)"
                        ]
                    },
                    "GuardDuty": {
                        subFeatures: [
                            { title: "Intelligent Threat Detection", desc: "Continuously monitors for malicious activity and unauthorized behavior" },
                            { title: "Machine Learning & Signatures", desc: "Uses ML, anomaly detection, and threat intelligence to identify threats" },
                            { title: "Findings", desc: "Generates actionable security findings with severity levels" }
                        ],
                        navigation: "GuardDuty Console -> Get started -> Enable GuardDuty",
                        options: [
                            "Enable GuardDuty in all accounts and regions",
                            "Integrate with Security Hub and Detective for aggregated findings and deeper investigation"
                        ]
                    },
                    "Security Hub": {
                        subFeatures: [
                            { title: "Consolidated Security View", desc: "Aggregates security findings from various AWS services (GuardDuty, Inspector, Macie, Config) and partner products" },
                            { title: "Compliance Checks", desc: "Automated checks against security industry standards and best practices (e.g., CIS Benchmarks)" },
                            { title: "Automated Remediation", desc: "Trigger automated responses to findings using EventBridge" }
                        ],
                        navigation: "Security Hub Console -> Enable Security Hub",
                        options: [
                            "Enable relevant security standards (e.g., AWS Foundational Security Best Practices)",
                            "Configure integrations with other AWS services and third-party tools",
                            "Set up custom insights and dashboards"
                        ]
                    },
                    "Config": {
                        subFeatures: [
                            { title: "Configuration Recorder", desc: "Records configuration changes of your AWS resources" },
                            { title: "Config Rules", desc: "Evaluate recorded configurations against desired policies" },
                            { title: "Compliance History", desc: "View historical configuration changes and compliance status" }
                        ],
                        navigation: "Config Console -> Get started -> Enable Config recorder -> Rules",
                        options: [
                            "Record all resource types or specific types",
                            "Deploy managed Config rules or create custom ones (Lambda-backed)",
                            "Set up remediation actions for non-compliant resources"
                        ]
                    },
                    "VPC Flow Logs": {
                        subFeatures: [
                            { title: "Network Traffic Monitoring", desc: "Captures information about IP traffic going to and from network interfaces in your VPC" },
                            { title: "Anomaly Detection", desc: "Identify unusual network patterns that may indicate compromise" },
                            { title: "Troubleshooting", desc: "Diagnose connectivity issues and identify security policy gaps" }
                        ],
                        navigation: "VPC Console -> VPCs -> Select VPC -> Flow Logs -> Create flow log",
                        options: [
                            "Choose destination for flow logs (CloudWatch Logs, S3, Kinesis Firehose)",
                            "Filter by traffic type (Accepted, Rejected, All)",
                            "Set up log format and aggregation interval"
                        ]
                    }
                },
                interdependencies: [
                    "**CloudWatch**: For metric collection, alarms, and logging from all services.",
                    "**S3**: For storing CloudTrail logs, Config snapshots, and VPC Flow Logs.",
                    "**Kinesis Firehose**: For streaming logs to various destinations.",
                    "**Lambda**: For automated remediation actions triggered by Security Hub or Config rules.",
                    "**AWS Detective**: For deeper security investigations by analyzing activity logs from various sources.",
                    "**IAM**: For controlling access to security monitoring services and logs."
                ],
                techStack: ["AWS CloudTrail", "Amazon GuardDuty", "AWS Security Hub", "AWS Config", "Amazon VPC Flow Logs"],
                pricing: "Pricing varies by service, generally based on data volume ingested, rules evaluated, or findings generated.",
                scenarios: [
                    "Detecting unauthorized access attempts to SageMaker notebooks",
                    "Monitoring for unusual data exfiltration patterns from S3 buckets containing training data",
                    "Automated alerts for non-compliant AI resource configurations",
                    "Investigating suspicious network activity related to ML inference endpoints",
                    "Centralizing security posture for all AI-related AWS accounts"
                ]
            },
            'data-protection': {
                name: "Data Protection & Privacy",
                description: "Implementing measures to safeguard sensitive data from unauthorized access, loss, or misuse, especially critical for AI workloads dealing with personal or proprietary information",
                features: ["Amazon Macie", "PII/PHI Detection", "Data Masking/Tokenization", "Data Loss Prevention (DLP)"],
                featureDetails: {
                    "Amazon Macie": {
                        subFeatures: [
                            { title: "S3 Data Discovery", desc: "Discovers and classifies sensitive data (PII, financial, credentials) in S3 buckets" },
                            { title: "Anomaly Detection", desc: "Detects unusual access patterns to sensitive data" },
                            { title: "Automated Remediation", desc: "Integrates with EventBridge to trigger alerts or automated actions on findings" }
                        ],
                        navigation: "Macie Console -> Get started -> Enable Macie",
                        options: [
                            "Enable Macie for your AWS account(s)",
                            "Configure sensitive data discovery jobs for specific S3 buckets",
                            "Set up alerts for high-severity findings"
                        ]
                    },
                    "PII/PHI Detection": {
                        subFeatures: [
                            { title: "Comprehend PII/PHI", desc: "Detects and redacts Protected Health Information and Personally Identifiable Information in text" },
                            { title: "Transcribe Content Redaction", desc: "Removes or masks PII from transcribed audio" },
                            { title: "Configurable Detection", desc: "Choose which types of PII/PHI to detect and redact" }
                        ],
                        navigation: "Comprehend Console -> PII / PHI detection (or API calls) / Transcribe Console -> Transcription jobs -> Content redaction",
                        options: [
                            "Specify PII entity types for detection (e.g., NAME, ADDRESS, CREDIT_CARD_NUMBER)",
                            "Choose redaction strategy (e.g., 'REPLACE_WITH_PII_ENTITY_TYPE')"
                        ]
                    },
                    "Data Masking/Tokenization": {
                        subFeatures: [
                            { title: "Data Transformation", desc: "Replace sensitive data with non-sensitive substitutes (tokens or masked values)" },
                            { title: "Pseudonymization", desc: "Transforming data to obscure identities while retaining analytical utility" },
                            { title: "AWS Glue (DataBrew)", desc: "Use Glue DataBrew for visual data preparation and masking transformations" }
                        ],
                        navigation: "Implemented in ETL processes (e.g., AWS Glue, Lambda functions) or application code",
                        options: [
                            "Define masking rules (e.g., last 4 digits of credit card, hash email)",
                            "Consider data retention policies for original sensitive data"
                        ]
                    },
                    "Data Loss Prevention (DLP)": {
                        subFeatures: [
                            { title: "Preventing Data Exfiltration", desc: "Implement controls to prevent sensitive data from leaving your AWS environment" },
                            { title: "Access Controls", desc: "Strong IAM policies to restrict who can access and move data" },
                            { title: "Network Segmentation", desc: "Use VPCs and Security Groups to isolate sensitive data flows" }
                        ],
                        navigation: "Combination of IAM policies, VPC configurations, and services like Macie and GuardDuty",
                        options: [
                            "Implement strict IAM policies (e.g., Deny S3 PutObject if not encrypted)",
                            "Use S3 bucket policies to restrict public access",
                            "Monitor for unusual data transfer activity with VPC Flow Logs and CloudWatch"
                        ]
                    }
                },
                interdependencies: [
                    "**S3**: Primary target for Macie to scan for sensitive data.",
                    "**Comprehend / Transcribe**: Integrate PII/PHI detection directly into text and speech processing pipelines.",
                    "**AWS Glue / Glue DataBrew**: For data preparation and masking operations on datasets before ML training.",
                    "**KMS**: For encrypting masked or tokenized data.",
                    "**IAM**: For strict access control over sensitive data buckets and related services.",
                    "**CloudWatch / EventBridge**: For alerting and automating responses to data protection events.",
                    "**AWS Config**: To audit that data protection configurations (e.g., S3 encryption) are in place."
                ],
                techStack: ["Amazon Macie", "Amazon Comprehend", "Amazon Transcribe", "AWS Glue DataBrew", "IAM Policies"],
                pricing: "Macie is priced per GB scanned and per GB for sensitive data discovery. Comprehend and Transcribe have their own pricing for PII/PHI detection.",
                scenarios: [
                    "Automated detection of customer credit card numbers in S3 data lakes for compliance",
                    "Redacting patient names from medical notes before using them for NLP research",
                    "Masking PII in training datasets for SageMaker to ensure data privacy",
                    "Setting up alerts for any attempts to move sensitive PII data outside designated secure buckets"
                ]
            }
        };

        let currentActiveBranch = null; // Tracks the currently active main branch

        /**
         * Toggles the visibility of sub-services within a main branch.
         * Closes other open branches when a new one is opened.
         * @param {string} branchId - The ID of the branch element to toggle.
         */
        function toggleBranch(branchId) {
            const subServices = document.getElementById(branchId);
            // If there's an active branch and it's not the one just clicked, close it
            if (currentActiveBranch && currentActiveBranch !== subServices) {
                currentActiveBranch.classList.remove('active');
            }
            // Toggle the 'active' class for the clicked branch's sub-services
            subServices.classList.toggle('active');
            // Update the currentActiveBranch variable
            currentActiveBranch = subServices.classList.contains('active') ? subServices : null;
        }

        /**
         * Calls the Gemini API with a given prompt and displays the response.
         * @param {string} prompt - The text prompt to send to the LLM.
         * @param {string} outputElementId - The ID of the HTML element where the response should be displayed.
         * @param {HTMLElement} buttonElement - The button element that triggered the call, to manage its state.
         */
        async function callGeminiAPI(prompt, outputElementId, buttonElement) {
            const outputElement = document.getElementById(outputElementId);
            const originalButtonText = buttonElement.innerHTML; // Store original button text

            outputElement.style.display = 'block'; // Ensure output area is visible
            outputElement.innerHTML = '<div class="loading-indicator"></div> Loading AI response...'; // Show loading

            buttonElement.disabled = true; // Disable button during API call

            try {
                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: prompt }] });
                const payload = { contents: chatHistory };
                const apiKey = ""; // Canvas will automatically provide the API key here.
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                // Check if the response is OK before trying to parse JSON
                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);
                }

                const result = await response.json();
                console.log("Gemini API raw response:", result); // Log raw response for debugging

                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const text = result.candidates[0].content.parts[0].text;
                    outputElement.innerHTML = text; // Display the response
                } else {
                    outputElement.innerHTML = 'Error: Could not get a valid response from the AI. Unexpected structure.';
                    console.error("Unexpected API response structure:", result);
                }
            } catch (error) {
                outputElement.innerHTML = `Error: Failed to connect to AI. Details: ${error.message}`;
                console.error("Gemini API call failed:", error);
            } finally {
                buttonElement.innerHTML = originalButtonText; // Restore button text
                buttonElement.disabled = false; // Re-enable button
            }
        }


        /**
         * Displays the detailed information about an AWS AI service in a modal.
         * @param {string} serviceId - The ID of the service to display.
         */
        function showServiceDetails(serviceId) {
            const data = serviceData[serviceId];
            if (!data) return; // Exit if service data is not found

            const modalContent = document.getElementById('modalContent');
            // Populate the modal with service details using template literals
            modalContent.innerHTML = `
                <h2>${data.name}</h2>
                <p class="service-desc">${data.description}</p>
                <div style="display: flex; gap: 10px; margin-top: 10px;">
                    <button class="llm-action-button" onclick="summarizeServiceDescription('${data.description}', this)">Summarize Service Description ‚ú®</button>
                    <button class="llm-action-button" onclick="generateUseCasesIdea('${data.name}', '${data.description}', this)">Generate Use Cases Idea ‚ú®</button>
                </div>
                <div id="llm-service-output" class="llm-output-area" style="display: none;"></div>
                <hr style="margin: 20px 0; border-color: rgba(255,255,255,0.1);">

                <h3>üåü Key Features (click to expand)</h3>
                <div class="features">
                    ${data.features.map(feature => 
                        // Generate feature tags, replacing spaces for valid IDs in toggleFeatureDetails
                        `<span class="feature-tag" onclick="toggleFeatureDetails('${serviceId}', '${feature}')">${feature}</span>`
                    ).join('')}
                </div>
                ${data.features.map(feature => `
                    <!-- Feature details section for each feature -->
                    <div id="feature-details-${serviceId}-${feature.replace(/\s/g, '-')}" class="feature-details">
                        ${data.featureDetails[feature] && data.featureDetails[feature].subFeatures && data.featureDetails[feature].subFeatures.length > 0 ?
                            data.featureDetails[feature].subFeatures.map(sf => `
                                <div class="sub-feature">
                                    <div class="sub-feature-title">${sf.title}</div>
                                    <div class="sub-feature-desc">${sf.desc}</div>
                                </div>
                            `).join('')
                            : '<p>No specific details available for this feature.</p>'}
                        
                        ${data.featureDetails[feature] && data.featureDetails[feature].navigation ? `
                            <div class="feature-nav">
                                <strong>Console Navigation:</strong> ${data.featureDetails[feature].navigation}
                            </div>
                        ` : ''}
                        ${data.featureDetails[feature] && data.featureDetails[feature].options && data.featureDetails[feature].options.length > 0 ? `
                            <div class="feature-nav">
                                <strong>Key Configuration Options:</strong>
                                <ul class="config-options">
                                    ${data.featureDetails[feature].options.map(option => `<li>${option}</li>`).join('')}
                                </ul>
                            </div>
                        ` : ''}
                    </div>
                `).join('')}
                <hr style="margin: 20px 0; border-color: rgba(255,255,255,0.1);">

                <h3>ü§ù Interdependent Services</h3>
                <div class="interdependencies">
                    <ul>
                        ${data.interdependencies.map(dep => `<li>${dep}</li>`).join('')}
                    </ul>
                </div>
                <hr style="margin: 20px 0; border-color: rgba(255,255,255,0.1);">

                <h3>üíª Tech Stack / Integrations</h3>
                <div class="tech-stack">
                    ${data.techStack.map(tech => `<span class="feature-tag">${tech}</span>`).join('')}
                </div>
                <hr style="margin: 20px 0; border-color: rgba(255,255,255,0.1);">

                <h3>üí≤ Pricing Model</h3>
                <div class="pricing-info">
                    ${data.pricing} <a href="https://aws.amazon.com/pricing/${serviceId}" target="_blank" style="color: #87ceeb; text-decoration: underline;">Learn More</a>
                </div>
                <hr style="margin: 20px 0; border-color: rgba(255,255,255,0.1);">

                <h3>üéØ Common Scenarios</h3>
                <div class="scenario-box">
                    <ul>
                        ${data.scenarios.map(scenario => `<li>${scenario}</li>`).join('')}
                    </ul>
                </div>
            `;
            document.getElementById('serviceModal').style.display = 'block'; // Show the modal
        }

        /**
         * Toggles the visibility of specific feature details within the service modal.
         * Ensures only one feature detail section is open at a time.
         * @param {string} serviceId - The ID of the current service.
         * @param {string} featureName - The name of the feature to toggle.
         */
        function toggleFeatureDetails(serviceId, featureName) {
            // Construct the ID for the specific feature details element
            const featureId = `feature-details-${serviceId}-${featureName.replace(/\s/g, '-')}`;
            const featureDetailsElement = document.getElementById(featureId);

            // Get all feature details elements within the current modal
            // This assumes the feature-details are siblings or within a common parent
            // that is reset when the modal content is re-rendered (which it is)
            const allFeatureDetailsInModal = document.querySelectorAll('#modalContent .feature-details');

            // If the clicked element is already active, close it and return
            if (featureDetailsElement.classList.contains('active')) {
                featureDetailsElement.classList.remove('active');
                return;
            }

            // Close all other active feature details elements in the modal
            allFeatureDetailsInModal.forEach(element => {
                if (element.classList.contains('active')) {
                    element.classList.remove('active');
                }
            });

            // Now, open the clicked feature details element
            featureDetailsElement.classList.add('active');
        }

        /**
         * Closes the service details modal.
         */
        function closeModal() {
            document.getElementById('serviceModal').style.display = 'none';
            // Optionally, clear modal content or reset state when closing
            document.getElementById('modalContent').innerHTML = '';
        }

        // Close modal when clicking outside of it
        window.onclick = function(event) {
            const modal = document.getElementById('serviceModal');
            if (event.target == modal) {
                closeModal(); // Use the existing closeModal function
            }
        }

        // LLM specific functions
        /**
         * Summarizes the given service description using the Gemini API.
         * @param {string} description - The service description to summarize.
         * @param {HTMLElement} buttonElement - The button element that triggered the call.
         */
        async function summarizeServiceDescription(description, buttonElement) {
            const prompt = `Summarize the following AWS AI service description concisely in 2-3 sentences:\n\n"${description}"`;
            await callGeminiAPI(prompt, 'llm-service-output', buttonElement);
        }

        /**
         * Generates new use cases for a given AWS AI service using the Gemini API.
         * @param {string} serviceName - The name of the AWS AI service.
         * @param {string} serviceDescription - The description of the AWS AI service.
         * @param {HTMLElement} buttonElement - The button element that triggered the call.
         */
        async function generateUseCasesIdea(serviceName, serviceDescription, buttonElement) {
            const prompt = `Given the AWS AI service "${serviceName}" with the description "${serviceDescription}", suggest 3-4 creative and distinct real-world use cases or applications not already mentioned, in bullet point format.`;
            await callGeminiAPI(prompt, 'llm-service-output', buttonElement);
        }

        /**
         * Generates a detailed explanation for a given AI/ML technology using the Gemini API.
         * @param {string} techName - The name of the AI/ML technology.
         * @param {string} techDescription - The brief description of the technology.
         * @param {string} outputElementId - The ID of the HTML element where the response should be displayed.
         * @param {HTMLElement} buttonElement - The button element that triggered the call.
         */
        async function generateTechExplanation(techName, techDescription, outputElementId, buttonElement) {
            const prompt = `Provide a detailed explanation of "${techName}" (${techDescription}), including its core concepts, how it works, and its significance in modern AI/ML. Focus on clarity and comprehensive information.`;
            await callGeminiAPI(prompt, outputElementId, buttonElement);
        }

        /**
         * Fetches and displays the latest AWS AI/ML news using Gemini AI for a "deep search" (simulated).
         * In a real deployment outside Canvas, this would require a backend service
         * to call a search API (e.g., Google Search API) with the query,
         * and then potentially use Gemini to summarize/extract key info from results.
         */
        async function fetchAINews() {
            const newsOutputElement = document.getElementById('news-list');
            newsOutputElement.innerHTML = '<div class="loading-indicator"></div> Fetching latest AWS AI/ML news via Gemini AI...';

            const prompt = `Find and summarize at least 10 recent news articles (published within the last 3-6 months) related to AWS AI and Machine Learning services, advancements, or significant use cases. For each article, provide the title, a 1-2 sentence summary, and a placeholder URL. Ensure the summaries are concise and informative.`;
            
            try {
                // Simulate a call to a backend service that would perform the deep search with Gemini/Google Search.
                // In a real application, you'd fetch from your own API endpoint, e.g., /api/aws-ai-news
                // This 'await new Promise' simulates network delay.
                const simulatedNewsResponse = await new Promise(resolve => {
                    setTimeout(() => {
                        // Dummy data as if returned from a Gemini-powered search backend.
                        // This structure mimics what Gemini could extract/summarize from search results.
                        const articles = [
                            { title: "AWS Announces General Availability of Amazon Bedrock Agents for Streamlined Gen AI Applications", summary: "AWS Bedrock Agents simplify the creation of generative AI applications by orchestrating complex tasks, connecting to data sources, and invoking APIs, allowing developers to build more dynamic and intelligent solutions faster.", url: "https://aws.amazon.com/blogs/aws/amazon-bedrock-agents/" },
                            { title: "Amazon SageMaker Canvas Now Supports Text-to-Image Generation with Foundation Models", summary: "SageMaker Canvas empowers business analysts and citizen data scientists to generate images from text prompts directly within a visual interface, democratizing access to generative AI without writing code.", url: "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-canvas-now-supports-text-to-image-generation/" },
                            { title: "AWS Introduces New Capabilities for Amazon Rekognition to Enhance Content Moderation", summary: "Rekognition's content moderation features receive updates, including improved accuracy for detecting harmful content and more granular control over moderation labels, critical for safeguarding user-generated content.", url: "https://aws.amazon.com/rekognition/new-features/" },
                            { title: "Amazon CodeWhisperer Offers New Security Scans to Identify Code Vulnerabilities", summary: "CodeWhisperer now includes enhanced security scanning capabilities that identify potential vulnerabilities in code suggestions and existing projects, providing real-time remediation advice to developers.", url: "https://aws.amazon.com/codewhisperer/" },
                            { title: "AWS Expands Machine Learning Training Options with New EC2 Instances and Pricing Models", summary: "AWS introduces new EC2 instance types optimized for machine learning training, along with flexible pricing options, allowing customers to cost-effectively scale their deep learning workloads.", url: "https://aws.amazon.com/ec2/instance-types/ml/" },
                            { title: "Amazon Comprehend Medical Updates for Improved PHI Detection and De-identification", summary: "Comprehend Medical receives enhancements to its ability to accurately detect and de-identify Protected Health Information (PHI) within clinical text, further supporting HIPAA compliance in healthcare applications.", url: "https://aws.amazon.com/blogs/machine-learning/amazon-comprehend-medical-updates/" },
                            { title: "AWS Lambda Launches Support for Larger Container Images for AI Model Deployments", summary: "Lambda now allows deploying functions packaged as container images up to 10 GB, enabling developers to run more complex and larger AI/ML models as serverless functions.", url: "https://aws.amazon.com/blogs/aws/aws-lambda-container-images-10gb/" },
                            { title: "Amazon Forecast Introduces Inter-Item Demand Forecasting for Retailers", summary: "New features in Amazon Forecast enable the prediction of demand for items that influence each other, providing more accurate inventory and supply chain planning for businesses with complex product relationships.", url: "https://aws.amazon.com/blogs/machine-learning/amazon-forecast-inter-item-demand/" },
                            { title: "AWS DeepRacer Evolves with Multi-Agent Reinforcement Learning for Competitive AI", summary: "DeepRacer, AWS's autonomous racing league, introduces new multi-agent reinforcement learning features, allowing developers to train and compete with more sophisticated AI models.", url: "https://aws.amazon.com/deepracer/" },
                            { title: "Amazon Translate Improves Language Support and Custom Terminology Management", summary: "Amazon Translate expands its supported languages and enhances tools for managing custom terminology, ensuring more accurate and consistent translations for global content.", url: "https://aws.amazon.com/translate/" },
                            { title: "AWS Announces New Responsible AI Resources and Best Practices Guide", description: "AWS releases updated documentation and tools to guide developers in building AI applications responsibly, focusing on fairness, privacy, and transparency, and aligning with ethical AI principles.", source: "AWS Responsible AI", url: "https://aws.amazon.com/machine-learning/responsible-ai/" }
                        ];
                        resolve({ articles });
                    }, 1500); // Simulate 1.5 second network delay
                });

                if (simulatedNewsResponse.articles && simulatedNewsResponse.articles.length > 0) {
                    newsOutputElement.innerHTML = simulatedNewsResponse.articles.map(article => `
                        <li class="news-item">
                            <h3><a href="${article.url}" target="_blank">${article.title}</a></h3>
                            <p>${article.summary}</p>
                            <p style="font-size: 0.75rem; color: #ccc;">Source: ${article.source || 'Various'} | Published: ${article.date || 'Recent'}</p>
                        </li>
                    `).join('');
                } else {
                    newsOutputElement.innerHTML = '<p>No AWS AI news found at this time. Please try again later.</p>';
                }
            } catch (error) {
                newsOutputElement.innerHTML = `<p style="color: #ffcccc;">Error fetching news: ${error.message}. Displaying placeholder data.</p>`;
                console.error("Failed to fetch news from simulated backend:", error);
            }
        }

        /**
         * Performs a search using Gemini AI and displays the result in the floating search area.
         */
        async function performAISearch() {
            const searchInput = document.getElementById('ai-search-input');
            const searchOutput = document.getElementById('ai-search-output');
            const searchButton = document.getElementById('ai-search-button');
            const userQuery = searchInput.value.trim();

            if (!userQuery) {
                searchOutput.innerHTML = '<p style="color: #ffcccc;">Please enter a search query.</p>';
                searchOutput.style.display = 'block';
                return;
            }

            const prompt = `As a concise AI search assistant focused *only* on AWS AI/ML services and related advancements, provide a relevant and helpful answer to the following query: "${userQuery}". Keep the answer to a maximum of 3-4 sentences. If the query is outside the scope of AWS AI or its advancements, state that politely.`;
            await callGeminiAPI(prompt, 'ai-search-output', searchButton);
        }

        /**
         * Toggles the visibility and expanded state of the floating AI search bar.
         */
        function toggleFloatingSearch() {
            const floatingSearch = document.getElementById('floatingAISearch');
            floatingSearch.classList.toggle('expanded');

            const searchIcon = document.getElementById('searchIcon');
            if (floatingSearch.classList.contains('expanded')) {
                searchIcon.textContent = '‚úñ'; // Change icon to close
                document.getElementById('ai-search-input').focus(); // Focus on input when expanded
            } else {
                searchIcon.textContent = 'üîç'; // Change icon to search
                document.getElementById('ai-search-output').style.display = 'none'; // Hide output when collapsed
                document.getElementById('ai-search-output').innerHTML = ''; // Clear output when collapsed
                document.getElementById('ai-search-input').value = ''; // Clear input when collapsed
            }
        }

        // Draggable functionality for the floating search bar
        let isDragging = false;
        let offset = { x: 0, y: 0 };
        const floatingSearchElement = document.getElementById('floatingAISearch');

        floatingSearchElement.addEventListener('mousedown', (e) => {
            // Only start dragging if the bar is not expanded or if the click is on the toggle button itself
            if (!floatingSearchElement.classList.contains('expanded') || e.target.closest('.search-toggle-button')) {
                isDragging = true;
                offset = {
                    x: e.clientX - floatingSearchElement.getBoundingClientRect().left,
                    y: e.clientY - floatingSearchElement.getBoundingClientRect().top
                };
                floatingSearchElement.style.cursor = 'grabbing';
            }
        });

        document.addEventListener('mousemove', (e) => {
            if (!isDragging) return;

            // Calculate new position
            let newX = e.clientX - offset.x;
            let newY = e.clientY - offset.y;

            // Get viewport dimensions to constrain dragging
            const viewportWidth = window.innerWidth;
            const viewportHeight = window.innerHeight;
            const elementWidth = floatingSearchElement.offsetWidth;
            const elementHeight = floatingSearchElement.offsetHeight;

            // Constrain newX to stay within viewport bounds
            newX = Math.max(0, Math.min(newX, viewportWidth - elementWidth));
            // Constrain newY to stay within viewport bounds
            newY = Math.max(0, Math.min(newY, viewportHeight - elementHeight));

            floatingSearchElement.style.left = `${newX}px`;
            floatingSearchElement.style.top = `${newY}px`;
            floatingSearchElement.style.right = 'auto'; // Disable right/bottom positioning when dragging via left/top
            floatingSearchElement.style.bottom = 'auto';
        });

        document.addEventListener('mouseup', () => {
            isDragging = false;
            floatingSearchElement.style.cursor = floatingSearchElement.classList.contains('expanded') ? 'auto' : 'grab';
        });


        // Initialize fetching news on page load
        document.addEventListener('DOMContentLoaded', fetchAINews);

        // Remove onclick from central node as showOverview() is not defined
        document.querySelector('.central-node').onclick = null;
    </script>
</body>
</html>
