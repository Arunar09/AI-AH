<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Hierarchy Chart with AWS Integration</title>
    <!-- Tailwind CSS for modern styling and responsiveness -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- D3.js library for powerful data visualization, including tree layouts -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <!-- Google Fonts - Inter for a clean, modern typeface -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        /* Base styles for the body and overall container */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f4f8; /* Light gray background */
            color: #334155; /* Dark slate text */
            overflow: hidden; /* Prevent body scroll, let SVG handle pan */
            margin: 0;
            padding: 0;
        }
        /* Main container to hold header, chart, and description panel */
        .container {
            display: flex;
            flex-direction: column;
            height: 100vh; /* Full viewport height */
            overflow: hidden;
			max-width:100% !important
        }
        /* Header styling */
        .header {
            background-color: #1e293b; /* Dark slate background */
            color: #f8fafc; /* Off-white text */
            padding: 1rem 2rem;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.5rem;
        }
        /* Search bar styling */
        .search-bar {
            display: flex;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        .search-bar input {
            padding: 0.5rem 0.75rem;
            border-radius: 0.5rem;
            border: 1px solid #64748b;
            background-color: #f8fafc;
            color: #1e293b;
            width: 200px;
        }
        .search-bar button {
            padding: 0.5rem 1rem;
            background-color: #3b82f6;
            color: white;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: background-color 0.2s ease-in-out;
        }
        .search-bar button:hover {
            background-color: #2563eb;
        }

        /* Breadcrumb styling */
        .breadcrumbs {
            padding: 0.75rem 1rem;
            background-color: #cbd5e1; /* Light gray-blue */
            color: #475569; /* Darker gray-blue */
            font-size: 0.875rem;
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .breadcrumbs span {
            margin: 0 0.25rem;
        }
        .breadcrumbs a {
            color: #1e40af; /* Darker blue for links */
            text-decoration: none;
            font-weight: 500;
            cursor: pointer;
        }
        .breadcrumbs a:hover {
            text-decoration: underline;
        }
        .breadcrumbs .current-node {
            font-weight: 700;
            color: #1e293b;
        }

        /* Export buttons container */
        .export-options {
            display: flex;
            gap: 0.5rem;
            margin-top: 1rem;
        }
        .export-options button {
            padding: 0.5rem 1rem;
            background-color: #10b981; /* Green-500 */
            color: white;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: background-color 0.2s ease-in-out;
        }
        .export-options button:hover {
            background-color: #059669; /* Green-600 */
        }


        /* Area for chart and description, uses flexbox */
        .content-area {
            flex-grow: 1; /* Takes remaining vertical space */
            display: flex;
            flex-direction: column; /* Default to column for mobile layout */
            overflow: hidden;
        }
        /* Chart container styling */
        .chart-container {
            flex-grow: 1; /* Takes remaining space within content-area */
            background-color: #ffffff; /* White background */
            border-radius: 0.75rem; /* Rounded corners */
            box-shadow: 0 4px 12px rgba(0,0,0,0.08); /* Soft shadow */
            margin: 1rem;
            overflow: hidden; /* Ensures SVG stays within bounds */
            position: relative; /* Needed for absolute positioning of tooltips */
        }
        /* Description panel styling */
        .description-panel {
            background-color: #e2e8f0; /* Light blue-gray background */
            border-radius: 0.75rem; /* Rounded corners */
            box-shadow: 0 4px 12px rgba(0,0,0,0.08); /* Soft shadow */
            margin: 1rem;
            padding: 1.5rem;
            width: auto; /* Full width on mobile */
            max-height: 30vh; /* Limit height on mobile to prevent overflow */
            overflow-y: auto; /* Enable vertical scrolling if content exceeds max-height */
            transition: all 0.3s ease-in-out; /* Smooth transition for active state */
            opacity: 0; /* Hidden by default */
            transform: translateY(20px); /* Slightly offset for animation */
            pointer-events: none; /* Prevents interaction when hidden */
            display: flex; /* Use flexbox for content and button */
            flex-direction: column;
        }
        /* Active state for description panel - visible and interactive */
        .description-panel.active {
            opacity: 1;
            transform: translateY(0);
            pointer-events: auto;
        }

        /* Responsive layout for desktop (min-width: 1024px) */
        @media (min-width: 1024px) {
            .content-area {
                flex-direction: row; /* Change to row for side-by-side layout */
            }
            .chart-container {
                flex: 3; /* Chart takes 3 parts of available space */
                margin-right: 0.5rem;
            }
            .description-panel {
                flex: 1; /* Description panel takes 1 part of available space */
                margin-left: 0.5rem;
                max-height: none; /* Remove height limit on desktop */
            }
        }

        /* D3 Tree Specific Styling */
        /* Node circles */
        .node circle {
            fill: #60a5fa; /* Blue-500 */
            stroke: #3b82f6; /* Blue-600 */
            stroke-width: 2px;
            cursor: pointer; /* Indicates interactivity */
            transition: fill 0.2s ease-in-out; /* Smooth color transition on hover/click */
        }
        /* Styling for the root node circle */
        .node circle.root {
            fill: #ef4444; /* Red-500 */
            stroke: #dc2626; /* Red-600 */
        }
        /* Styling for collapsed node circles (indicating hidden children) */
        .node circle.collapsed {
            fill: #a8a29e; /* Stone-400 */
            stroke: #78716c; /* Stone-500 */
        }
        /* Node text labels */
        .node text {
            font-size: 10px;
            fill: #1f2937; /* Gray-800 */
            text-anchor: middle; /* Center text horizontally */
            pointer-events: none; /* Allows mouse events to pass through to the circle */
            font-weight: 600; /* Semi-bold text */
        }
        /* Styling for the links (connecting lines between nodes) */
        .link {
            fill: none;
            stroke: #94a3b8; /* Slate-400 */
            stroke-width: 1.5px;
            opacity: 0.7;
        }
        /* Tooltip for node names on hover */
        .tooltip {
            position: absolute;
            text-align: center;
            padding: 8px;
            font-size: 12px;
            background: #1e293b; /* Dark slate background */
            color: white;
            border: 0px;
            border-radius: 8px;
            pointer-events: none; /* Prevents tooltip from interfering with mouse events */
            opacity: 0; /* Hidden by default */
            transition: opacity 0.2s; /* Smooth fade-in/out */
            z-index: 1000; /* Ensures tooltip appears on top */
        }

        /* Styles for the full content modal */
        #full-content-modal.hidden {
            display: none;
        }
        #full-content-modal {
            /* Tailwind classes handle most of this now, but explicitly setting display */
            display: flex;
        }
        /* Ensure modal content scrolls if too long */
        #modal-body {
            -webkit-overflow-scrolling: touch; /* Smooth scrolling on iOS */
        }

        /* Highlight for searched node */
        .node.highlight circle {
            stroke: #f97316; /* Orange-500 */
            stroke-width: 4px;
            animation: pulse-highlight 1.5s infinite alternate;
        }

        @keyframes pulse-highlight {
            from {
                stroke-opacity: 1;
                transform: scale(1);
            }
            to {
                stroke-opacity: 0.5;
                transform: scale(1.1);
            }
        }

        /* Custom message box for alerts */
        #message-box {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background-color: #fff;
            padding: 1.5rem;
            border-radius: 0.75rem;
            box-shadow: 0 8px 16px rgba(0,0,0,0.2);
            z-index: 10000;
            display: none; /* Hidden by default */
            text-align: center;
            font-size: 1rem;
            color: #334155;
        }
        #message-box button {
            margin-top: 1rem;
            padding: 0.5rem 1rem;
            background-color: #3b82f6;
            color: white;
            border-radius: 0.5rem;
            font-weight: 600;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <h1 class="text-3xl font-bold">AI Hierarchy Chart with AWS Integration</h1>
            <p class="text-sm text-gray-300 mt-1">Explore the interconnected world of Artificial Intelligence and Machine Learning, with insights into AWS implementations.</p>
            <div class="search-bar">
                <input type="text" id="search-input" placeholder="Search node name..." class="focus:ring-blue-500 focus:border-blue-500">
                <button id="search-button">Search</button>
            </div>
            <div class="export-options">
                <button id="export-svg-button">Export SVG</button>
            </div>
        </header>

        <div id="breadcrumbs" class="breadcrumbs">
            <!-- Breadcrumbs will be dynamically inserted here -->
			
        </div>

        <div class="content-area">
            <!-- Container for the D3.js tree visualization -->
            <div class="chart-container">
                <svg id="tree-chart" class="w-full h-full"></svg>
            </div>
            <!-- Panel to display detailed descriptions of selected nodes -->
            <div id="description-panel" class="description-panel">
                <h2 id="description-title" class="text-xl font-semibold mb-2 text-gray-800">Select a Node</h2>
                <p id="description-content" class="text-gray-700 text-sm">Click on any node in the hierarchy chart to view its detailed explanation, including how it can be achieved using AWS AI/ML services.</p>
                <!-- "Read More" button and "Copy Markdown" button will be appended here by JavaScript -->
            </div>
        </div>
    </div>

    <!-- Full Content Modal -->
    <div id="full-content-modal" class="fixed inset-0 bg-gray-900 bg-opacity-75 flex items-center justify-center z-50 hidden p-4">
        <div class="bg-white rounded-lg shadow-xl w-full max-w-3xl lg:max-w-4xl max-h-[90vh] overflow-hidden flex flex-col">
            <div class="flex justify-between items-center p-4 border-b border-gray-200">
                <h3 id="modal-title" class="text-xl font-bold text-gray-800"></h3>
                <button id="close-modal" class="text-gray-500 hover:text-gray-700 text-2xl font-bold">&times;</button>
            </div>
            <div id="modal-body" class="p-4 flex-grow overflow-y-auto text-gray-700 text-sm">
                <!-- Content will be injected here -->
            </div>
        </div>
    </div>

    <!-- Custom Message Box -->
    <div id="message-box" class="hidden">
        <p id="message-content"></p>
        <button id="message-ok-button">OK</button>
    </div>

    <script>
        // Define the hierarchical data for the AI chart
        const treeData = {
          "name": "Artificial Intelligence (AI)",
          "description": `The broad scientific field concerned with building intelligent machines capable of performing tasks that typically require human intelligence, such as learning, problem-solving, perception, reasoning, and understanding language. It aims to create systems that can think, learn, and act autonomously.

### AWS Implementation for AI
AWS offers a comprehensive suite of AI services, broadly categorized into:
1.  **AI Services (Pre-trained Models):** Ready-to-use APIs for common AI tasks (e.g., Amazon Rekognition for Computer Vision, Amazon Comprehend for NLP, Amazon Polly for Text-to-Speech). These require no ML expertise.
2.  **ML Services (Build, Train, Deploy):** Tools for data scientists and developers to build, train, and deploy custom ML models (e.g., Amazon SageMaker).
3.  **ML Frameworks & Infrastructure:** Low-level access to compute (EC2, GPUs), storage (S3), and popular ML frameworks (TensorFlow, PyTorch) for advanced users.

---

#### How to Enable/Config
* **AWS Account:** You need an AWS account.
* **IAM Permissions:** Ensure your AWS user or role has the necessary IAM permissions (e.g., \`AmazonSageMakerFullAccess\`, \`AmazonRekognitionFullAccess\`, \`AmazonS3FullAccess\`).
* **AWS Console:** Log in to the AWS Management Console to access services.
* **AWS CLI:** Install and configure the AWS CLI (\`aws configure\`) with your access key ID, secret access key, default region, and output format.

#### Key Features
* **Scalability:** Easily scale compute and storage resources.
* **Managed Services:** Reduce operational overhead for ML infrastructure.
* **Integration:** Seamless integration with other AWS services (S3, Lambda, EC2).
* **Cost-Effectiveness:** Pay-as-you-go pricing model.

#### Parameters/Dependencies
* **Data:** Data stored in Amazon S3 is a common dependency for many ML workflows.
* **IAM Roles:** Services often require IAM roles with specific permissions to access other AWS resources.
* **VPC Configuration:** For private network access, services might need to be configured within a Virtual Private Cloud (VPC).

#### Tool Navigations (AWS Console)
* Log in to the AWS Console.
* Use the search bar at the top to find specific services (e.g., "SageMaker", "Rekognition", "Comprehend").
* Navigate through the "Machine Learning" or "AI Services" sections in the services menu.

#### CLI & Output Syntax Examples (General AWS CLI)
\`\`\`bash
# Configure AWS CLI (if not already done)
aws configure

# Example: List S3 buckets (common data source)
aws s3 ls

# Example: Describe SageMaker endpoints
aws sagemaker list-endpoints
\`\`\`

#### Common Issues and Solutions
* **Issue: Insufficient Permissions.**
    * **Solution:** Check IAM policies attached to your user or role. Ensure required actions (e.g., \`sagemaker:CreateTrainingJob\`, \`s3:GetObject\`) are allowed.
* **Issue: Region Mismatch.**
    * **Solution:** Ensure your CLI region (\`aws configure\`) matches the region where your resources are deployed or where you intend to deploy them.
* **Issue: Service Limits.**
    * **Solution:** AWS services have default limits. If you hit a limit (e.g., number of concurrent training jobs), request a limit increase through the AWS Support Center.
* **Issue: Cost Management.**
    * **Solution:** Monitor costs using AWS Cost Explorer. Set up billing alarms. Stop unused instances/endpoints.`,
          "children": [
            {
              "name": "Machine Learning (ML)",
              "description": `A core subfield of AI that focuses on enabling systems to learn from data without being explicitly programmed. ML algorithms identify patterns and make predictions or decisions based on training data. It's about building models that can generalize from observed data.

### AWS Implementation for Machine Learning
AWS provides a full spectrum of ML capabilities, from high-level AI services to low-level infrastructure:
* **Amazon SageMaker:** A fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. It includes SageMaker Studio, Notebook Instances, Training, Hosting, Ground Truth (data labeling), Feature Store, and more.
* **AWS Glue:** For ETL (Extract, Transform, Load) operations on data before ML training.
* **Amazon S3:** Scalable object storage, typically used for storing datasets and model artifacts.
* **AWS Step Functions/Lambda:** For orchestrating ML workflows and serverless inference.

---

#### How to Enable/Config (SageMaker Example)
* **AWS Console:** Navigate to Amazon SageMaker.
* **SageMaker Studio:** Recommended environment. Launch Studio from the SageMaker dashboard.
* **Notebook Instances:** Create a Notebook Instance (e.g., \`ml.t3.medium\`) for development.
* **IAM Role:** SageMaker requires an IAM role with permissions to access S3, CloudWatch, and other necessary services. SageMaker can create a default role for you.

#### Key Features (SageMaker)
* **Managed Notebooks:** Jupyter notebooks for interactive development.
* **Managed Training:** Scalable training with built-in algorithms or custom code.
* **Managed Hosting:** Deploy models as API endpoints for real-time or batch inference.
* **Hyperparameter Tuning:** Automate the search for optimal model parameters.
* **Data Labeling (Ground Truth):** Create high-quality training datasets.
* **Feature Store:** Centralized repository for ML features.

#### Parameters/Dependencies (SageMaker)
* **Input Data:** S3 path to training data.
* **Output Location:** S3 path to store model artifacts and output.
* **Instance Types:** Specify compute instance types for training and inference (e.g., \`ml.m5.xlarge\`, \`ml.g4dn.xlarge\` for GPU).
* **IAM Role ARN:** The Amazon Resource Name (ARN) of the IAM role SageMaker uses.
* **Container Image:** For custom algorithms, specify a Docker image URI.

#### Tool Navigations (AWS Console)
* **SageMaker:** Services -> Machine Learning -> Amazon SageMaker.
* **SageMaker Studio:** From SageMaker dashboard, click "Launch Studio".
* **Notebook Instances:** SageMaker -> Notebook -> Notebook instances.
* **Training Jobs:** SageMaker -> Training -> Training jobs.
* **Endpoints:** SageMaker -> Inference -> Endpoints.

#### CLI & Output Syntax Examples (SageMaker)
\`\`\`bash
# Create a SageMaker Notebook Instance
aws sagemaker create-notebook-instance \\
    --notebook-instance-name my-ml-notebook \\
    --instance-type ml.t3.medium \\
    --role-arn arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole-20231026T123456

# Start a SageMaker Notebook Instance
aws sagemaker start-notebook-instance \\
    --notebook-instance-name my-ml-notebook

# Stop a SageMaker Notebook Instance
aws sagemaker stop-notebook-instance \\
    --notebook-instance-name my-ml-notebook

# Example: Create a training job (simplified)
aws sagemaker create-training-job \\
    --training-job-name my-xgboost-job \\
    --hyper-parameters '{"eta": "0.2", "objective": "binary:logistic"}' \\
    --algorithm-specification TrainingImage="<your-xgboost-image-uri>",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole-20231026T123456 \\
    --input-data-config '[{"ChannelName": "train", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-bucket/train/"}}}]' \\
    --output-data-config S3OutputPath="s3://your-bucket/output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.m5.xlarge,VolumeSizeInGB=50 \\
    --stopping-condition MaxRuntimeInSeconds=3600
\`\`\`

#### Common Issues and Solutions (SageMaker)
* **Issue: Notebook Instance stuck in 'Pending' or 'Failed' status.**
    * **Solution:** Check CloudWatch logs for the notebook instance. Often due to incorrect IAM permissions, VPC configuration issues, or service limits. Ensure the IAM role has permissions to create ENIs (Elastic Network Interfaces) if in a VPC.
* **Issue: Training Job fails.**
    * **Solution:** Examine the SageMaker training job logs in CloudWatch. Common causes include incorrect data paths, insufficient IAM permissions for S3 access, errors in your training script, or out-of-memory issues on the chosen instance type.
* **Issue: Endpoint invocation errors.**
    * **Solution:** Check the endpoint's CloudWatch logs. Verify the input payload format matches what your model expects. Ensure the endpoint instance has sufficient resources and the model artifact is correctly loaded.
* **Issue: Data access denied.**
    * **Solution:** Confirm the IAM role used by SageMaker has \`s3:GetObject\` and \`s3:PutObject\` permissions on the relevant S3 buckets and prefixes. Check bucket policies.`,
              "children": [
                {
                  "name": "Supervised Learning",
                  "description": `Algorithms learn from labeled training data (input-output pairs) to map new inputs to desired outputs. The model is 'supervised' by having access to the correct answers during training.

### AWS Implementation for Supervised Learning
AWS SageMaker provides robust support for supervised learning tasks. You can use:
* **SageMaker Built-in Algorithms:** Many supervised algorithms like XGBoost, Linear Learner, Factorization Machines, K-Nearest Neighbors (k-NN) are available out-of-the-box.
* **Custom Algorithms:** Bring your own code (e.g., Python scripts with scikit-learn, TensorFlow, PyTorch) in a Docker container.
* **Amazon Rekognition/Comprehend/Forecast:** High-level AI services that internally use supervised learning for specific tasks like image classification, sentiment analysis, or time-series forecasting.

---

#### How to Enable/Config (SageMaker Built-in Algorithm Example - XGBoost)
1.  **Prepare Data:** Ensure your labeled dataset is in S3, typically in CSV or LibSVM format. For XGBoost, data should be in a format suitable for the algorithm (e.g., CSV with target variable as the first column).
2.  **SageMaker Console/SDK:**
    * **Console:** Navigate to SageMaker -> Training -> Training jobs -> Create training job.
    * **SDK (Python):** Use the SageMaker Python SDK in a Notebook Instance.

#### Key Features
* **Managed Training:** SageMaker handles infrastructure for training.
* **Hyperparameter Optimization (HPO):** Automate tuning for best model performance.
* **Model Deployment:** Easily deploy trained models as endpoints.

#### Parameters/Dependencies (XGBoost Example)
* **Input Data S3 URI:** \`s3://your-bucket/train_data/\`
* **Output Model S3 URI:** \`s3://your-bucket/model_output/\`
* **Hyperparameters:** \`objective\` (e.g., \`binary:logistic\`, \`reg:squarederror\`), \`eta\`, \`max_depth\`, \`num_round\`, etc.
* **Instance Type:** \`ml.m5.xlarge\` (CPU) or \`ml.g4dn.xlarge\` (GPU for larger datasets/models).

#### Tool Navigations (AWS Console)
* SageMaker -> Training -> Training jobs -> Create training job.
* Select "Built-in algorithm" and choose "XGBoost".
* Configure input data, output location, instance types, and hyperparameters.

#### CLI & Output Syntax Examples (XGBoost Training Job)
\`\`\`bash
# Example: Create an XGBoost training job using AWS CLI
aws sagemaker create-training-job \\
    --training-job-name xgboost-classification-job \\
    --hyper-parameters '{"objective": "binary:logistic", "num_round": "100", "max_depth": "5"}' \\
    --algorithm-specification TrainingImage="<your-region>.dkr.ecr.<your-region>.amazonaws.com/sagemaker-xgboost:1.7-1",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/service-role/SageMakerExecutionRole \\
    --input-data-config '[{"ChannelName": "train", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/train-data/"}}, "ContentType": "text/csv"}]' \\
    --output-data-config S3OutputPath="s3://your-s3-bucket/model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.m5.xlarge,VolumeSizeInGB=30 \\
    --stopping-condition MaxRuntimeInSeconds=3600

# To get the ECR image URI for XGBoost in your region:
# aws sagemaker list-training-jobs --query "TrainingJobSummaries[?TrainingJobStatus=='Completed'].AlgorithmSpecification.TrainingImage" --output text | grep xgboost
\`\`\`

#### Common Issues and Solutions
* **Issue: Data format errors.**
    * **Solution:** Ensure your CSV data is clean, consistent, and the target variable is in the expected column (often the first for built-in algorithms). Verify no headers are present if the algorithm expects raw data.
* **Issue: Model performance is poor.**
    * **Solution:**
        * **Hyperparameter Tuning:** Use SageMaker Automatic Model Tuning (AMT) to find optimal hyperparameters.
        * **Feature Engineering:** Improve the quality and relevance of your input features.
        * **More Data:** Increase the size and diversity of your training data.
        * **Algorithm Choice:** Experiment with different algorithms.
* **Issue: Training job takes too long or is too expensive.**
    * **Solution:**
        * **Instance Type:** Choose an appropriate instance type. Smaller datasets might not need large GPU instances.
        * **Data Size:** Optimize data loading and preprocessing.
        * **Stopping Conditions:** Set \`MaxRuntimeInSeconds\` and \`MaxWaitTimeInSeconds\` to prevent runaway jobs.`,
                  "children": [
                    {
                      "name": "Classification",
                      "description": `Predicts a categorical output (e.g., spam/not spam, disease/no disease). The goal is to assign input data to one of several predefined classes.

### AWS Implementation for Classification
* **Amazon SageMaker:** Use built-in algorithms like XGBoost, Linear Learner, or custom models (TensorFlow, PyTorch, scikit-learn) for various classification tasks (binary, multi-class).
* **Amazon Rekognition:** For image and video classification (e.g., detecting objects, scenes).
* **Amazon Comprehend:** For text classification (e.g., categorizing documents, identifying spam).
* **Amazon Personalize:** For user-item classification (e.g., recommending products).

---

#### How to Enable/Config (SageMaker XGBoost for Classification)
See "Supervised Learning" section for general SageMaker setup. For classification, ensure your target variable is categorical and your chosen algorithm's objective is set accordingly (e.g., \`binary:logistic\` for binary, \`multi:softmax\` or \`multi:softprob\` for multi-class with XGBoost).

#### Key Features
* **Accuracy Metrics:** SageMaker training jobs provide metrics like accuracy, precision, recall, F1-score.
* **Model Explainability:** SageMaker Clarify helps understand model predictions.

#### Parameters/Dependencies
* **Objective Function:** Crucial for classification (e.g., \`binary:logistic\`, \`multi:softmax\`).
* **Number of Classes:** For multi-class, specify the number of classes.

#### Tool Navigations (AWS Console)
* SageMaker -> Training -> Training jobs.
* Rekognition -> Custom Labels (for custom image classification).
* Comprehend -> Custom Classification.

#### CLI & Output Syntax Examples (Rekognition Custom Labels)
\`\`\`bash
# Example: Create a Rekognition Custom Labels project
aws rekognition create-project --project-name "MyImageClassifier"

# Example: Start a Rekognition Custom Labels model
aws rekognition start-project-version \\
    --project-version-arn "arn:aws:rekognition:us-east-1:123456789012:project/MyImageClassifier/version/MyImageClassifier.2023-01-01T00.00.00/1672531200000" \\
    --min-inference-units 1

# Example: Detect custom labels in an image
aws rekognition detect-custom-labels \\
    --project-version-arn "arn:aws:rekognition:us-east-1:123456789012:project/MyImageClassifier/version/MyImageClassifier.2023-01-01T00.00.00/1672531200000" \\
    --image '{"S3Object":{"Bucket":"my-rekognition-bucket","Name":"test-image.jpg"}}'
\`\`\`

#### Common Issues and Solutions
* **Issue: Imbalanced classes.**
    * **Solution:** Use techniques like oversampling (SMOTE), undersampling, or adjust class weights in your training algorithm.
* **Issue: Low accuracy.**
    * **Solution:**
        * **Feature Engineering:** Create more informative features.
        * **Hyperparameter Tuning:** Optimize model parameters.
        * **More Data:** Increase labeled data.
        * **Model Selection:** Try different classification algorithms.
        * **Cross-validation:** Use cross-validation to get a more robust estimate of model performance.`,
                      "children": [
                        {"name": "Logistic Regression", "description": "A statistical model used for binary classification, estimating the probability of an instance belonging to a certain class. Despite its name, it's a classification algorithm."},
                        {"name": "Support Vector Machines (SVM)", "description": "A powerful algorithm for classification and regression, finding the optimal hyperplane that separates data points into different classes with the largest margin."},
                        {"name": "Decision Trees", "description": "A tree-like model where each internal node represents a 'test' on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (decision)."},
                        {"name": "Random Forest", "description": "An ensemble learning method that constructs a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees."},
                        {"name": "Naive Bayes", "description": "A probabilistic classifier based on Bayes' theorem with the 'naive' assumption of independence between features. Often used in text classification."},
                        {"name": "k-Nearest Neighbors (k-NN)", "description": "A non-parametric, instance-based learning algorithm used for both classification and regression. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors."}
                      ]
                    },
                    {
                      "name": "Regression",
                      "description": `Predicts a continuous numerical output (e.g., house prices, temperature). The goal is to model the relationship between input features and a continuous target variable.

### AWS Implementation for Regression
* **Amazon SageMaker:** Use built-in algorithms like Linear Learner, XGBoost (with regression objective), or custom models (TensorFlow, PyTorch, scikit-learn) for various regression tasks.
* **Amazon Forecast:** A fully managed service that uses machine learning to deliver highly accurate forecasts (time-series regression).

---

#### How to Enable/Config (SageMaker Linear Learner for Regression)
1.  **Prepare Data:** Data in S3, typically CSV, with the target variable as the first column.
2.  **SageMaker Console/SDK:** Use SageMaker to create a training job.

#### Key Features
* **Evaluation Metrics:** SageMaker training jobs provide metrics like RMSE (Root Mean Squared Error), MAE (Mean Absolute Error).
* **Scalability:** Train on large datasets with distributed training.

#### Parameters/Dependencies (Linear Learner Example)
* **Input Data S3 URI:** \`s3://your-bucket/train_data/\`
* **Output Model S3 URI:** \`s3://your-bucket/model_output/\`
* **Algorithm:** Linear Learner.
* **Hyperparameters:** \`predictor_type\` (e.g., \`regressor\`), \`mini_batch_size\`.

#### Tool Navigations (AWS Console)
* SageMaker -> Training -> Training jobs -> Create training job.
* Select "Built-in algorithm" and choose "Linear Learner".
* Configure input data, output location, instance types, and hyperparameters.

#### CLI & Output Syntax Examples (Linear Learner Training Job)
\`\`\`bash
# Example: Create a Linear Learner training job for regression
aws sagemaker create-training-job \\
    --training-job-name linear-regression-job \\
    --hyper-parameters '{"predictor_type": "regressor", "mini_batch_size": "100"}' \\
    --algorithm-specification TrainingImage="<your-region>.dkr.ecr.<your-region>.amazonaws.com/sagemaker-linear-learner:1.2-1",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/service-role/SageMakerExecutionRole \\
    --input-data-config '[{"ChannelName": "train", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/regression-data/"}}, "ContentType": "text/csv"}]' \\
    --output-data-config S3OutputPath="s3://your-s3-bucket/model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.m5.xlarge,VolumeSizeInGB=30 \\
    --stopping-condition MaxRuntimeInSeconds=3600
\`\`\`

#### Common Issues and Solutions
* **Issue: Underfitting or Overfitting.**
    * **Solution:**
        * **Underfitting:** Increase model complexity (e.g., add more features, use polynomial regression, or a more complex algorithm like XGBoost).
        * **Overfitting:** Regularization (Ridge, Lasso), more data, feature selection, or simpler models.
* **Issue: Poor R-squared or high RMSE.**
    * **Solution:**
        * **Feature Engineering:** Improve feature quality.
        * **Data Preprocessing:** Handle outliers, missing values, and scale features appropriately.
        * **Algorithm Selection:** Try different regression algorithms.`,
                      "children": [
                        {"name": "Linear Regression", "description": "A statistical method for modeling the relationship between a scalar dependent variable and one or more explanatory variables (or features) by fitting a linear equation to the observed data."},
                        {"name": "Polynomial Regression", "description": "A form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. Useful for non-linear relationships."},
                        {"name": "Ridge Regression", "description": "A regularization technique that adds a penalty term to the linear regression cost function to prevent overfitting by shrinking the coefficients towards zero."},
                        {"name": "Lasso Regression", "description": "Another regularization technique similar to Ridge, but it can shrink some coefficients to exactly zero, effectively performing feature selection."}
                      ]
                    }
                  ]
                },
                {
                  "name": "Unsupervised Learning",
                  "description": `Algorithms learn from unlabeled data, identifying hidden patterns or structures without prior knowledge of the output. It's used for exploratory data analysis and feature learning.

### AWS Implementation for Unsupervised Learning
* **Amazon SageMaker:** Provides built-in algorithms for clustering (K-Means) and dimensionality reduction (Principal Component Analysis - PCA). You can also implement custom unsupervised algorithms using popular ML frameworks.
* **Amazon Comprehend:** Can perform topic modeling on text data (a form of clustering).
* **Amazon Personalize:** Can discover user segments and item similarities (related to clustering/association).

---

#### How to Enable/Config (SageMaker K-Means Example)
1.  **Prepare Data:** Unlabeled data in S3, typically in CSV or RecordIO-protobuf format.
2.  **SageMaker Console/SDK:** Use SageMaker to create a training job.

#### Key Features
* **Managed Infrastructure:** SageMaker handles the compute for training.
* **Scalability:** Process large unlabeled datasets.

#### Parameters/Dependencies (K-Means Example)
* **Input Data S3 URI:** \`s3://your-bucket/unlabeled_data/\`
* **Output Model S3 URI:** \`s3://your-bucket/model_output/\`
* **Hyperparameters:** \`k\` (number of clusters), \`init_method\` (e.g., \`random\`, \`kmeans++\`), \`epochs\`.
* **Instance Type:** \`ml.m5.xlarge\` (CPU).

#### Tool Navigations (AWS Console)
* SageMaker -> Training -> Training jobs -> Create training job.
* Select "Built-in algorithm" and choose "K-Means".
* Configure input data, output location, instance types, and hyperparameters.

#### CLI & Output Syntax Examples (K-Means Training Job)
\`\`\`bash
# Example: Create a K-Means training job
aws sagemaker create-training-job \\
    --training-job-name kmeans-clustering-job \\
    --hyper-parameters '{"k": "5", "extra_center_factor": "2", "epochs": "10"}' \\
    --algorithm-specification TrainingImage="<your-region>.dkr.ecr.<your-region>.amazonaws.com/sagemaker-kmeans:1.2-1",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/service-role/SageMakerExecutionRole \\
    --input-data-config '[{"ChannelName": "train", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/unlabeled-data/"}}, "ContentType": "text/csv"}]' \\
    --output-data-config S3OutputPath="s3://your-s3-bucket/model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.m5.xlarge,VolumeSizeInGB=30 \\
    --stopping-condition MaxRuntimeInSeconds=3600
\`\`\`

#### Common Issues and Solutions
* **Issue: Determining optimal 'k' for K-Means.**
    * **Solution:** Use methods like the elbow method or silhouette score analysis outside of SageMaker's direct training, typically in a SageMaker Notebook.
* **Issue: Clustering results are not meaningful.**
    * **Solution:**
        * **Feature Scaling:** Ensure features are scaled properly (e.g., standardization).
        * **Dimensionality Reduction:** Reduce noise and irrelevant features before clustering.
        * **Algorithm Choice:** Experiment with other clustering algorithms (e.g., DBSCAN if clusters are density-based).
        * **Domain Knowledge:** Incorporate domain knowledge to interpret clusters.`,
                  "children": [
                    {
                      "name": "Clustering",
                      "description": "Groups similar data points together into clusters based on their intrinsic properties. The goal is to discover natural groupings in data.",
                      "children": [
                        {"name": "K-Means", "description": "An iterative algorithm that partitions n observations into k clusters, where each observation belongs to the cluster with the nearest mean (cluster centroid)."},
                        {"name": "Hierarchical Clustering", "description": "Builds a hierarchy of clusters, either by starting with individual data points and merging them (agglomerative) or by starting with one large cluster and splitting it (divisive)."},
                        {"name": "DBSCAN", "description": "Density-Based Spatial Clustering of Applications with Noise. It groups together points that are closely packed together, marking as outliers points that lie alone in low-density regions."},
                        {"name": "Gaussian Mixture Models (GMM)", "description": "A probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters."}
                      ]
                    },
                    {
                      "name": "Dimensionality Reduction",
                      "description": "Reduces the number of random variables under consideration by obtaining a set of principal variables. This helps in visualization, noise reduction, and improving model performance.",
                      "children": [
                        {"name": "Principal Component Analysis (PCA)", "description": "A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. Widely used for feature extraction."},
                        {"name": "t-SNE (t-Distributed Stochastic Neighbor Embedding)", "description": "A non-linear dimensionality reduction technique well-suited for visualizing high-dimensional datasets. It maps multi-dimensional data to two or more dimensions suitable for human observation."},
                        {"name": "Linear Discriminant Analysis (LDA)", "description": "A dimensionality reduction technique used in supervised classification problems. It finds a linear combination of features that characterizes or separates two or more classes of objects or events."}
                      ]
                    },
                    {
                      "name": "Association Rule Learning",
                      "description": "Discovers interesting relationships between variables in large datasets. Often used in market basket analysis to find items that are frequently purchased together.",
                      "children": [
                        {"name": "Apriori", "description": "An algorithm for frequent item set mining and association rule learning over transactional databases. It identifies frequent individual items in the database and extends them to larger item sets as long as those item sets appear sufficiently often in the database."},
                        {"name": "Eclat", "description": "An algorithm for frequent item set mining that uses a depth-first search approach and intersection of transaction IDs to find frequent item sets. Generally faster than Apriori for certain datasets."}
                      ]
                    }
                  ]
                },
                {
                  "name": "Semi-Supervised Learning",
                  "description": `Combines a small amount of labeled data with a large amount of unlabeled data during training. It's particularly useful when labeling data is expensive or time-consuming, leveraging the structural information from unlabeled data.

### AWS Implementation for Semi-Supervised Learning
While AWS doesn't have a dedicated "semi-supervised learning" service, its components can be orchestrated to achieve this:
* **Amazon SageMaker Ground Truth:** Can be used for active learning strategies where a model identifies uncertain samples for human labeling, thus combining human labeling with automated data selection.
* **Amazon SageMaker:** You can implement semi-supervised algorithms (e.g., self-training, co-training) using custom code within SageMaker Notebook Instances or Training Jobs, leveraging both labeled and unlabeled data stored in S3.
* **AWS Step Functions/Lambda:** For orchestrating workflows that involve iterative labeling and model training.

---

#### How to Enable/Config (Conceptual)
1.  **Data Preparation:** Separate your small labeled dataset and larger unlabeled dataset in S3.
2.  **Ground Truth (Optional):** Use Ground Truth to iteratively label data based on model uncertainty.
3.  **Custom Script in SageMaker:** Write a Python script that implements your chosen semi-supervised algorithm. This script will typically train on labeled data, make predictions on unlabeled data, select high-confidence predictions as new "labeled" data, and retrain.
4.  **SageMaker Training Job:** Run your custom script as a SageMaker Training Job.

#### Key Features
* **Cost Efficiency:** Reduces the need for extensive manual data labeling.
* **Leverages Unlabeled Data:** Utilizes readily available unlabeled data to improve model performance.
* **Improved Generalization:** Can lead to more robust models, especially with limited labeled data.

#### Parameters/Dependencies
* **Labeled Data S3 URI:** Path to your small labeled dataset.
* **Unlabeled Data S3 URI:** Path to your large unlabeled dataset.
* **Confidence Thresholds:** For self-training, a threshold to determine which pseudo-labels to trust.
* **IAM Role:** With permissions for S3, SageMaker.

#### Tool Navigations (AWS Console)
* SageMaker -> Ground Truth (for data labeling).
* SageMaker -> Training -> Training jobs (for running custom semi-supervised algorithms).

#### CLI & Output Syntax Examples (Conceptual)
\`\`\`bash
# Example: Starting a SageMaker training job for a custom semi-supervised algorithm
# This assumes your training script (e.g., semi_supervised_train.py) handles the logic.
aws sagemaker create-training-job \\
    --training-job-name semi-supervised-job \\
    --algorithm-specification TrainingImage="<your-custom-container-uri>",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/SageMakerExecutionRole \\
    --input-data-config '[{"ChannelName": "labeled", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/labeled-data/"}}}, {"ChannelName": "unlabeled", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/unlabeled-data/"}}}]' \\
    --output-data-config S3OutputPath="s3://your-s3-bucket/model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.m5.xlarge,VolumeSizeInGB=50 \\
    --stopping-condition MaxRuntimeInSeconds=7200
\`\`\`

#### Common Issues and Solutions
* **Issue: Propagation of errors.**
    * **Solution:** If pseudo-labels are inaccurate, errors can propagate. Use robust confidence thresholds, or human review for critical pseudo-labels.
* **Issue: Performance highly dependent on initial labeled data quality.**
    * **Solution:** Ensure the initial small labeled dataset is high quality and representative.
* **Issue: Complexity of implementation.**
    * **Solution:** Semi-supervised learning often requires custom code. Leverage SageMaker's flexible environment for custom algorithms.`,
                  "children": [
                    {
                      "name": "Federated Learning",
                      "description": `A machine learning approach that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. This approach enables privacy-preserving collaborative model training.

### AWS Implementation for Federated Learning
AWS does not currently offer a dedicated managed service for Federated Learning. However, it can be implemented using a combination of AWS services:
* **Amazon SageMaker:** For training the global model and potentially for local model training on decentralized data. You can use SageMaker's custom container capabilities to package your federated learning framework (e.g., Flower, PySyft).
* **AWS IoT Greengrass/AWS IoT Core:** For deploying local models and orchestrating training on edge devices.
* **AWS Lambda/AWS Step Functions:** For orchestrating the federated learning rounds, including aggregation of model updates (e.g., federated averaging).
* **Amazon S3:** For secure storage and exchange of encrypted model updates (not raw data).
* **AWS PrivateLink/VPC Endpoints:** For secure communication between decentralized nodes and the central aggregator.

---

#### How to Enable/Config (Conceptual)
1.  **Choose a Federated Learning Framework:** Select an open-source framework like Flower, PySyft, or TensorFlow Federated.
2.  **Containerize Local Training:** Create Docker images for your local training clients, including your chosen framework and model code. Deploy these to edge devices (e.g., via Greengrass) or EC2 instances.
3.  **Central Aggregator:** Implement a central server (e.g., on an EC2 instance or as a SageMaker endpoint) that orchestrates training rounds, collects model updates, aggregates them, and sends back the global model. This can be managed by Lambda/Step Functions.
4.  **Secure Communication:** Ensure all communication channels are encrypted (e.g., TLS) and authenticated (e.g., IAM roles).

#### Key Features
* **Privacy Preservation:** Raw data never leaves the local device, enhancing data privacy.
* **Decentralized Training:** Leverages distributed computing resources.
* **Reduced Data Transfer Costs:** Only model updates are transmitted, not raw data.
* **Access to More Data:** Allows training on data that cannot be centralized due to privacy, regulatory, or logistical constraints.

#### Parameters/Dependencies
* **Number of Clients:** How many decentralized nodes participate.
* **Aggregation Strategy:** How local model updates are combined (e.g., Federated Averaging).
* **Communication Protocol:** Secure methods for exchanging model parameters.
* **Compute Resources:** Sufficient compute on edge devices/local servers for training.
* **IAM Roles:** Permissions for S3, EC2, IoT services, and SageMaker.

#### Tool Navigations (AWS Console)
* SageMaker -> Notebook instances / Training jobs (for central model or custom client training).
* IoT Core / Greengrass (for edge device deployment).
* Lambda / Step Functions (for orchestration).
* S3 (for model update storage).

#### CLI & Output Syntax Examples (Conceptual)
\`\`\`bash
# Example: Deploying a custom Docker image for a federated learning client to ECR
aws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<your-region>.amazonaws.com
docker build -t federated-client-image .
docker tag federated-client-image:latest <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/federated-client-image:latest
docker push <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/federated-client-image:latest

# Example: Starting a SageMaker training job for the central aggregator (conceptual)
# This would involve a custom script that orchestrates the federated learning rounds.
aws sagemaker create-training-job \\
    --training-job-name federated-aggregator-job \\
    --algorithm-specification TrainingImage="<your-custom-aggregator-image-uri>",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/SageMakerExecutionRole \\
    --input-data-config '[]' \\ # Data is decentralized
    --output-data-config S3OutputPath="s3://your-s3-bucket/federated-model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.m5.xlarge,VolumeSizeInGB=50 \\
    --stopping-condition MaxRuntimeInSeconds=7200
\`\`\`

#### Common Issues and Solutions
* **Issue: Communication overhead.**
    * **Solution:** Optimize model update size (e.g., sparsification, compression). Use efficient communication protocols.
* **Issue: Statistical heterogeneity (Non-IID data).**
    * **Solution:** Advanced federated learning algorithms (e.g., FedProx, SCAFFOLD) are designed to handle non-IID data.
* **Issue: Client dropout/unavailability.**
    * **Solution:** Implement robust aggregation strategies that can handle missing client updates.
* **Issue: Security vulnerabilities.**
    * **Solution:** Implement differential privacy, secure multi-party computation (SMC), or homomorphic encryption to protect model updates and prevent inference attacks.`,
                    }
                  ]
                },
                {
                  "name": "Reinforcement Learning (RL)",
                  "description": `An agent learns to make decisions by performing actions in an environment to maximize a cumulative reward. It learns through trial and error, receiving feedback (rewards or penalties) for its actions.

### AWS Implementation for Reinforcement Learning
* **Amazon SageMaker Reinforcement Learning:** SageMaker provides managed services for training RL models. It integrates with popular RL toolkits like Coach and Ray RLlib.
* **AWS DeepRacer:** A 1/18th scale autonomous race car driven by reinforcement learning, providing a hands-on way to learn RL.
* **AWS RoboMaker:** For developing, simulating, and deploying robotics applications, including those using RL for robot control.

---

#### How to Enable/Config (SageMaker RL Example)
1.  **Prepare Environment:** Define your RL environment (e.g., OpenAI Gym) and reward function.
2.  **SageMaker RL Container:** Use pre-built RL containers or create your own.
3.  **SageMaker Console/SDK:** Set up a training job with the RL estimator.

#### Key Features
* **Managed RL Training:** SageMaker abstracts away infrastructure management.
* **Integration with RL Toolkits:** Supports popular frameworks.
* **Simulation Integration:** Can integrate with simulators for training.

#### Parameters/Dependencies
* **RL Toolkit:** Specify the toolkit (e.g., \`coach\`, \`ray\`).
* **Environment:** Define the simulation environment.
* **Hyperparameters:** Specific to the chosen RL algorithm (e.g., learning rate, discount factor, episodes).
* **Instance Type:** Often requires GPU instances (\`ml.g4dn\`, \`ml.p3\`) for deep RL.

#### Tool Navigations (AWS Console)
* SageMaker -> Training -> Training jobs -> Create training job.
* Select "Custom script" and provide your RL training script and container.
* AWS DeepRacer console.

#### CLI & Output Syntax Examples (SageMaker RL Training Job - conceptual)
\`\`\`bash
# Example: Create a SageMaker RL training job (conceptual, requires custom script and image)
# This is highly dependent on your specific RL setup (environment, algorithm, toolkit)
aws sagemaker create-training-job \\
    --training-job-name my-rl-training-job \\
    --hyper-parameters '{"num_episodes": "1000", "gamma": "0.99"}' \\
    --algorithm-specification TrainingImage="<your-custom-rl-image-uri>",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/service-role/SageMakerExecutionRole \\
    --input-data-config '[]' \\ # RL often doesn't have static input data in this way
    --output-data-config S3OutputPath="s3://your-s3-bucket/rl-model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.g4dn.xlarge,VolumeSizeInGB=100 \\
    --stopping-condition MaxRuntimeInSeconds=7200
\`\`\`

#### Common Issues and Solutions
* **Issue: RL training is unstable or converges slowly.**
    * **Solution:**
        * **Reward Function Design:** Refine the reward function to guide the agent effectively.
        * **Hyperparameter Tuning:** Experiment with different RL hyperparameters.
        * **Exploration vs. Exploitation:** Adjust exploration strategies.
        * **Network Architecture:** For deep RL, optimize neural network architecture.
* **Issue: High simulation costs.**
    * **Solution:** Optimize simulation environment efficiency, use cheaper instance types for initial testing, or explore local training for smaller experiments.`,
                  "children": [
                    {"name": "Q-Learning", "description": "A model-free reinforcement learning algorithm to learn a quality or 'Q' function, which determines the value of taking a given action in a given state."},
                    {"name": "SARSA (State-Action-Reward-State-Action)", "description": "An on-policy reinforcement learning algorithm that learns the optimal policy by observing the agent's actual actions and rewards, unlike Q-Learning which is off-policy."},
                    {"name": "Deep Q-Networks (DQN)", "description": "Combines Q-Learning with Deep Neural Networks to approximate the Q-function, allowing it to handle environments with large or continuous state spaces. Pioneered by DeepMind for Atari games."},
                    {"name": "Policy Gradients", "description": "A class of reinforcement learning algorithms that directly optimize the policy function (which maps states to actions) using gradient ascent. Examples include REINFORCE and Actor-Critic methods."}
                  ]
                },
                {
                  "name": "Deep Learning (DL)",
                  "description": `A subfield of ML that uses artificial neural networks with multiple layers (deep neural networks) to learn complex patterns from large amounts of data. It has revolutionized many AI applications like image recognition and natural language processing. Key frameworks: TensorFlow, PyTorch.

### AWS Implementation for Deep Learning
AWS is a primary platform for Deep Learning due to its scalable compute (GPUs), storage, and specialized services:
* **Amazon SageMaker:** Fully managed service for building, training, and deploying deep learning models. Supports TensorFlow, PyTorch, MXNet, and custom containers.
* **AWS Deep Learning AMIs (DLAMI):** Pre-configured Amazon Machine Images with popular DL frameworks, drivers, and tools for EC2 instances.
* **Amazon EC2 (GPU Instances):** Direct access to powerful GPU instances (e.g., P3, P4, G4dn) for raw compute power.
* **Amazon EKS/ECS:** For deploying deep learning models in containers using Kubernetes or ECS.
* **AWS Inferentia/Trainium:** Custom AWS-designed chips for high-performance, cost-effective inference and training of large models, including FMs.

---

#### How to Enable/Config (SageMaker Deep Learning Example)
1.  **Choose Environment:**
    * **SageMaker Studio/Notebook Instance:** For interactive development and managed training.
    * **DLAMI on EC2:** For full control over the environment.
2.  **Prepare Data:** Store large datasets in Amazon S3.
3.  **SageMaker Training Job:** Use SageMaker Estimators (e.g., \`TensorFlow\`, \`PyTorch\`) to define and run training jobs.

#### Key Features
* **Managed GPU Training:** SageMaker abstracts away infrastructure management.
* **Distributed Training:** Easily scale training across multiple GPUs or instances.
* **Automatic Model Tuning (AMT):** Optimize hyperparameters for deep learning models.
* **Model Parallelism/Data Parallelism:** SageMaker supports advanced training strategies.
* **Quantization/Compilation:** SageMaker Neo for optimizing models for inference.

#### Parameters/Dependencies
* **Framework Version:** Specify TensorFlow, PyTorch version.
* **Python Version:** Compatible Python version.
* **Entry Point Script:** Your training script (\`train.py\`).
* **Instance Type:** GPU instances (e.g., \`ml.g4dn.xlarge\`, \`ml.p3.2xlarge\`).
* **S3 Paths:** For input data and model output.

#### Tool Navigations (AWS Console)
* SageMaker -> Training -> Training jobs.
* EC2 -> Instances -> Launch Instance (select DLAMI).

#### CLI & Output Syntax Examples (SageMaker PyTorch Training Job)
\`\`\`bash
# Example: Create a SageMaker PyTorch training job (simplified)
# Assuming you have a 'src/main.py' training script and data in S3
aws sagemaker create-training-job \\
    --training-job-name pytorch-dl-job \\
    --hyper-parameters '{"epochs": "10", "batch-size": "64"}' \\
    --algorithm-specification TrainingImage="<your-region>.dkr.ecr.<your-region>.amazonaws.com/sagemaker-pytorch:1.13.1-gpu-py39-cu117-ubuntu20.04",TrainingInputMode="File" \\
    --role-arn arn:aws:iam::123456789012:role/service-role/SageMakerExecutionRole \\
    --input-data-config '[{"ChannelName": "training", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/training-data/"}}}]' \\
    --output-data-config S3OutputPath="s3://your-s3-bucket/model-output/" \\
    --resource-config InstanceCount=1,InstanceType=ml.g4dn.xlarge,VolumeSizeInGB=100 \\
    --stopping-condition MaxRuntimeInSeconds=7200 \\
    --code-config S3DataSource='{"S3Uri":"s3://your-s3-bucket/code/source.tar.gz", "CompressionType":"GZIP"}' \\
    --environment '{"SM_CHANNEL_TRAINING": "/opt/ml/input/data/training"}'

# To get the ECR image URI for PyTorch in your region:
# aws sagemaker list-training-jobs --query "TrainingJobSummaries[?TrainingJobStatus=='Completed'].AlgorithmSpecification.TrainingImage" --output text | grep pytorch
\`\`\`

#### Common Issues and Solutions
* **Issue: Out of memory (OOM) errors during training.**
    * **Solution:**
        * **Reduce Batch Size:** Decrease the batch size in your training script.
        * **Smaller Model:** Use a less complex neural network architecture.
        * **Larger Instance Type:** Switch to a GPU instance with more memory (e.g., from \`g4dn.xlarge\` to \`g4dn.2xlarge\` or \`p3.8xlarge\`).
        * **Gradient Accumulation:** Accumulate gradients over multiple mini-batches to simulate a larger batch size without increasing memory.
* **Issue: Training job fails due to dependency issues.**
    * **Solution:** Ensure all required libraries are specified in your \`requirements.txt\` (for SageMaker) or baked into your custom Docker image. Verify the correct framework version is used.
* **Issue: Slow training.**
    * **Solution:**
        * **Data Loading Optimization:** Ensure efficient data loading from S3 (e.g., using SageMaker Pipe Mode or TFRecord/WebDataset).
        * **Distributed Training:** For very large datasets/models, use SageMaker's distributed training capabilities.
        * **Instance Type:** Use more powerful GPU instances.`,
                  "children": [
                    {
                      "name": "Artificial Neural Networks (ANNs)",
                      "description": "The foundational structure of Deep Learning, inspired by the biological neural networks of the human brain. Consist of interconnected nodes (neurons) organized in layers.",
                      "children": [
                        {"name": "Feedforward Neural Networks (FNNs)", "description": "The simplest type of ANN where information flows in one direction, from input to output, without loops or cycles. Used for tasks like classification and regression on tabular data."},
                        {"name": "Recurrent Neural Networks (RNNs)", "description": "Designed for sequential data (e.g., time series, natural language), with connections that form cycles, allowing information to persist across time steps. They have a 'memory' of previous inputs."},
                        {"name": "Long Short-Term Memory (LSTM)", "description": "A special type of RNN capable of learning long-term dependencies. They address the vanishing/exploding gradient problem common in vanilla RNNs through 'gates' (input, forget, output)."},
                        {"name": "Gated Recurrent Units (GRU)", "description": "A simpler variant of LSTM, also designed to capture dependencies in sequential data. They have fewer gates (reset and update) and are computationally less expensive than LSTMs."},
                        {"name": "Convolutional Neural Networks (CNNs)", "description": "Primarily used for image and video processing, they employ convolutional layers to automatically learn spatial hierarchies of features from raw pixel data. Highly effective for tasks like image classification and object detection."},
                        {"name": "Generative Adversarial Networks (GANs)", "description": "Composed of two neural networks, a 'generator' and a 'discriminator', that compete against each other in a zero-sum game. The generator tries to create realistic data, while the discriminator tries to distinguish real data from generated data. Used for generating images, art, etc."},
                        {"name": "Transformers", "description": "A novel neural network architecture that relies entirely on self-attention mechanisms to draw global dependencies between input and output. They have become the state-of-the-art for many NLP tasks (e.g., BERT, GPT) and are increasingly used in computer vision."}
                      ]
                    },
                    {"name": "Deep Reinforcement Learning", "description": "Combines Deep Learning with Reinforcement Learning, allowing agents to learn complex strategies directly from high-dimensional inputs (e.g., raw pixel data from a game screen). This fusion has led to breakthroughs in AI for games and robotics."},
                    {"name": "Transfer Learning", "description": "A machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It leverages pre-trained models (often large deep neural networks) to accelerate training and improve performance on new, related tasks with less data."},
                    {"name": "Autoencoders", "description": "Neural networks that learn an efficient coding (representation) of data in an unsupervised manner. They consist of an encoder that compresses the input into a latent-space representation, and a decoder that reconstructs the input from this representation. Used for dimensionality reduction, feature learning, and denoising."},
                    {"name": "Restricted Boltzmann Machines (RBMs)", "description": "Generative stochastic neural networks that can learn a probability distribution over their set of inputs. They are two-layer neural networks with a visible layer and a hidden layer, and no intra-layer connections. Used in collaborative filtering and feature learning."}
                  ]
                },
                {
                  "name": "MLOps",
                  "description": `A set of practices that combines Machine Learning, DevOps, and Data Engineering to streamline the entire machine learning lifecycle. It focuses on automating and standardizing the process of building, deploying, monitoring, and managing ML models in production.

### AWS Implementation for MLOps
AWS provides a comprehensive set of services that support MLOps best practices:
* **Amazon SageMaker MLOps capabilities:**
    * **SageMaker Pipelines:** A purpose-built CI/CD service for ML, allowing you to create, automate, and manage end-to-end ML workflows.
    * **SageMaker Model Registry:** A central repository to catalog models, manage versions, and track model lineage.
    * **SageMaker Model Monitor:** Continuously monitors deployed ML models for data drift, model quality drift, and bias, alerting you to issues.
    * **SageMaker Feature Store:** A purpose-built repository for ML features, enabling feature reuse, consistency, and low-latency access.
    * **SageMaker Projects:** Provides templates for creating CI/CD pipelines for ML using AWS CodeCommit, CodeBuild, CodePipeline, and SageMaker.
* **AWS CodeCommit/CodeBuild/CodePipeline:** For source control, continuous integration, and continuous delivery of ML code and models.
* **AWS Lambda:** For serverless orchestration of ML workflows and inference.
* **Amazon CloudWatch/CloudTrail:** For monitoring and logging ML operations.
* **Amazon S3:** For storing datasets, model artifacts, and pipeline outputs.
* **Amazon EKS/ECS:** For deploying ML models in containerized environments.

---

#### How to Enable/Config (SageMaker Pipelines Example)
1.  **SageMaker Studio:** Open SageMaker Studio.
2.  **SageMaker Projects:** Create a new SageMaker Project (e.g., "MLOps template for model building, training, and deployment"). This sets up a full CI/CD pipeline.
3.  **Define Pipeline:** Use the SageMaker Python SDK to define your ML pipeline steps (processing, training, model registration, deployment).
4.  **Execute Pipeline:** Start pipeline executions from Studio or via SDK/CLI.

#### Key Features
* **Automation:** Automate data preparation, model training, evaluation, and deployment.
* **Reproducibility:** Track model versions, data, and code to ensure reproducibility.
* **Monitoring:** Detect model performance degradation and data drift in production.
* **Collaboration:** Facilitate collaboration between data scientists and operations teams.
* **Governance:** Manage model lifecycle, approvals, and lineage.

#### Parameters/Dependencies (SageMaker Pipelines)
* **Pipeline Definition:** Python script defining pipeline steps.
* **Input Data:** S3 paths for datasets.
* **IAM Roles:** Roles with permissions for SageMaker, S3, Code* services.
* **Container Images:** For custom processing or training steps.

#### Tool Navigations (AWS Console)
* SageMaker -> MLOps.
* SageMaker -> Pipelines.
* SageMaker -> Model registry.
* SageMaker -> Feature Store.
* Developer Tools -> CodePipeline, CodeBuild, CodeCommit.

#### CLI & Output Syntax Examples (SageMaker Pipelines)
\`\`\`bash
# Example: Create a SageMaker Pipeline (conceptual, typically done via SDK)
# This is a high-level representation; actual pipeline definition is in Python.
aws sagemaker create-pipeline \\
    --pipeline-name "MyMLPipeline" \\
    --pipeline-definition-s3-location Bucket="your-s3-bucket",Key="pipeline_definition.json" \\
    --role-arn "arn:aws:iam::123456789012:role/SageMakerPipelineExecutionRole"

# Example: Start a pipeline execution
aws sagemaker start-pipeline-execution \\
    --pipeline-name "MyMLPipeline" \\
    --pipeline-execution-description "Initial run"

# Example: List pipeline executions
aws sagemaker list-pipeline-executions \\
    --pipeline-name "MyMLPipeline"
\`\`\`

#### Common Issues and Solutions
* **Issue: Pipeline failures.**
    * **Solution:** Check CloudWatch logs for individual pipeline steps. Verify IAM permissions, input/output paths, and script errors.
* **Issue: Data drift detected.**
    * **Solution:** Investigate the cause of data drift using SageMaker Model Monitor. Retrain the model with updated data.
* **Issue: Model performance degrades in production.**
    * **Solution:** Use SageMaker Model Monitor to identify model quality issues. Analyze inference data. Trigger retraining based on performance metrics.
* **Issue: Managing multiple model versions.**
    * **Solution:** Leverage SageMaker Model Registry to manage different model versions, their metrics, and approval status.`,
                  "children": [
                    {"name": "CI/CD for ML", "description": "Applying Continuous Integration and Continuous Delivery principles to machine learning workflows, automating code integration, testing, model training, and deployment."},
                    {"name": "Model Monitoring", "description": "Continuously tracking the performance, data drift, concept drift, and bias of deployed ML models to ensure they remain accurate and fair in production."},
                    {"name": "Feature Engineering & Management", "description": "The process of creating new input features for ML models and managing them effectively, often using feature stores for consistency and reuse across different models."},
                    {"name": "Model Governance & Lineage", "description": "Establishing processes and tools to manage the lifecycle of ML models, track their origins (data, code, parameters), and ensure compliance and auditability."},
                    {
                      "name": "Explainable AI (XAI)",
                      "description": `A set of techniques and methods that make the decisions of AI models understandable to humans. XAI is crucial for building trust, ensuring fairness, debugging models, and meeting regulatory requirements.

### AWS Implementation for Explainable AI (XAI)
* **Amazon SageMaker Clarify:** A dedicated SageMaker feature that helps detect bias in ML models and provides explainability for predictions. It supports various explainability methods like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations).
* **SageMaker Model Monitor:** While primarily for drift detection, it can indirectly help in XAI by alerting when model behavior changes, prompting further investigation.
* **Custom Implementations:** You can integrate open-source XAI libraries (e.g., SHAP, LIME, InterpretML) within SageMaker Notebook Instances or custom training jobs.

---

#### How to Enable/Config (SageMaker Clarify Example)
1.  **Prepare Data:** Ensure your dataset (including features and labels) is in Amazon S3.
2.  **SageMaker Studio/SDK:** Use SageMaker Studio or the SageMaker Python SDK to configure and run Clarify jobs.
3.  **Bias Detection Job:** Define a bias analysis job, specifying the dataset, target label, and sensitive attributes.
4.  **Explainability Job:** Define an explainability job, specifying the model endpoint or model artifact, and the desired explainability method (e.g., SHAP).

#### Key Features
* **Bias Detection:** Identify pre-training, post-training, and runtime biases in your data and models.
* **Feature Importance:** Understand which features contribute most to model predictions.
* **Local and Global Explanations:** Get explanations for individual predictions (local) or overall model behavior (global).
* **Fairness Metrics:** Evaluate model fairness across different sensitive groups.
* **Integration with SageMaker:** Seamlessly integrates with SageMaker training and inference workflows.

#### Parameters/Dependencies (SageMaker Clarify)
* **Dataset:** S3 URI to the dataset for analysis.
* **Model URI/Endpoint:** The S3 URI of the model artifact or the SageMaker endpoint name.
* **Explainability Method:** \`SHAP\`, \`LIME\`.
* **Bias Config:** Sensitive features, facet type, group names.
* **Instance Type:** Compute instance type for the Clarify job.

#### Tool Navigations (AWS Console)
* SageMaker -> Clarify.
* SageMaker Studio -> SageMaker Clarify tab.

#### CLI & Output Syntax Examples (SageMaker Clarify - conceptual)
\`\`\`bash
# Example: Create a SageMaker Clarify job for explainability (conceptual)
# This is typically done via SageMaker Python SDK in a notebook.
aws sagemaker create-model-explainability-job \\
    --job-name "my-model-explainability-job" \\
    --model-config '{"ModelName":"my-deployed-model", "InstanceType":"ml.m5.xlarge"}' \\
    --explainability-config '{"DataConfig":{"S3Uri":"s3://your-s3-bucket/explainability-data/", "Features":["feature1", "feature2"]}, "Method":"SHAP"}' \\
    --output-config '{"S3Uri":"s3://your-s3-bucket/clarify-output/"}' \\
    --role-arn "arn:aws:iam::123456789012:role/SageMakerExecutionRole"

# Example: Create a SageMaker Clarify job for bias detection (conceptual)
aws sagemaker create-model-bias-job \\
    --job-name "my-model-bias-job" \\
    --model-config '{"ModelName":"my-deployed-model", "InstanceType":"ml.m5.xlarge"}' \\
    --bias-config '{"AnalysisMethod":"SHAP", "FacetConfig":{"S3Uri":"s3://your-s3-bucket/bias-data/", "Header":"gender", "GroupNames":["Male", "Female"]}}' \\
    --output-config '{"S3Uri":"s3://your-s3-bucket/clarify-output/"}' \\
    --role-arn "arn:aws:iam::123456789012:role/SageMakerExecutionRole"
\`\`\`

#### Common Issues and Solutions
* **Issue: Clarify job fails.**
    * **Solution:** Check CloudWatch logs for the Clarify job. Common causes include incorrect S3 paths, insufficient IAM permissions, or data format issues.
* **Issue: Explainability results are difficult to interpret.**
    * **Solution:** Ensure you understand the chosen explainability method (e.g., SHAP values). Use SageMaker Studio's Clarify reports for visualizations.
* **Issue: Bias detection reports false positives/negatives.**
    * **Solution:** Carefully define your sensitive attributes and groups. Experiment with different bias metrics and thresholds. Ensure your dataset is representative.`,
                    },
                    {
                      "name": "AI Security & Privacy",
                      "description": `The practice of protecting AI systems from malicious attacks, ensuring data privacy, and maintaining the integrity and confidentiality of models and data throughout their lifecycle. This includes addressing vulnerabilities like adversarial attacks, model inversion, and data poisoning.

### AWS Implementation for AI Security & Privacy
AWS provides a robust security framework and services that can be leveraged to secure AI/ML workloads:
* **AWS Identity and Access Management (IAM):** Crucial for fine-grained access control to all AWS resources, including SageMaker, S3, and Bedrock. Implement least privilege.
* **Amazon SageMaker Clarify:** Helps detect bias and provides model explainability, which can indirectly contribute to security by identifying unintended behaviors.
* **Amazon Macie:** A data security and data privacy service that uses machine learning to discover, classify, and protect sensitive data in AWS. It can help detect data leakage in S3 buckets used for ML.
* **AWS Key Management Service (KMS):** For encrypting data at rest (e.g., S3 buckets, SageMaker volumes) and in transit (e.g., TLS/SSL for API endpoints).
* **Amazon Virtual Private Cloud (VPC):** To isolate ML resources within your private network, preventing unauthorized access.
* **AWS CloudTrail/Amazon CloudWatch:** For auditing API calls and monitoring logs to detect suspicious activities.
* **AWS WAF (Web Application Firewall):** To protect ML inference endpoints (if exposed via API Gateway) from common web exploits.
* **Custom Implementations:** For advanced adversarial robustness, you may need to implement custom defense mechanisms within your SageMaker training or inference code.

---

#### How to Enable/Config (General Best Practices)
1.  **IAM Policies:** Create specific IAM roles for SageMaker, Bedrock, etc., with only the necessary permissions. Avoid using root credentials.
2.  **Data Encryption:** Enable S3 default encryption (SSE-S3 or SSE-KMS) for all data buckets. Configure SageMaker to use KMS for encrypting training volumes and model artifacts.
3.  **Network Isolation:** Deploy SageMaker Notebook Instances, Training Jobs, and Endpoints within a private VPC. Use VPC endpoints for secure access to S3 and other AWS services.
4.  **Logging and Monitoring:** Enable CloudTrail for API call logging. Configure CloudWatch logs for SageMaker jobs and endpoints, and set up alarms for unusual activity.
5.  **Data Anonymization/Pseudonymization:** Before training, apply techniques to sensitive data where appropriate.

#### Key Features
* **Access Control:** Granular control over who can access and use ML resources.
* **Data Protection:** Encryption of data at rest and in transit.
* **Threat Detection:** Identify sensitive data, suspicious activities, and potential data breaches.
* **Model Integrity:** Protect models from poisoning and tampering.
* **Privacy Compliance:** Aid in meeting regulatory requirements (e.g., GDPR, HIPAA) by securing data.

#### Parameters/Dependencies
* **IAM Policies:** Correctly configured policies.
* **KMS Keys:** Customer-managed keys for encryption.
* **VPC Subnets/Security Groups:** Network configuration for isolation.
* **Logging Destinations:** CloudWatch log groups, S3 buckets for logs.

#### Tool Navigations (AWS Console)
* IAM -> Roles / Policies.
* S3 -> Bucket properties -> Default encryption.
* SageMaker -> Notebook instances / Training jobs / Endpoints -> Network settings.
* Macie.
* KMS.
* VPC.
* CloudWatch / CloudTrail.

#### CLI & Output Syntax Examples (Conceptual)
\`\`\`bash
# Example: Create an S3 bucket with default encryption (SSE-S3)
aws s3api create-bucket --bucket my-secure-ml-bucket --region us-east-1
aws s3api put-bucket-encryption \\
    --bucket my-secure-ml-bucket \\
    --server-side-encryption-configuration '{"Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]}'

# Example: Get an IAM role's policy
aws iam get-role-policy --role-name SageMakerExecutionRole --policy-name SageMakerPolicy

# Example: Describe a SageMaker endpoint's network configuration
aws sagemaker describe-endpoint --endpoint-name my-secure-endpoint
\`\`\`

#### Common Issues and Solutions
* **Issue: Overly permissive IAM policies.**
    * **Solution:** Regularly review IAM policies and adhere to the principle of least privilege. Use IAM Access Analyzer.
* **Issue: Data leakage.**
    * **Solution:** Use Amazon Macie to scan S3 buckets for sensitive data. Implement strict S3 bucket policies.
* **Issue: Adversarial attacks on models.**
    * **Solution:** Implement adversarial training techniques. Use input validation and anomaly detection at inference endpoints. Monitor model behavior for deviations.
* **Issue: Compliance with data residency.**
    * **Solution:** Ensure data is stored and processed in the correct AWS regions according to regulatory requirements.
* **Issue: Securely sharing models/data.**
    * **Solution:** Use AWS Resource Access Manager (RAM) for controlled sharing within your organization. For external sharing, use signed URLs for S3 or API Gateway for endpoints with strong authentication.`,
                    },
                    {
                      "name": "Experiment Tracking",
                      "description": `The process of systematically recording and organizing all aspects of a machine learning experiment, including code versions, hyperparameters, datasets, metrics, and model artifacts. It's essential for reproducibility, collaboration, and efficient iteration in ML development.

### AWS Implementation for Experiment Tracking
* **Amazon SageMaker Experiments:** A fully managed service within SageMaker that automatically tracks all steps of your ML workflows, including data processing, model training, and model evaluation. It captures inputs, parameters, and results.
* **SageMaker Studio:** Provides a visual interface to view, compare, and manage your experiments.
* **Amazon S3:** Used to store model artifacts, datasets, and experiment outputs.
* **Amazon CloudWatch:** For logging and monitoring metrics generated during experiments.
* **AWS CodeCommit/CodeBuild/CodePipeline:** For version controlling your ML code and automating experiment runs as part of CI/CD.

---

#### How to Enable/Config (SageMaker Experiments Example)
1.  **SageMaker Studio:** Open SageMaker Studio.
2.  **SageMaker Python SDK:** Use the \`sagemaker.experiments\` SDK in your SageMaker Notebook or training script.
3.  **Create Experiment:** Instantiate an \`Experiment\` object and start a \`Run\` within your training script.
4.  **Log Parameters/Metrics:** Use \`run.log_parameters()\` and \`run.log_metrics()\` to record key experiment details. SageMaker also auto-logs many built-in algorithm parameters and metrics.

#### Key Features
* **Automatic Tracking:** Automatically captures training job configurations, metrics, and artifacts.
* **Reproducibility:** Easily recreate past experiments by reviewing logged parameters and code versions.
* **Comparison:** Visually compare multiple experiment runs to identify the best performing models.
* **Lineage Tracking:** Understand the full lineage of a model, from data to training job to deployment.
* **Collaboration:** Share experiment results with team members.

#### Parameters/Dependencies (SageMaker Experiments)
* **SageMaker Session:** An active SageMaker session in your notebook or script.
* **Experiment Name:** A unique name for your experiment.
* **Run Name:** A unique name for each specific run within an experiment.
* **IAM Role:** With permissions for SageMaker Experiments, S3, and CloudWatch.

#### Tool Navigations (AWS Console)
* SageMaker -> Experiments (in the left navigation pane of SageMaker Studio).
* SageMaker -> Training -> Training jobs (each training job is an experiment run).

#### CLI & Output Syntax Examples (SageMaker Experiments - conceptual)
\`\`\`bash
# Example: List SageMaker Experiments
aws sagemaker list-experiments

# Example: Describe a specific experiment
aws sagemaker describe-experiment --experiment-name "MyImageClassificationExperiment"

# Example: List runs within an experiment
aws sagemaker list-runs --experiment-name "MyImageClassificationExperiment"

# Example: Describe a specific run
aws sagemaker describe-run --run-name "MyImageClassificationExperiment-Run-2023-07-04-10-30"
\`\`\`

#### Common Issues and Solutions
* **Issue: Metrics/parameters not logging.**
    * **Solution:** Ensure you are using the \`sagemaker.experiments\` SDK correctly within your training script and that the \`run.log_parameters()\` and \`run.log_metrics()\` calls are executed. Check IAM permissions for SageMaker Experiments.
* **Issue: Difficulty comparing many runs.**
    * **Solution:** Use SageMaker Studio's experiment comparison view. Filter and sort runs based on specific metrics or parameters.
* **Issue: Large number of experiments/runs.**
    * **Solution:** Organize experiments logically using meaningful names. Archive or delete old/irrelevant experiments to keep the workspace clean.`,
                    }
                  ]
                }
              ]
            },
            {
              "name": "Generative AI",
              "description": `A rapidly evolving subfield of AI focused on creating new, original content (text, images, audio, video, code) that resembles real-world data. It leverages large models trained on vast datasets to understand patterns and generate novel outputs. Key models include Large Language Models (LLMs) and Diffusion Models.

### AWS Implementation for Generative AI
AWS is heavily investing in Generative AI, offering foundational models and tools to build custom generative applications:
* **Amazon Bedrock:** A fully managed service that makes Foundation Models (FMs) from Amazon and leading AI startups available through a single API. It allows you to easily experiment with, fine-tune, and deploy FMs.
    * **Amazon Titan FMs:** A family of FMs developed by Amazon (e.g., Titan Text for text generation, Titan Embeddings for text embeddings, Titan Image for image generation).
    * **Third-party FMs:** Access to models like Anthropic Claude, AI21 Labs Jurassic, Stability AI Stable Diffusion, and Cohere Command/Embed.
* **Amazon SageMaker JumpStart:** Provides a hub with pre-trained FMs, notebooks, and solutions for various tasks, including generative AI. You can deploy these models with a few clicks.
* **Amazon CodeWhisperer:** An AI coding companion that generates code suggestions in real-time, helping developers write code faster.
* **Amazon Q:** A new generative AI-powered assistant for business users, offering conversational Q&A, content generation, and task automation within enterprise data.
* **Amazon EC2 (GPU Instances):** For training and fine-tuning very large generative models from scratch or for custom deployments.
* **AWS Lambda/Step Functions:** For orchestrating generative AI workflows and integrating with other applications.

---

#### How to Enable/Config (Amazon Bedrock Example)
1.  **AWS Console:** Navigate to Amazon Bedrock.
2.  **Model Access:** Request access to the desired foundation models (e.g., Anthropic Claude, Amazon Titan) within the Bedrock console. This is a one-time step.
3.  **API/SDK:** Use the AWS SDKs (e.g., Boto3 for Python) or direct API calls to interact with the FMs.
4.  **IAM Permissions:** Ensure your user/role has necessary permissions (e.g., \`bedrock:InvokeModel\`, \`bedrock:ListFoundationModels\`).

#### Key Features
* **Managed FMs:** Access powerful FMs without managing underlying infrastructure.
* **Customization (Fine-tuning):** Fine-tune FMs with your own data to make them specialized for your use case.
* **Agents for Bedrock:** Build conversational agents that can perform multi-step tasks by orchestrating FM calls and interacting with your systems.
* **Retrieval Augmented Generation (RAG):** Integrate FMs with your data sources (e.g., knowledge bases in Amazon Kendra) to provide more accurate and contextually relevant responses.
* **Cost-Effective:** Pay-as-you-go pricing for FM inference.

#### Parameters/Dependencies (Bedrock Text Generation)
* **Model ID:** The specific FM to invoke (e.g., \`anthropic.claude-v2\`, \`amazon.titan-text-express-v1\`).
* **Prompt:** The input text or instruction for the FM.
* **Inference Parameters:** Model-specific parameters like \`max_tokens_to_sample\`, \`temperature\`, \`top_p\`, \`stop_sequences\`.
* **IAM Role:** For fine-tuning or creating agents, an IAM role with S3 access for data.

#### Tool Navigations (AWS Console)
* Services -> Machine Learning -> Amazon Bedrock.
* Explore "Playgrounds" for text, chat, and image generation.
* Bedrock -> "Model access" to enable models.
* Bedrock -> "Custom models" for fine-tuning.
* Bedrock -> "Agents" to build conversational agents.

#### CLI & Output Syntax Examples (Bedrock InvokeModel - conceptual for text)
\`\`\`bash
# Example: Invoke a text generation model via Bedrock (conceptual with AWS CLI)
# Note: Bedrock API calls are typically done via SDKs (Boto3) for complex payloads.
# CLI example is simplified for illustration.

# For Amazon Titan Text Express V1
aws bedrock-runtime invoke-model \\
    --model-id amazon.titan-text-express-v1 \\
    --body '{ "inputText": "Tell me a short story about a brave knight.", "textGenerationConfig": { "maxTokenCount": 200, "temperature": 0.7, "topP": 0.9 } }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json

# Output (output.json):
# {
#     "results": [
#         {
#             "outputText": "Once upon a time...",
#             "completionReason": "FINISH"
#         }
#     ]
# }

# For Anthropic Claude (requires specific prompt format)
aws bedrock-runtime invoke-model \\
    --model-id anthropic.claude-v2 \\
    --body '{ "prompt": "\\n\\nHuman: Write a poem about a cloud.\\n\\nAssistant:", "max_tokens_to_sample": 300, "temperature": 0.8, "stop_sequences": ["\\n\\nHuman:"] }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Model access denied.**
    * **Solution:** Ensure you have requested and been granted access to the specific foundation model in the Bedrock console under "Model access".
* **Issue: Invalid inference parameters.**
    * **Solution:** Check the documentation for the specific model you are invoking. Parameters like \`max_tokens_to_sample\`, \`temperature\`, \`top_p\` vary slightly or have specific ranges.
* **Issue: High latency or rate limiting.**
    * **Solution:** For high-throughput applications, consider requesting service quota increases for Bedrock. Optimize your application's call patterns.
* **Issue: Model hallucinations or irrelevant output.**
    * **Solution:**
        * **Prompt Engineering:** Refine your prompts to be clearer, more specific, and provide examples.
        * **Context:** Provide more relevant context to the model.
        * **RAG:** Implement Retrieval Augmented Generation to ground the model's responses in your own data.
        * **Fine-tuning:** Fine-tune the model on your domain-specific data for better relevance.`,
              "children": [
                {
                  "name": "Foundation Models (FMs)",
                  "description": `Large-scale machine learning models, typically deep neural networks, trained on vast and diverse datasets to perform a wide range of general-purpose tasks. FMs can be adapted to various downstream applications through fine-tuning, prompt engineering, or few-shot learning, making them highly versatile and powerful.

### AWS Implementation for Foundation Models
* **Amazon Bedrock:** The primary AWS service for accessing and building applications with Foundation Models. It provides a managed API to FMs from Amazon (Titan) and third-party providers (Anthropic Claude, AI21 Labs Jurassic, Stability AI Stable Diffusion, Cohere).
* **Amazon SageMaker JumpStart:** Offers a collection of pre-trained FMs that can be deployed directly or fine-tuned. It provides notebooks and solutions for common FM use cases.
* **Amazon EC2 (GPU Instances):** For users who want to train or fine-tune FMs from scratch with full control over the infrastructure.
* **AWS Inferentia/Trainium:** Custom AWS-designed chips optimized for high-performance, cost-effective inference and training of large models, including FMs.

---

#### How to Enable/Config (Amazon Bedrock Example)
1.  **Access Bedrock:** Navigate to the Amazon Bedrock console.
2.  **Request Model Access:** Go to "Model access" and enable the specific FMs you want to use (e.g., Amazon Titan, Anthropic Claude). This is a one-time approval process.
3.  **Use API/SDK:** Once enabled, you can interact with the FMs using the AWS SDKs (e.g., Boto3 for Python) or the Bedrock Runtime API.

#### Key Features
* **Versatility:** Perform various tasks (text generation, summarization, translation, Q&A, image generation) from a single model.
* **Transfer Learning:** Adapt to new tasks with minimal data through fine-tuning or prompt engineering.
* **Scalability:** Managed services like Bedrock handle the underlying infrastructure.
* **Cost Efficiency:** Pay-as-you-go pricing for inference.

#### Parameters/Dependencies
* **Model ID:** Unique identifier for the FM.
* **Input Data:** Text prompts, image inputs, etc.
* **Inference Parameters:** Model-specific parameters (e.g., temperature, top_p, max_tokens).
* **IAM Permissions:** \`bedrock:InvokeModel\` for inference, \`bedrock:CreateModelCustomizationJob\` for fine-tuning.

#### Tool Navigations (AWS Console)
* Amazon Bedrock console.
* SageMaker JumpStart in SageMaker Studio.

#### CLI & Output Syntax Examples (Bedrock InvokeModel for FM)
\`\`\`bash
# Example: Invoke a text generation FM via Bedrock Runtime API (simplified)
# This is typically done with AWS SDKs for more complex payloads.
# For Amazon Titan Text Express V1
aws bedrock-runtime invoke-model \\
    --model-id amazon.titan-text-express-v1 \\
    --body '{ "inputText": "Write a short marketing slogan for a new coffee shop.", "textGenerationConfig": { "maxTokenCount": 50, "temperature": 0.8 } }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Access denied to FMs.**
    * **Solution:** Ensure you have explicitly requested and been granted access to the specific FMs in the Bedrock console.
* **Issue: Suboptimal output.**
    * **Solution:** Improve prompt engineering, provide more context, or consider fine-tuning the model on your specific dataset.
* **Issue: Cost management.**
    * **Solution:** Monitor usage in AWS Cost Explorer. Optimize inference calls and choose appropriate models for your use case.`,
                },
                {
                  "name": "Large Language Models (LLMs)",
                  "description": `Powerful deep learning models trained on vast amounts of text data, capable of understanding, generating, and translating human language. They are the backbone of many generative AI applications.

### AWS Implementation for Large Language Models (LLMs)
* **Amazon Bedrock:** Provides access to a variety of LLMs, including Amazon's own Titan Text models and third-party models like Anthropic Claude, AI21 Labs Jurassic, and Cohere Command. You can interact with these models via API for text generation, summarization, Q&A, and more.
* **Amazon SageMaker:** For fine-tuning open-source LLMs (e.g., from Hugging Face) on your custom datasets. You can deploy these fine-tuned models as SageMaker endpoints for inference.
* **AWS EC2 (GPU Instances):** For advanced users who want to train LLMs from scratch or perform complex fine-tuning with full control over compute resources.

---

#### How to Enable/Config (Bedrock LLM Example)
1.  **AWS Console:** Navigate to Amazon Bedrock.
2.  **Model Access:** Request access to the desired LLMs (e.g., Anthropic Claude, Amazon Titan Text) within the Bedrock console.
3.  **API/SDK:** Use the AWS SDKs (e.g., Boto3 for Python) to invoke the models.

#### Key Features
* **Text Generation:** Create human-like text for articles, stories, marketing copy, etc.
* **Summarization:** Condense long documents into concise summaries.
* **Question Answering:** Answer questions based on provided context.
* **Translation:** Translate text between languages.
* **Code Generation:** Generate code snippets or entire functions based on natural language descriptions (e.g., CodeWhisperer).
* **Fine-tuning:** Adapt pre-trained LLMs to specific tasks or domains with your own data.

#### Parameters/Dependencies
* **Prompt:** The input text or instruction for the LLM.
* **Inference Parameters:**
    * \`max_tokens_to_sample\` (or \`maxTokenCount\`): Controls the length of the generated output.
    * \`temperature\`: Controls the randomness of the output (higher for more creative, lower for more deterministic).
    * \`top_p\` (or \`topP\`): Nucleus sampling; considers tokens whose cumulative probability exceeds this value.
    * \`stop_sequences\`: Defines sequences of tokens that will stop the generation.
* **IAM Permissions:** \`bedrock:InvokeModel\` for inference.

#### Tool Navigations (AWS Console)
* Amazon Bedrock -> Playgrounds (Text, Chat)
* Amazon Bedrock -> Model access
* Amazon SageMaker -> Notebook instances (for custom LLM fine-tuning)

#### CLI & Output Syntax Examples (Bedrock LLM Inference)
\`\`\`bash
# Example: Invoke Amazon Titan Text Express V1 for text generation
aws bedrock-runtime invoke-model \\
    --model-id amazon.titan-text-express-v1 \\
    --body '{ "inputText": "Write a short paragraph about the benefits of cloud computing.", "textGenerationConfig": { "maxTokenCount": 150, "temperature": 0.7, "topP": 0.9 } }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json

# Example: Invoke Anthropic Claude for chat (requires specific prompt format)
aws bedrock-runtime invoke-model \\
    --model-id anthropic.claude-v2 \\
    --body '{ "prompt": "\\n\\nHuman: What is the capital of France?\\n\\nAssistant:", "max_tokens_to_sample": 100, "temperature": 0.5, "stop_sequences": ["\\n\\nHuman:"] }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Model hallucinations (generating incorrect information).**
    * **Solution:** Implement Retrieval Augmented Generation (RAG) by fetching relevant data from your knowledge base and providing it as context to the LLM. Refine prompts to be more specific.
* **Issue: Output is too generic or repetitive.**
    * **Solution:** Increase \`temperature\` or adjust \`top_p\` to encourage more diverse outputs. Provide more detailed and varied examples in your prompts.
* **Issue: Cost optimization.**
    * **Solution:** Monitor token usage. Optimize prompt length. Choose smaller, more cost-effective models if they meet your performance requirements. Consider batching requests.`,
                },
                {
                  "name": "Diffusion Models",
                  "description": `Diffusion Models are a class of generative models that learn to reverse a gradual noise process (diffusion) to generate high-quality data, particularly images. They start with pure noise and iteratively denoise it to produce realistic and diverse outputs. They have become state-of-the-art for text-to-image generation.

### AWS Implementation for Diffusion Models
* **Amazon Bedrock:** Provides direct access to Stability AI's Stable Diffusion models, allowing you to generate images from text prompts with a simple API call. This is the easiest way to leverage diffusion models on AWS without managing infrastructure.
* **Amazon SageMaker:** You can use SageMaker to fine-tune open-source diffusion models (e.g., from Hugging Face Diffusers library) on your domain-specific image datasets. This allows you to create custom image generation capabilities.
* **Amazon EC2 (GPU Instances):** For users who want to train diffusion models from scratch or perform extensive research and development with full control over powerful GPU resources (e.g., P3, P4, G5 instances).

---

#### How to Enable/Config (Bedrock Stable Diffusion Example)
1.  **AWS Console:** Navigate to Amazon Bedrock.
2.  **Model Access:** Request access to the Stable Diffusion model from Stability AI within the Bedrock console.
3.  **API/SDK:** Use the AWS SDKs (e.g., Boto3 for Python) to invoke the model.

#### Key Features
* **Text-to-Image Generation:** Create photorealistic or artistic images from natural language descriptions.
* **Image-to-Image Translation:** Transform existing images based on text prompts.
* **Inpainting/Outpainting:** Fill missing parts of an image or extend an image beyond its original boundaries.
* **High-Quality Output:** Capable of generating highly detailed and coherent images.

#### Parameters/Dependencies
* **Prompt:** The textual description of the image to generate.
* **Negative Prompt (Optional):** Text describing what you *don't* want in the image.
* **Inference Parameters:**
    * \`cfg_scale\` (Classifier-Free Guidance Scale): Controls how strongly the image adheres to the prompt (higher means more adherence).
    * \`steps\`: Number of denoising steps (more steps generally mean better quality but higher latency).
    * \`seed\`: For reproducibility; using the same seed and prompt will generate the same image.
    * \`width\`, \`height\`: Output image dimensions.
* **IAM Permissions:** \`bedrock:InvokeModel\` for inference.

#### Tool Navigations (AWS Console)
* Amazon Bedrock -> Playgrounds (Image)
* Amazon Bedrock -> Model access
* Amazon SageMaker -> Notebook instances (for custom model fine-tuning)

#### CLI & Output Syntax Examples (Bedrock Stable Diffusion Inference)
\`\`\`bash
# Example: Invoke Stability AI Stable Diffusion XL via Bedrock Runtime API
aws bedrock-runtime invoke-model \\
    --model-id stability.stable-diffusion-xl \\
    --body '{ "text_prompts": [ { "text": "A majestic lion standing on a rock in the savanna, golden hour" } ], "cfg_scale": 7.5, "seed": 0, "steps": 50 }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Generated images are not what was expected (prompt sensitivity).**
    * **Solution:** Experiment with prompt engineering techniques. Use more descriptive and precise prompts. Utilize negative prompts to guide the generation away from undesirable elements.
* **Issue: High GPU cost for training/fine-tuning.**
    * **Solution:** Leverage managed services like Amazon Bedrock for inference to reduce infrastructure overhead. For custom training, optimize model architecture and training data size.
* **Issue: Slow generation time.**
    * **Solution:** Reduce the number of \`steps\`. Choose smaller model variants if available. Optimize inference endpoint configurations on SageMaker.`,
                },
                {
                  "name": "Text-to-Image Generation",
                  "description": `Text-to-Image Generation is a generative AI task that creates novel images directly from textual descriptions or prompts. This involves sophisticated models that understand the semantic meaning of text and translate it into visual elements, styles, and compositions. Popular examples include Stable Diffusion, DALL·E, and Midjourney.

### AWS Implementation for Text-to-Image Generation
* **Amazon Bedrock:** The primary service for text-to-image generation on AWS. It provides managed access to powerful models like Stability AI's Stable Diffusion. You can use simple API calls to generate images by providing text prompts.
* **Amazon SageMaker:** For advanced users, SageMaker allows you to deploy and fine-tune open-source text-to-image models (e.g., from Hugging Face Diffusers) on your own data. This is useful for creating highly customized image generation capabilities for specific domains or styles.
* **Amazon EC2 (GPU Instances):** For training text-to-image models from scratch or for heavy experimentation, EC2 instances with powerful GPUs (e.g., P3, P4, G5 instances) provide the necessary compute.

---

#### How to Enable/Config (Bedrock Stable Diffusion Example)
1.  **AWS Console:** Navigate to Amazon Bedrock.
2.  **Model Access:** Request and enable access to the Stability AI Stable Diffusion model in the Bedrock console.
3.  **API/SDK:** Use the AWS SDK (e.g., Boto3 for Python) or direct API calls to invoke the model with your text prompts.

#### Key Features
* **Creative Content Creation:** Generate unique images for marketing campaigns, digital art, social media, and more.
* **Rapid Prototyping:** Quickly visualize ideas and concepts without manual design.
* **Personalization:** Create tailored images for individual users or specific contexts.
* **Style Transfer:** Generate images in a specific artistic style by including style descriptions in the prompt.

#### Parameters/Dependencies
* **Text Prompt:** The core input, describing the desired image (e.g., "A spaceship landing on a futuristic city at sunset, highly detailed, cyberpunk style").
* **Negative Prompt (Optional):** Text to guide the model away from unwanted elements (e.g., "blurry, low quality, deformed").
* **Image Dimensions:** Specify the width and height of the output image.
* **Guidance Scale (CFG Scale):** Controls how closely the generated image adheres to the text prompt.
* **Seed:** A numerical seed for reproducible results.
* **Number of Inference Steps:** Affects image quality and generation time.
* **IAM Permissions:** \`bedrock:InvokeModel\` for calling the generative models.

#### Tool Navigations (AWS Console)
* Amazon Bedrock -> Playgrounds -> Image
* Amazon Bedrock -> Model access

#### CLI & Output Syntax Examples (Bedrock Stable Diffusion Inference)
\`\`\`bash
# Example: Generate an image using Stability AI Stable Diffusion in Bedrock
aws bedrock-runtime invoke-model \\
    --model-id stability.stable-diffusion-xl \\
    --body '{ "text_prompts": [ { "text": "A serene forest with a hidden waterfall, misty atmosphere, enchanted, photorealistic" } ], "cfg_scale": 8, "seed": 42, "steps": 60, "width": 1024, "height": 768 }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Images are not aesthetically pleasing or accurate.**
    * **Solution:** Refine your prompts using prompt engineering techniques. Experiment with different parameters like \`cfg_scale\` and \`steps\`.
* **Issue: Generating inappropriate or biased content.**
    * **Solution:** Implement content moderation filters (e.g., Amazon Rekognition) on generated outputs. Carefully review and filter prompts.
* **Issue: High inference costs for large-scale generation.**
    * **Solution:** Optimize image resolution and number of steps. Consider batching requests. Leverage Bedrock's managed service for cost efficiency.`,
                },
                {
                  "name": "Code Generation",
                  "description": `Code Generation is a generative AI capability that automatically produces programming code based on natural language descriptions, existing code snippets, or other specifications. It significantly boosts developer productivity by automating repetitive coding tasks, generating boilerplate code, and assisting with complex logic.

### AWS Implementation for Code Generation
* **Amazon CodeWhisperer:** The primary AWS service for AI-powered code generation. It's an ML-powered coding companion that provides real-time code recommendations directly in your Integrated Development Environment (IDE) based on your comments and existing code. It supports various programming languages (Python, Java, JavaScript, TypeScript, C#, Go, Rust, PHP, Ruby, Kotlin, SQL, Scala, JSON, YAML, and C++).
* **Amazon Bedrock:** While not directly for code generation, you can use LLMs available through Bedrock (like Anthropic Claude or Amazon Titan Text) for tasks like generating pseudo-code, explaining code snippets, or translating code comments into functional code, by crafting appropriate prompts.
* **Amazon SageMaker:** For building and deploying custom models that generate code, though this is a more advanced and less common use case than using CodeWhisperer.

---

#### How to Enable/Config (Amazon CodeWhisperer Example)
1.  **IDE Integration:** Install the AWS Toolkit for your IDE (e.g., VS Code, JetBrains IDEs).
2.  **Activate CodeWhisperer:** Enable CodeWhisperer within the AWS Toolkit settings in your IDE. You can use it with your AWS Builder ID or an IAM Identity Center (SSO) connection.
3.  **Start Coding:** Begin typing comments or code, and CodeWhisperer will automatically provide suggestions.

#### Key Features
* **Real-time Code Recommendations:** Get suggestions for single lines, full functions, or even entire code blocks.
* **Multi-language Support:** Supports a wide range of popular programming languages.
* **Natural Language to Code:** Convert natural language comments into executable code.
* **Security Scans:** CodeWhisperer can scan your code for security vulnerabilities and suggest fixes.
* **Reference Tracking:** Helps identify if generated code resembles publicly available code, providing links to original sources.

#### Use Cases
* **Boilerplate Generation:** Quickly create standard code structures, class definitions, or function stubs.
* **Automating Repetitive Tasks:** Generate code for common data processing, API calls, or UI components.
* **Learning New APIs/Libraries:** Get suggestions as you explore unfamiliar libraries.
* **Refactoring and Debugging Assistance:** Aid in understanding and modifying existing code.
* **DevOps Automation:** Generate scripts for infrastructure as code (IaC) or CI/CD pipelines.

#### Parameters/Dependencies
* **IDE:** Compatible IDE with AWS Toolkit installed.
* **AWS Account/Builder ID:** For authentication and usage tracking.
* **Context:** The comments and code you write provide the context for suggestions.

#### Tool Navigations (AWS Console)
* No direct console navigation for CodeWhisperer's core functionality, as it's an IDE plugin.
* For Bedrock-based code assistance: Amazon Bedrock -> Playgrounds (Text, Chat).

#### CLI & Output Syntax Examples (CodeWhisperer is IDE-based, so no direct CLI)
\`\`\`python
# Example of how CodeWhisperer would provide suggestions in an IDE:

# User types:
# def calculate_factorial(n):
#     """Calculates the factorial of a number."""
#     # CodeWhisperer might suggest:
#     # if n == 0:
#     #     return 1
#     # else:
#     #     return n * calculate_factorial(n - 1)

# User types:
# # Connect to S3 and list buckets
# # CodeWhisperer might suggest:
# # import boto3
# # s3 = boto3.client('s3')
# # response = s3.list_buckets()
# # for bucket in response['Buckets']:
# #     print(f"  {bucket['Name']}")
\`\`\`

#### Common Issues and Solutions
* **Issue: Suggestions are not relevant.**
    * **Solution:** Provide more detailed and specific comments. Ensure your existing code provides enough context.
* **Issue: CodeWhisperer is not providing any suggestions.**
    * **Solution:** Check if the AWS Toolkit is correctly installed and enabled in your IDE. Verify your internet connection and AWS authentication.
* **Issue: Security concerns with generated code.**
    * **Solution:** Always review generated code carefully. Utilize CodeWhisperer's built-in security scans. Integrate static code analysis tools into your CI/CD pipeline.`,
                },
                {
                  "name": "Content Summarization",
                  "description": `Content Summarization is an NLP task that involves generating a concise and coherent summary of a longer text document while retaining its most important information. There are two main types:
* **Extractive Summarization:** Identifies and extracts key sentences or phrases directly from the original text to form the summary.
* **Abstractive Summarization:** Generates new sentences and phrases that capture the main ideas of the original text, potentially using words not present in the source. This often requires more advanced models like LLMs.

### AWS Implementation for Content Summarization
* **Amazon Comprehend:** Offers basic summarization capabilities, particularly for extracting key phrases and entities which can form the basis of an extractive summary.
* **Amazon Bedrock:** The most powerful option for abstractive summarization, leveraging Large Language Models (LLMs) like Amazon Titan Text or Anthropic Claude. You can send a document as a prompt and instruct the LLM to summarize it.
* **Amazon SageMaker:** For building and deploying custom summarization models, especially if you have domain-specific documents or require highly specialized summarization techniques. You can fine-tune pre-trained summarization models (e.g., from Hugging Face Transformers) on your own data.

---

#### How to Enable/Config (Bedrock LLM Example for Abstractive Summarization)
1.  **AWS Console:** Navigate to Amazon Bedrock.
2.  **Model Access:** Ensure you have access to an LLM suitable for summarization (e.g., Amazon Titan Text, Anthropic Claude).
3.  **API/SDK:** Use the AWS SDKs (e.g., Boto3 for Python) to send your text to the model with a summarization prompt.

#### Key Features
* **Information Condensation:** Quickly grasp the main points of long articles, reports, or conversations.
* **Efficiency:** Save time by not having to read entire documents.
* **Content Creation:** Generate short descriptions or previews for content.
* **Search and Discovery:** Improve search relevance by summarizing documents.

#### Parameters/Dependencies
* **Input Text:** The long document or text to be summarized.
* **Prompt (for LLMs):** Instructions for the LLM (e.g., "Summarize the following article:", "Provide a bullet-point summary of the key findings:").
* **Inference Parameters (for LLMs):**
    * \`max_tokens_to_sample\` (or \`maxTokenCount\`): Controls the length of the summary.
    * \`temperature\`: Influences how creative/abstract the summary is.
* **IAM Permissions:** \`bedrock:InvokeModel\`.

#### Tool Navigations (AWS Console)
* Amazon Bedrock -> Playgrounds -> Text (for LLM-based summarization)
* Amazon Comprehend (for key phrase extraction and basic text analysis)

#### CLI & Output Syntax Examples (Bedrock LLM for Summarization)
\`\`\`bash
# Example: Summarize text using Amazon Titan Text Express V1 via Bedrock Runtime API
aws bedrock-runtime invoke-model \\
    --model-id amazon.titan-text-express-v1 \\
    --body '{ "inputText": "Summarize the following article: The Amazon rainforest is the largest rainforest in the world...", "textGenerationConfig": { "maxTokenCount": 100, "temperature": 0.5 } }' \\
    --content-type application/json \\
    --accept application/json \\
    output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Summary is too long or too short.**
    * **Solution:** Adjust the \`max_tokens_to_sample\` parameter. Explicitly state desired length in the prompt (e.g., "Summarize in 3 sentences").
* **Issue: Summary misses key information or includes irrelevant details.**
    * **Solution:** Refine your prompt to emphasize what aspects are most important. For extractive summaries, ensure the source text is well-structured. For abstractive, consider fine-tuning the LLM on domain-specific summarization examples.
* **Issue: Hallucinations in abstractive summaries.**
    * **Solution:** While LLMs can hallucinate, good prompt engineering and providing clear context can mitigate this. For critical applications, human review of summaries is recommended.`,
                },
                {
                  "name": "Conversational AI (Advanced)",
                  "description": `Advanced Conversational AI systems go beyond simple chatbots to create highly intelligent virtual assistants capable of multi-turn dialogues, maintaining context, understanding nuanced language, and performing complex tasks. They leverage large language models (LLMs) and sophisticated dialogue management techniques.

### AWS Implementation for Advanced Conversational AI
* **Amazon Lex:** A service for building conversational interfaces (chatbots and voice assistants) into any application. While it can be used for simpler, intent-based bots, its integration with Lambda allows for complex backend logic.
* **Amazon Bedrock (with Agents for Bedrock):** This is the most powerful and flexible approach for advanced conversational AI on AWS.
    * **Foundation Models (FMs):** LLMs like Anthropic Claude or Amazon Titan Text provide the core natural language understanding and generation capabilities.
    * **Agents for Bedrock:** Allow you to build conversational agents that can perform multi-step tasks by orchestrating FM calls, interacting with your company systems (via Lambda functions), and retrieving information from knowledge bases (Retrieval Augmented Generation).
* **Amazon Kendra:** Can be integrated with Lex or Bedrock Agents to provide highly accurate answers from enterprise knowledge bases, reducing hallucinations.
* **Amazon Transcribe:** For converting speech to text in voice-enabled conversational AI.
* **Amazon Polly:** For converting text to speech for voice responses.

---

#### How to Enable/Config (Bedrock Agents Example)
1.  **AWS Console:** Navigate to Amazon Bedrock.
2.  **Model Access:** Ensure you have access to the LLMs you want your agent to use.
3.  **Create Agent:** In Bedrock, go to "Agents" and create a new agent.
4.  **Define Agent Capabilities:**
    * **Instructions:** Provide natural language instructions on what the agent should do.
    * **Action Groups:** Define API schemas (e.g., OpenAPI) for your backend systems (e.g., order management, customer support database). Implement the logic for these APIs in AWS Lambda functions.
    * **Knowledge Bases:** Connect to Amazon Kendra indexes or S3 buckets containing your data for RAG.
5.  **Test and Deploy:** Test the agent in the console and deploy it.

#### Key Features
* **Multi-turn Dialogue:** Maintain conversation history and context across multiple exchanges.
* **Context Handling:** Understand and remember relevant information from previous turns.
* **Task Automation:** Perform actions in backend systems (e.g., book a flight, check order status).
* **Knowledge Retrieval:** Answer questions by retrieving information from internal documents and databases.
* **Personalization:** Tailor responses based on user history or preferences.
* **Human-like Interaction:** Generate natural and coherent responses.

#### Parameters/Dependencies
* **Foundation Model:** The underlying LLM for NLU/NLG.
* **Action Group APIs:** Lambda functions and OpenAPI schemas for backend integrations.
* **Knowledge Bases:** Amazon Kendra indexes or S3 data sources.
* **IAM Roles:** Permissions for Bedrock, Lambda, Kendra, etc.

#### Tool Navigations (AWS Console)
* Amazon Bedrock -> Agents
* Amazon Lex
* Amazon Kendra
* AWS Lambda

#### CLI & Output Syntax Examples (Bedrock Agent Invocation - conceptual)
\`\`\`bash
# Example: Invoke a Bedrock Agent (typically via SDK for session management)
# This is a simplified CLI representation.
aws bedrock-agent-runtime invoke-agent \\
    --agent-id "YOUR_AGENT_ID" \\
    --agent-alias-id "YOUR_AGENT_ALIAS_ID" \\
    --session-id "user123" \\
    --input-text "I want to book a flight from New York to London for tomorrow."
\`\`\`

#### Common Issues and Solutions
* **Issue: Agent fails to understand user intent.**
    * **Solution:** Improve the instructions provided to the agent. Refine the OpenAPI schemas for action groups. Provide more diverse examples in the agent's configuration.
* **Issue: Agent provides incorrect information (hallucinations).**
    * **Solution:** Strongly rely on Knowledge Bases for factual information. Ensure your knowledge base is comprehensive and up-to-date. Implement guardrails and human-in-the-loop for sensitive tasks.
* **Issue: Difficulty integrating with backend systems.**
    * **Solution:** Ensure your Lambda functions correctly implement the logic defined in your action group schemas. Debug Lambda logs for errors.
* **Issue: Managing complex dialogue flows.**
    * **Solution:** Design clear conversational paths. Break down complex tasks into smaller, manageable action groups.`,
                },
                {
                  "name": "Prompt Engineering",
                  "description": `The art and science of crafting effective inputs (prompts) for large language models (LLMs) and other generative AI models to guide their behavior and elicit desired outputs. It involves structuring queries, providing context, defining constraints, and specifying formats to optimize model performance for specific tasks.

### AWS Implementation for Prompt Engineering
Prompt engineering is primarily a technique applied when interacting with Foundation Models (FMs) available through AWS services:
* **Amazon Bedrock Playground:** Provides an interactive environment in the AWS console to experiment with different prompts and model parameters for various FMs (text, chat, image generation). This is an excellent tool for iterative prompt refinement.
* **AWS SDKs (Boto3):** When integrating FMs into applications, you use SDKs to programmatically send prompts and receive responses. This allows for dynamic prompt construction and integration into larger workflows.
* **Amazon SageMaker JumpStart:** Offers pre-built solutions and notebooks that include examples of effective prompts for various tasks using FMs.
* **Amazon Q:** As a generative AI assistant, Amazon Q's effectiveness is heavily influenced by how users phrase their questions and instructions.

---

#### How to Enable/Config
1.  **Access Bedrock:** Ensure you have access to Amazon Bedrock and the desired Foundation Models.
2.  **Experiment in Playground:** Start with the Bedrock Playground to test and iterate on prompts.
3.  **Integrate with Code:** Once a prompt strategy is effective, implement it in your application code using AWS SDKs.

#### Key Techniques
* **Clear Instructions:** Provide explicit and unambiguous instructions.
* **Contextual Information:** Include relevant background information or data.
* **Examples (Few-shot Learning):** Give examples of desired input-output pairs to guide the model.
* **Role-Playing:** Instruct the model to act as a specific persona (e.g., "Act as a marketing expert...").
* **Constraints & Format:** Specify output length, format (e.g., JSON, bullet points), or style.
* **Chain-of-Thought Prompting:** Break down complex tasks into intermediate steps, guiding the model through a reasoning process.
* **Iterative Refinement:** Continuously test and refine prompts based on model outputs.

#### Parameters/Dependencies
* **Foundation Model:** The specific LLM or generative model being used.
* **Prompt Text:** The carefully constructed input string.
* **Inference Parameters:** Model-specific parameters (e.g., \`temperature\`, \`top_p\`, \`max_tokens_to_sample\`) that influence the creativity, diversity, and length of the output.

#### Tool Navigations (AWS Console)
* Amazon Bedrock -> Playgrounds (Text, Chat, Image).
* SageMaker Studio (for notebooks using FMs).

#### CLI & Output Syntax Examples (Conceptual)
\`\`\`bash
# Example: Using a prompt with Bedrock (via Boto3 Python SDK)
import boto3
import json

bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')

prompt = "Explain the concept of quantum entanglement in simple terms."
body = json.dumps({
    "inputText": prompt,
    "textGenerationConfig": {
        "maxTokenCount": 200,
        "temperature": 0.7,
        "topP": 0.9
    }
})

response = bedrock_runtime.invoke_model(
    modelId='amazon.titan-text-express-v1',
    body=body,
    contentType='application/json',
    accept='application/json'
)

response_body = json.loads(response.get('body').read())
generated_text = response_body.get('results')[0].get('outputText')
print(generated_text)
\`\`\`

#### Common Issues and Solutions
* **Issue: Model generates irrelevant or off-topic content.**
    * **Solution:** Make prompts more specific. Provide more context. Use negative constraints (e.g., "Do not include...").
* **Issue: Model output is too generic or lacks creativity.**
    * **Solution:** Adjust \`temperature\` (higher for more creativity). Experiment with \`top_p\` or \`top_k\`. Provide examples of desired creative output.
* **Issue: Model output is too long or too short.**
    * **Solution:** Adjust \`max_tokens_to_sample\` or other length-controlling parameters. Specify desired length in the prompt (e.g., "Write a 50-word summary.").
* **Issue: Model "hallucinates" (generates factually incorrect information).**
    * **Solution:** Implement Retrieval Augmented Generation (RAG) to ground the model's responses in verified data sources. Explicitly instruct the model to state when it doesn't know an answer.`,
                }
              ]
            },
            {
              "name": "Natural Language Processing (NLP)",
              "description": `Enables computers to understand, interpret, and generate human language. It involves the interaction between computers and human (natural) languages. Key libraries: NLTK, spaCy, Hugging Face Transformers.

### AWS Implementation for Natural Language Processing
AWS offers a wide array of managed NLP services, largely categorized into:
* **Amazon Comprehend:** For analyzing text (sentiment, entities, key phrases, language detection, custom classification/entity recognition).
* **Amazon Transcribe:** For converting speech to text.
* **Amazon Polly:** For converting text to lifelike speech.
* **Amazon Translate:** For real-time language translation.
* **Amazon Lex:** For building conversational interfaces (chatbots, voice assistants).
* **Amazon Textract:** For extracting text and data from documents.
* **Amazon Kendra:** Enterprise search service powered by ML.
* **Amazon SageMaker:** For building, training, and deploying custom NLP models (e.g., fine-tuning BERT, GPT models).

---

#### How to Enable/Config (Amazon Comprehend Example)
1.  **AWS Console:** Navigate to Amazon Comprehend.
2.  **Input Data:** Text data can be provided directly via API or stored in S3 for batch analysis.
3.  **IAM Permissions:** Ensure your user/role has \`comprehend:*\` permissions.

#### Key Features (Comprehend)
* **Sentiment Analysis:** Determine positive, negative, mixed, or neutral sentiment.
* **Entity Recognition:** Identify people, places, organizations, events, etc.
* **Key Phrase Extraction:** Extract important phrases.
* **Language Detection:** Automatically identify the language of text.
* **Custom Classification/Entities:** Train models for domain-specific text classification or entity recognition.

#### Parameters/Dependencies (Comprehend)
* **Text:** The input text string.
* **Language Code:** (e.g., \`en\`, \`es\`) for many operations.
* **S3 URI:** For batch processing.

#### Tool Navigations (AWS Console)
* Services -> Machine Learning -> Amazon Comprehend.
* Try out "Analyze text" in the console for quick tests.
* For custom models: Comprehend -> Customization -> Custom classification/entity recognizers.

#### CLI & Output Syntax Examples (Comprehend)
\`\`\`bash
# Example: Detect sentiment using Comprehend
aws comprehend detect-sentiment \\
    --text "I love Amazon Comprehend, it's amazing!" \\
    --language-code en

# Output example:
# {
#     "Sentiment": "POSITIVE",
#     "SentimentScore": {
#         "Positive": 0.9999,
#         "Negative": 0.0000,
#         "Neutral": 0.0000,
#         "Mixed": 0.0000
#     }
# }

# Example: Start a batch sentiment analysis job
aws comprehend start-sentiment-detection-job \\
    --input-data-config S3Uri="s3://your-comprehend-bucket/input/",InputFormat="ONE_DOC_PER_FILE" \\
    --output-data-config S3Uri="s3://your-comprehend-bucket/output/" \\
    --data-access-role-arn "arn:aws:iam::123456789012:role/ComprehendDataAccessRole" \\
    --job-name "MyBatchSentimentJob" \\
    --language-code en
\`\`\`

#### Common Issues and Solutions
* **Issue: Inaccurate sentiment/entity detection.**
    * **Solution:**
        * **Context:** Comprehend works best with clear, well-formed text. Ambiguous or highly domain-specific language might require custom models.
        * **Custom Models:** For domain-specific entities or classifications, train a Custom Entity Recognizer or Custom Classifier in Comprehend.
* **Issue: Large file size for batch processing.**
    * **Solution:** For batch jobs, ensure your input files are within the service limits (e.g., max 5000 documents per file, max 1MB per document for some operations). Split larger files if necessary.
* **Issue: Latency for real-time inference.**
    * **Solution:** For low-latency needs, ensure your application is in the same region as the Comprehend endpoint. Consider using AWS Lambda for event-driven processing.`,
              "children": [
                {
                  "name": "Natural Language Understanding (NLU)",
                  "description": "Focuses on enabling computers to comprehend the meaning of human language, including its nuances, context, and intent.",
                  "children": [
                    {"name": "Sentiment Analysis", "description": "Determining the emotional tone or sentiment expressed in a piece of text (e.g., positive, negative, neutral)."},
                    {"name": "Named Entity Recognition (NER)", "description": "Identifying and classifying named entities (e.g., people, organizations, locations, dates) in text into predefined categories."},
                    {"name": "Text Classification", "description": "Categorizing text documents into one or more predefined classes (e.g., news articles by topic, email spam detection)."},
                    {"name": "Machine Translation", "description": "Automatically translating text or speech from one natural language to another. Modern approaches heavily rely on neural networks (e.g., Google Translate, DeepL)."}
                  ]
                },
                {
                  "name": "Natural Language Generation (NLG)",
                  "description": "Focuses on enabling computers to generate human-like text from structured data or other inputs. It's the opposite of NLU.",
                  "children": [
                    {"name": "Text Summarization", "description": "Creating concise and coherent summaries of longer texts, either extractive (picking key sentences) or abstractive (generating new sentences)."},
                    {"name": "Dialogue Systems (Chatbots)", "description": "AI systems designed to converse with humans in a natural language, ranging from rule-based systems to advanced conversational AI powered by large language models."}
                  ]
                },
                {"name": "Speech Recognition", "description": "Converting spoken language into text. This is a crucial component for voice assistants and dictation software. (e.g., Google Speech-to-Text, Amazon Transcribe)."},
                {"name": "Text-to-Speech", "description": "Converting text into spoken language. This is used in screen readers, voice assistants, and audiobooks (e.g., Google Text-to-Speech, Amazon Polly)."}
              ]
            },
            {
              "name": "Computer Vision",
              "description": `Enables computers to 'see,' interpret, and process visual information from the world (images, videos). It involves tasks like image acquisition, processing, analysis, and understanding. Key frameworks: OpenCV, scikit-image.

### AWS Implementation for Computer Vision
AWS provides several services for Computer Vision:
* **Amazon Rekognition:** A fully managed service that provides pre-trained and customizable computer vision capabilities to identify objects, people, text, scenes, and activities in images and videos.
* **Amazon SageMaker:** For building, training, and deploying custom computer vision models (e.g., training custom CNNs for specific object detection or image classification tasks).
* **AWS Panorama:** For bringing computer vision to on-premises cameras to improve operations.
* **Amazon Textract:** Specifically for extracting text and structured data from documents.

---

#### How to Enable/Config (Amazon Rekognition Example)
1.  **AWS Console:** Navigate to Amazon Rekognition.
2.  **Input Data:** Images/videos can be uploaded directly via console/API or referenced from S3.
3.  **IAM Permissions:** Ensure your user/role has \`rekognition:*\` permissions.

#### Key Features (Rekognition)
* **Object and Scene Detection:** Identify thousands of objects, scenes, and activities.
* **Face Analysis:** Detect faces, emotions, attributes, and compare faces.
* **Text in Image:** Detect and recognize text in images.
* **Moderation:** Detect inappropriate content.
* **Custom Labels:** Train custom models to identify objects and scenes specific to your business needs (e.g., identifying specific product defects).

#### Parameters/Dependencies (Rekognition)
* **Image/Video Input:** Base64 encoded image bytes, or S3 URI for image/video.
* **Features:** Specify which features to detect (e.g., \`"Faces"\`, \`"Labels"\`, \`"Text"\`).
* **Min Confidence:** Threshold for detection confidence.

#### Tool Navigations (AWS Console)
* Services -> Machine Learning -> Amazon Rekognition.
* Explore "Try Demo" sections for various features.
* For custom models: Rekognition -> Custom Labels.

#### CLI & Output Syntax Examples (Rekognition)
\`\`\`bash
# Example: Detect labels (objects, scenes) in an S3 image
aws rekognition detect-labels \\
    --image '{"S3Object":{"Bucket":"my-rekognition-bucket","Name":"sample.jpg"}}' \\
    --max-labels 10 \\
    --min-confidence 70

# Output example (truncated):
# {
#     "Labels": [
#         {
#             "Name": "Outdoors",
#             "Confidence": 99.99,
#             "Instances": [],
#             "Parents": []
#         },
#         {
#             "Name": "Nature",
#             "Confidence": 99.99,
#             "Instances": [],
#             "Parents": []
#         },
#         ...
#     ],
#     "OrientationCorrection": "ROTATE_0"
# }

# Example: Detect faces in an image
aws rekognition detect-faces \\
    --image '{"S3Object":{"Bucket":"my-rekognition-bucket","Name":"face.jpg"}}' \\
    --attributes "ALL"
\`\`\`

#### Common Issues and Solutions
* **Issue: Low detection accuracy for specific objects.**
    * **Solution:**
        * **Custom Labels:** If pre-trained Rekognition doesn't meet needs, train a Custom Labels model with your specific data.
        * **Image Quality:** Ensure input images are clear, well-lit, and show the objects clearly.
* **Issue: Cost optimization for large-scale video analysis.**
    * **Solution:** For video, use asynchronous operations (\`start-label-detection\`, \`get-label-detection\`) which are typically more cost-effective than frame-by-frame analysis. Optimize video resolution and frame rates if possible.
* **Issue: Real-time latency for video streams.**
    * **Solution:** For ultra-low latency, consider edge processing with AWS Panorama or using Kinesis Video Streams with Rekognition Stream Processors.`,
              "children": [
                {"name": "Image Classification", "description": "Categorizing entire images based on their content (e.g., 'this is a cat,' 'this is a car')."},
                {"name": "Object Detection", "description": "Identifying and locating specific objects within an image or video, often drawing bounding boxes around them (e.g., 'there is a car at these coordinates'). Popular models: YOLO, Faster R-CNN."},
                {"name": "Image Segmentation", "description": "Dividing an image into multiple segments or regions, often at a pixel level, to identify objects or boundaries more precisely (e.g., 'these pixels belong to the car')."},
                {"name": "Facial Recognition", "description": "Identifying or verifying individuals from their facial features. Used in security, authentication, and surveillance."},
                {"name": "Pose Estimation", "description": "Determining the position and orientation of objects or body parts in an image or video, often by identifying keypoints."},
                {"name": "Medical Imaging Analysis", "description": "Applying computer vision techniques to analyze medical scans (X-rays, MRIs, CTs) for diagnosis, disease detection, and treatment planning."}
              ]
            },
            {
              "name": "Robotics",
              "description": "The interdisciplinary branch of engineering and computer science that deals with the design, construction, operation, and use of robots. It integrates AI for intelligent decision-making and perception.",
              "children": [
                {"name": "Perception", "description": "Enabling robots to understand their environment using sensors (e.g., cameras, lidar, sonar), often leveraging Computer Vision and sensor fusion techniques."},
                {"name": "Motion Planning", "description": "Algorithms that determine the optimal path for a robot to move from a starting point to a destination while avoiding obstacles and respecting constraints."},
                {"name": "Control Systems", "description": "The mechanisms and algorithms that ensure the robot executes actions precisely and safely, maintaining stability and achieving desired movements."},
                {"name": "Human-Robot Interaction (HRI)", "description": "Designing robots that can effectively, naturally, and safely interact with humans, including communication, collaboration, and understanding human intent."}
              ]
            },
            {
              "name": "Expert Systems",
              "description": "Early AI systems that mimic the decision-making ability of a human expert within a narrow domain. They use a knowledge base and an inference engine to solve problems that require human expertise.",
              "children": [
                {"name": "Knowledge Representation", "description": "How domain-specific knowledge (facts, rules, heuristics) is stored and organized within the system, typically using symbolic methods like rules, frames, or semantic nets."},
                {"name": "Inference Engines", "description": "The component that reasons with the knowledge base to draw conclusions, make recommendations, or solve problems, often using forward or backward chaining."}
              ]
            },
            {
              "name": "Planning and Scheduling",
              "description": `Planning and Scheduling are AI techniques focused on creating optimal sequences of actions to achieve specific goals, often in complex, dynamic environments.
* **Planning** involves determining a sequence of actions from a starting state to a goal state.
* **Scheduling** involves allocating resources and timing activities to execute a plan efficiently, respecting constraints.

### AWS Implementation for Planning and Scheduling
While AWS doesn't have a direct "Planning and Scheduling" service, its services can be used to build and orchestrate such systems:
* **AWS Step Functions:** Ideal for orchestrating complex workflows and state machines, which can represent planning sequences. You can define a series of steps (Lambda functions, SageMaker jobs, etc.) that execute in a specific order.
* **AWS Lambda:** For implementing custom planning algorithms or decision logic that determines the next best action.
* **Amazon SageMaker:** For training machine learning models that can assist in planning (e.g., predicting optimal resource allocation) or for reinforcement learning agents that learn planning strategies.
* **Amazon DynamoDB/RDS:** For storing state information, resource availability, and historical data relevant to planning and scheduling.
* **Amazon SQS/SNS:** For asynchronous communication and triggering planning/scheduling events.

---

#### How to Enable/Config (Step Functions Example)
1.  **AWS Console:** Navigate to AWS Step Functions.
2.  **Define State Machine:** Create a new state machine using the Workflow Studio or ASL (Amazon States Language). Define states for actions, choices, and parallel execution.
3.  **Integrate Lambda/SageMaker:** Connect your state machine to Lambda functions (for custom logic) or SageMaker endpoints/training jobs (for ML-driven decisions).
4.  **Execute:** Start executions manually, via API, or triggered by other AWS services (e.g., CloudWatch Events).

#### Key Features
* **Workflow Orchestration:** Automate complex, multi-step processes.
* **State Management:** Track the progress and state of long-running tasks.
* **Scalability:** Scale to handle a large number of concurrent plans or schedules.
* **Flexibility:** Combine various AWS services to build custom solutions.

#### Examples of Algorithms
* **A* Search:** A popular graph traversal and pathfinding algorithm used in planning to find the shortest path between two nodes in a graph.
* **STRIPS (STanford Research Institute Problem Solver):** An early AI planning system that uses a set of states, actions (with preconditions and effects), and goals to generate plans.
* **Hierarchical Task Network (HTN) Planning:** Breaks down complex problems into smaller, more manageable sub-problems.

#### Parameters/Dependencies
* **Initial State:** The starting conditions for the planning problem.
* **Goal State:** The desired outcome.
* **Actions/Operators:** Defined actions with their preconditions and effects.
* **Resources:** Available resources and their constraints (for scheduling).
* **IAM Roles:** Permissions for Step Functions to invoke other services.

#### Tool Navigations (AWS Console)
* AWS Step Functions -> State machines
* AWS Lambda -> Functions
* Amazon SageMaker

#### CLI & Output Syntax Examples (Step Functions)
\`\`\`bash
# Example: Create a Step Functions State Machine (simplified ASL)
aws stepfunctions create-state-machine \\
    --name "MyPlanningWorkflow" \\
    --definition "{\"Comment\":\"A simple planning workflow.\",\"StartAt\":\"ProcessInput\",\"States\":{\"ProcessInput\":{\"Type\":\"Task\",\"Resource\":\"arn:aws:lambda:REGION:ACCOUNT_ID:function:MyPlanningLambda\",\"End\":true}}}" \\
    --role-arn "arn:aws:iam::123456789012:role/StepFunctionsExecutionRole"

# Example: Start a Step Functions execution
aws stepfunctions start-execution \\
    --state-machine-arn "arn:aws:states:REGION:ACCOUNT_ID:stateMachine:MyPlanningWorkflow" \\
    --input "{\"start_location\":\"A\",\"end_location\":\"B\",\"available_resources\":[\"truck1\",\"driver1\"]}"
\`\`\`

#### Common Issues and Solutions
* **Issue: Complex planning problems lead to long computation times.**
    * **Solution:** Use heuristic search algorithms to prune the search space. Break down large problems into smaller sub-problems. Leverage distributed computing with Lambda or EC2.
* **Issue: Dynamic environments invalidate plans.**
    * **Solution:** Implement reactive planning where the plan can be adjusted based on real-time sensor data or events. Use event-driven architectures with SQS/SNS.
* **Issue: Resource conflicts in scheduling.**
    * **Solution:** Use optimization algorithms to find schedules that minimize conflicts. Implement constraint programming techniques.`,
            },
            {
              "name": "Search and Optimization",
              "description": `Search and Optimization are fundamental AI techniques used to find optimal or near-optimal solutions to problems by systematically exploring a search space.
* **Search Algorithms:** Explore a set of states to find a path to a goal state (e.g., in pathfinding, game AI).
* **Optimization Algorithms:** Find the best possible solution from a set of alternatives, often by minimizing or maximizing an objective function (e.g., hyperparameter tuning, resource allocation).

### AWS Implementation for Search and Optimization
AWS provides various services that can be leveraged for implementing search and optimization algorithms:
* **Amazon SageMaker:**
    * **Automatic Model Tuning (AMT):** SageMaker's built-in hyperparameter optimization uses search and optimization algorithms (e.g., Bayesian optimization, random search) to find the best set of hyperparameters for your ML models.
    * **Training Jobs:** You can run custom optimization algorithms (e.g., gradient descent variants, evolutionary algorithms) as SageMaker training jobs on scalable compute resources.
* **AWS Lambda/EC2:** For running custom search algorithms (e.g., A*, Breadth-First Search) or optimization solvers for smaller, less compute-intensive problems.
* **Amazon S3:** For storing large datasets used by optimization algorithms or for storing search spaces/results.
* **AWS Step Functions:** To orchestrate complex optimization workflows that involve multiple steps or iterative search processes.

---

#### How to Enable/Config (SageMaker Hyperparameter Tuning Example)
1.  **AWS Console:** Navigate to Amazon SageMaker.
2.  **Hyperparameter Tuning Jobs:** Create a new tuning job.
3.  **Define Search Space:** Specify the range of hyperparameters to search (e.g., learning rate, batch size).
4.  **Define Objective Metric:** Specify the metric to optimize (e.g., validation accuracy, loss).
5.  **Choose Strategy:** Select a tuning strategy (e.g., Bayesian, Random).
6.  **Run Tuning Job:** SageMaker will launch multiple training jobs to find the best hyperparameters.

#### Key Features
* **Automated Hyperparameter Tuning:** Efficiently find optimal model configurations.
* **Scalable Compute:** Run complex search and optimization tasks on distributed infrastructure.
* **Algorithm Flexibility:** Implement a wide range of search and optimization algorithms.

#### Examples of Algorithms
* **Heuristic Search Algorithms:**
    * **A* Search:** Finds the shortest path in a graph using a heuristic function.
    * **Breadth-First Search (BFS):** Explores all nodes at the current depth level before moving on to nodes at the next depth level.
    * **Depth-First Search (DFS):** Explores as far as possible along each branch before backtracking.
* **Optimization Algorithms:**
    * **Gradient Descent:** Iteratively adjusts parameters to minimize a cost function.
    * **Stochastic Gradient Descent (SGD):** A variant of gradient descent that updates parameters using a single training example at a time.
    * **Simulated Annealing:** A probabilistic technique for approximating the global optimum of a given function.
    * **Genetic Algorithms:** (See Evolutionary Computation) Use principles of natural selection for optimization.

#### Parameters/Dependencies
* **Objective Function:** The function to minimize or maximize.
* **Search Space:** The range of possible solutions or parameters.
* **Constraints:** Any limitations on the solutions.
* **Compute Resources:** Appropriate instance types (CPU or GPU).

#### Tool Navigations (AWS Console)
* Amazon SageMaker -> Hyperparameter tuning jobs
* AWS Lambda
* Amazon EC2

#### CLI & Output Syntax Examples (SageMaker Hyperparameter Tuning)
\`\`\`bash
# Example: Create a SageMaker Hyperparameter Tuning Job (simplified)
aws sagemaker create-hyper-parameter-tuning-job \\
    --hyper-parameter-tuning-job-name "my-model-tuning-job" \\
    --hyper-parameter-tuning-job-config '{
        "Strategy": "Bayesian",
        "HyperParameterRanges": {
            "CategoricalParameterRanges": [],
            "ContinuousParameterRanges": [
                {"Name": "learning_rate", "MinValue": "0.0001", "MaxValue": "0.1", "ScalingType": "Logarithmic"}
            ],
            "IntegerParameterRanges": [
                {"Name": "epochs", "MinValue": "10", "MaxValue": "50", "ScalingType": "Linear"}
            ]
        },
        "ResourceLimits": {"MaxNumberOfTrainingJobs": 10, "MaxParallelTrainingJobs": 2},
        "ObjectiveMetric": {"MetricName": "validation:accuracy", "Type": "Maximize"}
    }' \\
    --training-job-definition '{
        "AlgorithmSpecification": {"TrainingImage": "<your-training-image-uri>", "TrainingInputMode": "File"},
        "RoleArn": "arn:aws:iam::123456789012:role/SageMakerExecutionRole",
        "InputDataConfig": [{"ChannelName": "train", "DataSource": {"S3DataSource": {"S3DataType": "S3Prefix", "S3Uri": "s3://your-s3-bucket/train/"}}}],
        "OutputDataConfig": {"S3OutputPath": "s3://your-s3-bucket/output/"},
        "ResourceConfig": {"InstanceCount": 1, "InstanceType": "ml.m5.large", "VolumeSizeInGB": 30},
        "StoppingCondition": {"MaxRuntimeInSeconds": 3600}
    }'
\`\`\`

#### Common Issues and Solutions
* **Issue: Tuning job takes too long.**
    * **Solution:** Increase \`MaxParallelTrainingJobs\`. Reduce the search space. Use a more efficient tuning strategy (e.g., Bayesian optimization over random search).
* **Issue: Tuning job doesn't find a good solution.**
    * **Solution:** Expand the search range for hyperparameters. Ensure your objective metric accurately reflects desired performance. Check for issues in your training script.
* **Issue: High costs.**
    * **Solution:** Monitor resource usage. Use smaller instance types for initial tuning. Set strict stopping conditions.`,
            },
            {
              "name": "Knowledge Representation and Reasoning",
              "description": "Focuses on how to represent information about the world in a form that a computer system can use to solve complex tasks, and how to derive new knowledge from existing knowledge using logical inference. (e.g., Ontologies, Semantic Web).",
              "children": [
                {
                  "name": "Knowledge Graphs & Symbolic AI",
                  "description": `**Knowledge Graphs:** Structured representations of knowledge that store entities (nodes) and their relationships (edges) in a graph format. They enable machines to understand complex relationships and context, facilitating more intelligent reasoning and data integration.
**Symbolic AI:** An older paradigm of AI that focuses on representing knowledge in explicit, human-readable symbols and rules, and then manipulating these symbols to perform reasoning. It's based on logic and expert systems, contrasting with the statistical nature of machine learning.

### AWS Implementation for Knowledge Graphs & Symbolic AI
* **Amazon Neptune:** A fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune supports popular graph models (Property Graph and RDF) and query languages (Gremlin and SPARQL). It's ideal for building knowledge graphs.
* **AWS Glue:** Can be used for ETL to transform data into a graph-compatible format before loading into Neptune.
* **Amazon S3:** For storing source data for knowledge graphs.
* **AWS Lambda:** For building serverless applications that interact with Neptune for reasoning or data loading.
* **Custom Implementations:** For purely symbolic AI systems, you would typically implement logic programming or rule-based systems on EC2 instances or within serverless functions.

---

#### How to Enable/Config (Amazon Neptune Example)
1.  **AWS Console:** Navigate to Amazon Neptune.
2.  **Create Cluster:** Create a Neptune DB cluster, specifying instance types and number of instances.
3.  **Load Data:** Load data into Neptune using bulk loading from S3 (e.g., using Neptune's S3 bulk loader) or through API/SDK.
4.  **Query Data:** Use Gremlin or SPARQL endpoints to query the graph.

#### Key Features
* **Graph Database:** Optimized for highly connected data.
* **Scalability:** Horizontally scalable for reads and storage.
* **High Performance:** Fast query execution for graph traversals.
* **Security:** Encryption at rest and in transit, VPC integration.
* **Reasoning:** Enables complex logical queries and inference over relationships.

#### Parameters/Dependencies (Neptune)
* **Instance Type:** \`db.r5.large\`, \`db.r5.xlarge\`, etc.
* **Number of Instances:** For read replicas and high availability.
* **VPC:** Deploy Neptune within a VPC for network isolation.
* **IAM Roles:** Permissions for Neptune, S3 (for bulk loading).
* **Query Language:** Gremlin or SPARQL.

#### Tool Navigations (AWS Console)
* Services -> Databases -> Amazon Neptune.
* Neptune -> Clusters.

#### CLI & Output Syntax Examples (Neptune - conceptual)
\`\`\`bash
# Example: Create a Neptune DB Cluster (simplified)
aws neptune create-db-cluster \\
    --db-cluster-identifier my-knowledge-graph \\
    --engine neptune \\
    --engine-version 1.2.0.0 \\
    --db-subnet-group-name my-neptune-subnet-group \\
    --vpc-security-group-ids sg-xxxxxxxxxxxxxxxxx \\
    --master-username admin --master-user-password <your-password> \\
    --skip-final-snapshot

# Example: Create a Neptune DB Instance
aws neptune create-db-instance \\
    --db-cluster-identifier my-knowledge-graph \\
    --db-instance-identifier my-kg-instance-1 \\
    --db-instance-class db.r5.large \\
    --engine neptune

# Example: Get cluster endpoints
aws neptune describe-db-clusters --db-cluster-identifier my-knowledge-graph
\`\`\`

#### Common Issues and Solutions
* **Issue: Slow query performance.**
    * **Solution:** Optimize graph schema. Ensure proper indexing. Scale up instance types or add read replicas.
* **Issue: Data loading errors.**
    * **Solution:** Ensure data format (e.g., CSV, N-Triples) is correct for bulk loading. Check IAM permissions for S3 access.
* **Issue: Complexity of graph queries.**
    * **Solution:** Utilize graph visualization tools. Break down complex queries into smaller, manageable parts. Consult Gremlin/SPARQL documentation.`,
                }
              ]
            },
            {
              "name": "Evolutionary Computation",
              "description": `Evolutionary Computation is a family of AI algorithms inspired by biological evolution (e.g., natural selection, mutation, crossover, survival of the fittest). These algorithms are used for optimization and search problems, evolving candidate solutions over successive "generations" to find optimal or near-optimal solutions. They are particularly effective for complex problems where traditional optimization methods struggle.

### AWS Implementation for Evolutionary Computation
AWS provides the scalable compute resources necessary to run computationally intensive evolutionary algorithms:
* **Amazon EC2:** The most common way to run evolutionary algorithms. You can provision powerful instances (CPU or GPU, depending on your workload) to run your custom evolutionary computation frameworks (e.g., DEAP, PyGAD).
* **AWS Batch:** For running large-scale evolutionary computation experiments that can be broken down into many independent tasks (e.g., evaluating a population of solutions). AWS Batch can manage the compute resources and job queues.
* **Amazon SageMaker:** While not a direct service for evolutionary computation, you can use SageMaker's custom training jobs to run evolutionary algorithms, especially for tasks like neural architecture search (NAS) or hyperparameter optimization where evolutionary strategies might be applied.
* **Amazon S3:** For storing large datasets, intermediate populations, and final evolved solutions.
* **AWS Step Functions/Lambda:** For orchestrating multi-step evolutionary processes, such as managing generations, fitness evaluations, and population updates.

---

#### How to Enable/Config (EC2 Example)
1.  **AWS Console:** Navigate to Amazon EC2.
2.  **Launch Instance:** Launch an EC2 instance (e.g., a C5 or M5 instance for CPU-bound tasks, or a G4dn/P3 instance for GPU-bound tasks if your algorithm uses deep learning).
3.  **Install Framework:** Install your preferred evolutionary computation framework (e.g., DEAP, PyGAD) and any dependencies.
4.  **Run Code:** Execute your evolutionary algorithm script on the EC2 instance.

#### Key Features
* **Global Optimization:** Effective at finding global optima in complex, non-convex search spaces.
* **Robustness:** Less prone to getting stuck in local optima compared to gradient-based methods.
* **Parallelization:** Many evolutionary algorithms are inherently parallelizable, making them suitable for distributed computing on AWS.
* **Adaptability:** Can be applied to a wide variety of problems without requiring specific knowledge of the problem's mathematical structure.

#### Use Cases
* **Neural Architecture Search (NAS):** Automatically designing optimal neural network architectures.
* **Hyperparameter Optimization:** Finding the best hyperparameters for machine learning models.
* **Feature Selection:** Identifying the most relevant features for a predictive model.
* **Robotics:** Optimizing robot control policies or physical designs.
* **Financial Modeling:** Optimizing trading strategies.

#### Parameters/Dependencies
* **Population Size:** Number of candidate solutions in each generation.
* **Number of Generations:** How many iterations the evolution runs for.
* **Fitness Function:** A function that evaluates the quality of each candidate solution.
* **Selection, Crossover, Mutation Operators:** Define how new generations are created.
* **Compute Resources:** CPU or GPU instances with sufficient memory.

#### Tool Navigations (AWS Console)
* Amazon EC2 -> Instances
* AWS Batch -> Job definitions, Job queues, Jobs
* Amazon SageMaker -> Training jobs (for custom algorithms)

#### CLI & Output Syntax Examples (EC2 - conceptual)
\`\`\`bash
# Example: Launch an EC2 instance for evolutionary computation
aws ec2 run-instances \\
    --image-id ami-0abcdef1234567890 \\ # Replace with a suitable AMI
    --count 1 \\
    --instance-type t3.medium \\
    --key-name MyKeyPair \\
    --security-group-ids sg-xxxxxxxxxxxxxxxxx \\
    --user-data file://install_ec_software.sh # Script to install dependencies

# Example content of install_ec_software.sh:
#!/bin/bash
sudo apt-get update
sudo apt-get install -y python3-pip
pip3 install deap # Example: install DEAP framework
# Add commands to download your EC script and run it
\`\`\`

#### Common Issues and Solutions
* **Issue: Slow convergence or high computational cost.**
    * **Solution:** Optimize the fitness function. Use more efficient genetic operators. Leverage parallel processing with AWS Batch or distributed computing on EC2.
* **Issue: Premature convergence (getting stuck in local optima).**
    * **Solution:** Increase population size. Adjust mutation rates. Use more diverse initialization strategies.
* **Issue: Difficulty designing effective fitness functions.**
    * **Solution:** Clearly define the problem's objective. Break down complex objectives into measurable components.
* **Issue: Debugging complex evolutionary processes.**
    * **Solution:** Implement robust logging of population statistics, best solutions, and fitness values across generations. Visualize the evolution process.`,
              "children": [
                {
                  "name": "Genetic Algorithms",
                  "description": `Genetic Algorithms (GAs) are a widely used type of evolutionary algorithm that mimics the process of natural selection to find optimal or near-optimal solutions to optimization and search problems. They operate on a population of candidate solutions, often represented as "chromosomes" or "genotypes," and evolve them over successive "generations" through processes analogous to biological evolution: selection, crossover (recombination), and mutation.

### AWS Implementation for Genetic Algorithms
AWS provides scalable compute infrastructure to run Genetic Algorithms, which can be computationally intensive, especially for large populations or many generations:
* **Amazon EC2:** The most common and flexible way to run GAs. You can provision various instance types (CPU-optimized like C5/C6gn, or memory-optimized like R5/R6g) to execute your GA code written in Python (e.g., with libraries like DEAP, PyGAD, or your custom implementation).
* **AWS Batch:** Ideal for running large-scale GA experiments. You can submit jobs that evaluate individual solutions or entire generations, and Batch will manage the underlying EC2 instances, scaling up and down as needed. This is particularly useful when fitness evaluation is a time-consuming step.
* **Amazon SageMaker:** While primarily for traditional ML, you could potentially use SageMaker's custom training jobs to run GAs, especially if your GA is being used for hyperparameter optimization or neural architecture search for a machine learning model.
* **Amazon S3:** For storing input data, intermediate populations, fitness evaluation results, and the best-evolved solutions.
* **AWS Lambda/Step Functions:** For orchestrating the GA workflow, such as triggering the start of a new generation, aggregating results, or managing the population state, especially in a serverless context.

---

#### How to Enable/Config (EC2 Example for a Python GA)
1.  **AWS Console:** Navigate to Amazon EC2.
2.  **Launch Instance:** Choose an appropriate instance type (e.g., \`t3.medium\` for small tests, \`c5.large\` for more compute). Select a suitable Amazon Machine Image (AMI) (e.g., Amazon Linux 2, Ubuntu).
3.  **SSH into Instance:** Connect to your EC2 instance using SSH.
4.  **Install Python & Libraries:** Install Python and your chosen GA library (e.g., \`pip install deap\`).
5.  **Upload & Run Code:** Upload your Python GA script to the instance and execute it.

#### Key Features
* **Population-Based Search:** Explores multiple solutions simultaneously, reducing the chance of getting stuck in local optima.
* **Heuristic Optimization:** Does not require the objective function to be differentiable or continuous.
* **Parallelizable:** Many steps (e.g., fitness evaluation of individuals in a population) can be run in parallel.
* **Robustness:** Can handle noisy data and complex, high-dimensional search spaces.

#### Core Strategies & Operators
* **Representation:** How a candidate solution is encoded (e.g., binary strings, real-valued vectors, permutations).
* **Fitness Function:** Evaluates the "goodness" of a solution. This is the objective function you want to optimize.
* **Selection:** Chooses individuals from the current population to be parents for the next generation (e.g., roulette wheel, tournament selection).
* **Crossover (Recombination):** Combines genetic material from two parents to create new offspring (e.g., one-point, two-point, uniform crossover).
* **Mutation:** Randomly alters a small part of an individual's genetic material to introduce diversity (e.g., bit flip, Gaussian mutation).
* **Elitism:** Preserving the best individuals from one generation to the next without modification.

#### Parameters/Dependencies
* **Population Size:** The number of individuals in each generation.
* **Number of Generations:** The total iterations for the algorithm to run.
* **Crossover Rate:** Probability that two parents will undergo crossover.
* **Mutation Rate:** Probability that an individual's gene will be mutated.
* **Compute Resources:** CPU or GPU, depending on the complexity of fitness evaluation.

#### Tool Navigations (AWS Console)
* Amazon EC2 -> Instances
* AWS Batch -> Job definitions, Job queues, Jobs
* Amazon S3 -> Buckets

#### CLI & Output Syntax Examples (Conceptual Python GA on EC2)
\`\`\`python
# Example Python script (my_ga_script.py) for a simple GA using DEAP
# This would be run on an EC2 instance.

from deap import base, creator, tools, algorithms
import random

# 1. Define the fitness function (e.g., maximizing a simple sum)
def eval_func(individual):
    return sum(individual),

# 2. Setup DEAP structures
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
# Attribute generator: generate 0 or 1
toolbox.register("attr_bool", random.randint, 0, 1)
# Structure initializers: define individual and population
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=10)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# 3. Register genetic operators
toolbox.register("evaluate", eval_func)
toolbox.register("mate", tools.cxTwoPoint) # Two-point crossover
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05) # Flip bit mutation
toolbox.register("select", tools.selTournament, tournsize=3) # Tournament selection

def main():
    population = toolbox.population(n=50) # Initial population of 50 individuals
    
    # Run the genetic algorithm
    algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=40, stats=None, halloffame=None, verbose=True)
    
    best_ind = tools.selBest(population, 1)[0]
    print(f"Best individual is {best_ind}, with fitness {best_ind.fitness.values}")

if __name__ == "__main__":
    main()

# To run this on EC2 after SSHing:
# python3 my_ga_script.py
\`\`\`

#### Common Issues and Solutions
* **Issue: Premature convergence (getting stuck in a suboptimal solution).**
    * **Solution:** Increase mutation rate to introduce more diversity. Increase population size. Use different selection or crossover operators. Implement techniques like niching or speciation.
* **Issue: Slow execution time.**
    * **Solution:** Optimize the fitness function (it's often the bottleneck). Reduce population size or number of generations for initial tests. Use more powerful EC2 instances. Distribute the workload using AWS Batch or a custom distributed framework.
* **Issue: Difficulty in defining the "chromosome" or representation.**
    * **Solution:** Carefully consider how to encode your problem's solution into a format suitable for genetic operations. This is often problem-specific.
* **Issue: Choosing optimal genetic algorithm parameters (population size, rates).**
    * **Solution:** These are often found through experimentation and heuristics. Start with common values and tune them. You could even use another optimization algorithm (like SageMaker's HPO) to tune the GA's parameters!`,
                }
              ]
            },
            {
              "name": "Fuzzy Logic",
              "description": `Fuzzy Logic is a form of multi-valued logic that deals with reasoning that is approximate rather than fixed and exact. Unlike classical Boolean logic (true/false, 0/1), fuzzy logic allows for "degrees of truth" (values between 0 and 1). It's particularly useful for controlling systems that involve imprecise, uncertain, or subjective information, mimicking human-like reasoning.

### AWS Implementation for Fuzzy Logic
AWS doesn't offer a specific managed service for Fuzzy Logic, but you can implement fuzzy logic systems using various AWS compute services:
* **AWS Lambda:** Ideal for serverless implementation of fuzzy inference systems (FIS). You can write Lambda functions in Python, Node.js, or Java to define fuzzy sets, membership functions, fuzzy rules, and defuzzification methods. This is suitable for event-driven, real-time decision-making.
* **Amazon EC2:** For more complex or computationally intensive fuzzy logic applications, you can deploy custom fuzzy logic frameworks (e.g., scikit-fuzzy in Python, jFuzzyLogic in Java) on EC2 instances. This provides full control over the environment.
* **AWS IoT Core/Greengrass:** For deploying fuzzy logic directly to edge devices (e.g., IoT sensors, actuators) for local, real-time control and decision-making, reducing latency and reliance on cloud connectivity.
* **Amazon DynamoDB/RDS:** For storing fuzzy rules, membership function parameters, or historical data used by the fuzzy system.

---

#### How to Enable/Config (Lambda Example for a Simple FIS)
1.  **AWS Console:** Navigate to AWS Lambda.
2.  **Create Function:** Create a new Lambda function (e.g., Python 3.9 runtime).
3.  **Upload Code:** Write your fuzzy logic code (e.g., using a library like \`scikit-fuzzy\` which you'd bundle in a Lambda layer or a deployment package).
4.  **Configure Trigger:** Set up an API Gateway endpoint, an IoT Core rule, or a CloudWatch event to trigger your Lambda function.

#### Key Concepts
* **Fuzzy Sets:** Sets where elements can have degrees of membership, represented by membership functions (e.g., "tall" as a fuzzy set for height).
* **Membership Functions:** Map input values to a degree of membership in a fuzzy set (e.g., triangular, trapezoidal, Gaussian).
* **Fuzzy Rules (IF-THEN Rules):** Express relationships between fuzzy sets (e.g., "IF temperature is HOT AND humidity is HIGH THEN fan speed is FAST").
* **Fuzzy Inference System (FIS):** The core of a fuzzy logic system, comprising fuzzification, rule evaluation, and defuzzification.
* **Fuzzification:** Converting crisp (precise) inputs into fuzzy values.
* **Defuzzification:** Converting fuzzy outputs back into a crisp, actionable value.

#### Use Cases
* **Control Systems:** Temperature control, motor speed control, washing machine cycles (e.g., "dirty" water).
* **Decision Making:** Medical diagnosis, financial risk assessment, loan approval.
* **Pattern Recognition:** Image processing, anomaly detection.
* **Expert Systems:** Handling uncertain or subjective expert knowledge.

#### Parameters/Dependencies
* **Input Variables:** Crisp numerical inputs (e.g., temperature, pressure).
* **Output Variables:** Crisp numerical outputs (e.g., fan speed, valve opening).
* **Fuzzy Rules:** The set of IF-THEN rules defining the system's logic.
* **Membership Function Definitions:** Parameters defining the fuzzy sets.
* **Compute Resources:** Lambda (serverless) or EC2 instance.

#### Tool Navigations (AWS Console)
* AWS Lambda -> Functions
* Amazon EC2 -> Instances
* AWS IoT Core -> Act -> Rules (for IoT integration)

#### CLI & Output Syntax Examples (Lambda Function for Fuzzy Logic)
\`\`\`python
# Example Python Lambda function (lambda_function.py) for a simple fuzzy controller
# This would require 'scikit-fuzzy' as a Lambda layer.

import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl

def lambda_handler(event, context):
    # Define fuzzy variables
    quality = ctrl.Antecedent(np.arange(0, 11, 1), 'quality')
    service = ctrl.Antecedent(np.arange(0, 11, 1), 'service')
    tip = ctrl.Consequent(np.arange(0, 26, 1), 'tip')

    # Define membership functions
    quality.automf(3) # Generates 'poor', 'mediocre', 'good'
    service.automf(3) # Generates 'poor', 'mediocre', 'good'

    tip['low'] = fuzz.trimf(tip.universe, [0, 0, 13])
    tip['medium'] = fuzz.trimf(tip.universe, [0, 13, 25])
    tip['high'] = fuzz.trimf(tip.universe, [13, 25, 25])

    # Define fuzzy rules
    rule1 = ctrl.Rule(quality['poor'] | service['poor'], tip['low'])
    rule2 = ctrl.Rule(service['mediocre'], tip['medium'])
    rule3 = ctrl.Rule(service['good'] | quality['good'], tip['high'])

    # Create control system and simulation
    tipping_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])
    tipping_simulation = ctrl.ControlSystemSimulation(tipping_ctrl)

    # Get input from event (e.g., API Gateway)
    input_quality = float(event.get('quality', 6.5))
    input_service = float(event.get('service', 9.8))

    tipping_simulation.input['quality'] = input_quality
    tipping_simulation.input['service'] = input_service

    # Compute the result
    tipping_simulation.compute()

    predicted_tip = tipping_simulation.output['tip']

    return {
        'statusCode': 200,
        'body': json.dumps({
            'predicted_tip': predicted_tip
        })
    }

# To deploy this Lambda function (after packaging skfuzzy in a layer):
# aws lambda create-function --function-name MyFuzzyLogicFunction --runtime python3.9 ...
# aws lambda invoke --function-name MyFuzzyLogicFunction --payload '{"quality": 6.5, "service": 9.8}' output.json
\`\`\`

#### Common Issues and Solutions
* **Issue: Difficulty defining accurate membership functions and rules.**
    * **Solution:** This often requires domain expertise. Start with simple, intuitive rules and refine them through testing. Use visualization tools to understand membership functions.
* **Issue: Performance for complex systems.**
    * **Solution:** Optimize the underlying code. For very large rule sets or complex defuzzification, consider using more powerful EC2 instances instead of Lambda.
* **Issue: Integration with other systems.**
    * **Solution:** Use AWS services like API Gateway for exposing fuzzy logic as an API, or IoT Core rules for integrating with IoT devices.`,
            },
            {
              "name": "Evolutionary Computation",
              "description": "A family of AI algorithms inspired by biological evolution (e.g., natural selection, mutation, crossover). They are used for optimization and search problems, evolving solutions over generations.",
              "children": [
                {
                  "name": "Genetic Algorithms",
                  "description": `Genetic Algorithms (GAs) are a widely used type of evolutionary algorithm that mimics the process of natural selection to find optimal or near-optimal solutions to optimization and search problems. They operate on a population of candidate solutions, often represented as "chromosomes" or "genotypes," and evolve them over successive "generations" through processes analogous to biological evolution: selection, crossover (recombination), and mutation.

### AWS Implementation for Genetic Algorithms
AWS provides scalable compute infrastructure to run Genetic Algorithms, which can be computationally intensive, especially for large populations or many generations:
* **Amazon EC2:** The most common and flexible way to run GAs. You can provision various instance types (CPU-optimized like C5/C6gn, or memory-optimized like R5/R6g) to execute your GA code written in Python (e.g., with libraries like DEAP, PyGAD, or your custom implementation).
* **AWS Batch:** Ideal for running large-scale GA experiments. You can submit jobs that evaluate individual solutions or entire generations, and Batch will manage the underlying EC2 instances, scaling up and down as needed. This is particularly useful when fitness evaluation is a time-consuming step.
* **Amazon SageMaker:** While primarily for traditional ML, you could potentially use SageMaker's custom training jobs to run GAs, especially if your GA is being used for hyperparameter optimization or neural architecture search for a machine learning model.
* **Amazon S3:** For storing input data, intermediate populations, fitness evaluation results, and the best-evolved solutions.
* **AWS Lambda/Step Functions:** For orchestrating the GA workflow, such as triggering the start of a new generation, aggregating results, or managing the population state, especially in a serverless context.

---

#### How to Enable/Config (EC2 Example for a Python GA)
1.  **AWS Console:** Navigate to Amazon EC2.
2.  **Launch Instance:** Choose an appropriate instance type (e.g., \`t3.medium\` for small tests, \`c5.large\` for more compute). Select a suitable Amazon Machine Image (AMI) (e.g., Amazon Linux 2, Ubuntu).
3.  **SSH into Instance:** Connect to your EC2 instance using SSH.
4.  **Install Python & Libraries:** Install Python and your chosen GA library (e.g., \`pip install deap\`).
5.  **Upload & Run Code:** Upload your Python GA script to the instance and execute it.

#### Key Features
* **Population-Based Search:** Explores multiple solutions simultaneously, reducing the chance of getting stuck in local optima.
* **Heuristic Optimization:** Does not require the objective function to be differentiable or continuous.
* **Parallelizable:** Many steps (e.g., fitness evaluation of individuals in a population) can be run in parallel.
* **Robustness:** Can handle noisy data and complex, high-dimensional search spaces.

#### Core Strategies & Operators
* **Representation:** How a candidate solution is encoded (e.g., binary strings, real-valued vectors, permutations).
* **Fitness Function:** Evaluates the "goodness" of a solution. This is the objective function you want to optimize.
* **Selection:** Chooses individuals from the current population to be parents for the next generation (e.g., roulette wheel, tournament selection).
* **Crossover (Recombination):** Combines genetic material from two parents to create new offspring (e.g., one-point, two-point, uniform crossover).
* **Mutation:** Randomly alters a small part of an individual's genetic material to introduce diversity (e.g., bit flip, Gaussian mutation).
* **Elitism:** Preserving the best individuals from one generation to the next without modification.

#### Parameters/Dependencies
* **Population Size:** The number of individuals in each generation.
* **Number of Generations:** The total iterations for the algorithm to run.
* **Crossover Rate:** Probability that two parents will undergo crossover.
* **Mutation Rate:** Probability that an individual's gene will be mutated.
* **Compute Resources:** CPU or GPU, depending on the complexity of fitness evaluation.

#### Tool Navigations (AWS Console)
* Amazon EC2 -> Instances
* AWS Batch -> Job definitions, Job queues, Jobs
* Amazon S3 -> Buckets

#### CLI & Output Syntax Examples (Conceptual Python GA on EC2)
\`\`\`python
# Example Python script (my_ga_script.py) for a simple GA using DEAP
# This would be run on an EC2 instance.

from deap import base, creator, tools, algorithms
import random
import json # Added for Lambda example, but useful here too

# 1. Define the fitness function (e.g., maximizing a simple sum)
def eval_func(individual):
    return sum(individual),

# 2. Setup DEAP structures
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
# Attribute generator: generate 0 or 1
toolbox.register("attr_bool", random.randint, 0, 1)
# Structure initializers: define individual and population
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=10)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# 3. Register genetic operators
toolbox.register("evaluate", eval_func)
toolbox.register("mate", tools.cxTwoPoint) # Two-point crossover
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05) # Flip bit mutation
toolbox.register("select", tools.selTournament, tournsize=3) # Tournament selection

def main():
    population = toolbox.population(n=50) # Initial population of 50 individuals
    
    # Run the genetic algorithm
    algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=40, stats=None, halloffame=None, verbose=True)
    
    best_ind = tools.selBest(population, 1)[0]
    print(f"Best individual is {best_ind}, with fitness {best_ind.fitness.values}")

if __name__ == "__main__":
    main()

# To run this on EC2 after SSHing:
# python3 my_ga_script.py
\`\`\`

#### Common Issues and Solutions
* **Issue: Premature convergence (getting stuck in a suboptimal solution).**
    * **Solution:** Increase mutation rate to introduce more diversity. Increase population size. Use different selection or crossover operators. Implement techniques like niching or speciation.
* **Issue: Slow execution time.**
    * **Solution:** Optimize the fitness function (it's often the bottleneck). Reduce population size or number of generations for initial tests. Use more powerful EC2 instances. Distribute the workload using AWS Batch or a custom distributed framework.
* **Issue: Difficulty in defining the "chromosome" or representation.**
    * **Solution:** Carefully consider how to encode your problem's solution into a format suitable for genetic operations. This is often problem-specific.
* **Issue: Choosing optimal genetic algorithm parameters (population size, rates).**
    * **Solution:** These are often found through experimentation and heuristics. Start with common values and tune them. You could even use another optimization algorithm (like SageMaker's HPO) to tune the GA's parameters!`,
                }
              ]
            },
            {"name": "Fuzzy Logic", "description": "A form of multi-valued logic that deals with reasoning that is approximate rather than fixed and exact. It allows for 'degrees of truth' (values between 0 and 1) rather than just true/false. Useful for controlling systems that involve imprecise or uncertain data."}
          ]
        };

        // D3.js variables for SVG, group element, zoom behavior, tree layout, and root node
        let svg, g, zoomListener, treeLayout, root;
        let width, height; // Dimensions of the chart area
        let i = 0, duration = 750; // 'i' for unique node IDs, 'duration' for animation speed
        let lastClickedNode = null; // To keep track of the last clicked node for breadcrumbs

        // Get references to the description panel elements
        const descriptionPanel = document.getElementById('description-panel');
        const descriptionTitle = document.getElementById('description-title');
        const descriptionContent = document.getElementById('description-content');

        // Get references to modal elements
        const fullContentModal = document.getElementById('full-content-modal');
        const modalTitle = document.getElementById('modal-title');
        const modalBody = document.getElementById('modal-body');
        const closeModalButton = document.getElementById('close-modal');

        // Get references to search elements
        const searchInput = document.getElementById('search-input');
        const searchButton = document.getElementById('search-button');

        // Get references to export elements
        const exportSvgButton = document.getElementById('export-svg-button');

        // Get reference to breadcrumbs container
        const breadcrumbsContainer = document.getElementById('breadcrumbs');

        // Get references to message box elements
        const messageBox = document.getElementById('message-box');
        const messageContent = document.getElementById('message-content');
        const messageOkButton = document.getElementById('message-ok-button');

        // Add a "Read More" button to the description panel
        const readMoreButton = document.createElement('button');
        readMoreButton.id = 'read-more-button';
        readMoreButton.className = 'mt-4 px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-opacity-50 hidden'; // Hidden by default
        readMoreButton.textContent = 'Read Full Description';
        descriptionPanel.appendChild(readMoreButton);

        // Add a "Copy Markdown" button to the description panel
        const copyMarkdownButton = document.createElement('button');
        copyMarkdownButton.id = 'copy-markdown-button';
        copyMarkdownButton.className = 'mt-2 px-4 py-2 bg-gray-500 text-white rounded-md hover:bg-gray-600 focus:outline-none focus:ring-2 focus:ring-gray-400 focus:ring-opacity-50 hidden'; // Hidden by default
        copyMarkdownButton.textContent = 'Copy Markdown';
        descriptionPanel.appendChild(copyMarkdownButton);


        // Map for node type icons (emojis)
        const nodeIcons = {
            "Artificial Intelligence (AI)": "🧠",
            "Machine Learning (ML)": "🤖",
            "Supervised Learning": "✅",
            "Unsupervised Learning": "🔍",
            "Semi-Supervised Learning": "🤝",
            "Reinforcement Learning (RL)": "🏆",
            "Deep Learning (DL)": "💡",
            "Generative AI": "✨",
            "Natural Language Processing (NLP)": "🗣️",
            "Computer Vision": "👁️",
            "Robotics": "⚙️",
            "Expert Systems": "👨‍🏫",
            "Planning and Scheduling": "🗓️",
            "Search and Optimization": "🔎",
            "Knowledge Representation and Reasoning": "📚",
            "Evolutionary Computation": "🌱",
            "Fuzzy Logic": "〰️",
            "MLOps": "🛠️",
            "Explainable AI (XAI)": "❓",
            "AI Security & Privacy": "🔒",
            "Experiment Tracking": "📊",
            "Foundation Models (FMs)": "🏗️",
            "Prompt Engineering": "✍️",
            "Federated Learning": "🌐",
            "Knowledge Graphs & Symbolic AI": "🔗"
        };


        /**
         * Initializes or re-initializes the D3.js tree chart.
         * This function is called on page load and window resize to ensure responsiveness.
         */
        function initializeChart() {
            const chartContainer = document.querySelector('.chart-container');
            width = chartContainer.clientWidth;
            height = chartContainer.clientHeight;

            // Clear any existing SVG content to prevent duplicates on resize
            d3.select("#tree-chart").selectAll("*").remove();

            // Select the SVG element and set its viewBox for responsiveness
            svg = d3.select("#tree-chart")
                .attr("viewBox", `0 0 ${width} ${height}`)
                .attr("preserveAspectRatio", "xMidYMid meet");

            // Append a group element to the SVG for all tree elements. This group will be zoomed/panned.
            g = svg.append("g");

            // Define the D3 zoom behavior
            zoomListener = d3.zoom()
                .scaleExtent([0.1, 3]) // Set zoom limits (min 10%, max 300%)
                .on("zoom", (event) => {
                    // Apply the zoom transform to the main group element
                    g.attr("transform", event.transform);
                });

            // Apply the zoom behavior to the SVG
            svg.call(zoomListener);

            // Dynamically calculate the tree layout width based on description panel visibility
            const descriptionPanelWidth = descriptionPanel.offsetWidth;
            const treeLayoutWidth = (window.innerWidth >= 1024) ? (width - descriptionPanelWidth - 50) : width; // Adjust for desktop layout

            // Create the D3 tree layout.
            treeLayout = d3.tree().size([height, treeLayoutWidth]);

            // Create a root node from the hierarchical data
            root = d3.hierarchy(treeData, d => d.children);
            root.x0 = height / 2; // Initial x-coordinate for the root (center vertically)
            root.y0 = 0; // Initial y-coordinate for the root (left edge)

            // Collapse all children of the root initially, except the first level.
            // This provides a cleaner initial view.
            if (root.children) {
                root.children.forEach(collapse);
            }

            // Perform the initial update to draw the tree
            update(root);

            // Center the root node initially with a smooth transition
            setTimeout(() => {
                const initialScale = 0.8; // Adjust initial zoom level
                const initialTranslateX = 100; // Adjust initial X position
                const initialTranslateY = height / 2; // Adjust initial Y position
                svg.transition()
                    .duration(750) // Animation duration
                    .call(zoomListener.transform, d3.zoomIdentity.translate(initialTranslateX, initialTranslateY).scale(initialScale));
            }, 100); // Small delay to ensure SVG is rendered

            // Initialize breadcrumbs for the root node
            updateBreadcrumbs(root);
        }

        /**
         * Updates the tree visualization based on the current state of the root node.
         * @param {object} source - The node that was clicked (used for transition origin).
         */
        function update(source) {
            // Compute the new tree layout
            const treeData = treeLayout(root);

            // Get lists of all nodes and links
            const nodes = treeData.descendants();
            const links = treeData.descendants().slice(1); // Exclude the root node for links

            // Normalize for fixed-depth spacing.
            // d.y corresponds to the horizontal position in a horizontal tree.
            nodes.forEach(d => { d.y = d.depth * 180; }); // Adjust node spacing based on depth

            // ****************** Nodes section ******************

            // Select all existing node groups
            const node = g.selectAll('g.node')
                .data(nodes, d => d.id || (d.id = ++i)); // Assign unique ID to each node

            // Enter any new nodes at the parent's previous position.
            const nodeEnter = node.enter().append('g')
                .attr('class', 'node')
                // Set initial position for new nodes to the source node's old position for animation
                .attr('transform', d => `translate(${source.y0},${source.x0})`)
                .on('click', click) // Attach click event listener for expand/collapse and description
                .on('mouseover', function(event, d) { // Mouseover for tooltip
                    const tooltip = d3.select("body").append("div")
                        .attr("class", "tooltip")
                        .html(`<strong>${d.data.name}</strong>`);
                    tooltip.style("left", (event.pageX + 10) + "px")
                           .style("top", (event.pageY - 20) + "px")
                           .style("opacity", 1);
                })
                .on('mouseout', function() { // Mouseout to remove tooltip
                    d3.selectAll(".tooltip").remove();
                });

            // Add Circle for the nodes
            nodeEnter.append('circle')
                .attr('r', 1e-6) // Start with radius 0 for animation
                .attr('class', d => {
                    if (d.data.name === "Artificial Intelligence (AI)") return 'root'; // Special class for root
                    return d._children ? 'collapsed' : ''; // Class for collapsed nodes
                });

            // Add labels for the nodes
            nodeEnter.append('text')
                .attr('dy', '.35em') // Vertical alignment
                .attr('x', d => d.children || d._children ? -13 : 13) // Position text relative to circle
                .attr('text-anchor', d => d.children || d._children ? 'end' : 'start') // Align text
                .text(d => {
                    const icon = nodeIcons[d.data.name] || ''; // Get icon if available
                    return `${icon} ${d.data.name}`; // Prepend icon to text
                })
                .clone(true).lower() // Clone text for an outline effect
                .attr("stroke-linejoin", "round")
                .attr("stroke-width", 3)
                .attr("stroke", "white");

            // UPDATE section: Merge entering nodes with existing nodes
            const nodeUpdate = nodeEnter.merge(node);

            // Remove any existing highlight before applying new one
            nodeUpdate.classed('highlight', false);

            // Transition nodes to their new position
            nodeUpdate.transition()
                .duration(duration)
                .attr('transform', d => `translate(${d.y},${d.x})`);

            // Update the node attributes and style (e.g., radius, class for collapsed state)
            nodeUpdate.select('circle')
                .attr('r', 10) // Set final radius for nodes
                .attr('class', d => {
                    if (d.data.name === "Artificial Intelligence (AI)") return 'root';
                    return d._children ? 'collapsed' : '';
                });

            // EXIT section: Remove any exiting nodes
            const nodeExit = node.exit().transition()
                .duration(duration)
                // Transition exiting nodes back to the source node's position before removing
                .attr('transform', d => `translate(${source.y},${source.x})`)
                .remove();

            // On exit, shrink node circles to 0
            nodeExit.select('circle')
                .attr('r', 1e-6);

            // On exit, fade out text labels
            nodeExit.select('text')
                .style('fill-opacity', 1e-6);

            // ****************** Links section ******************

            // Select all existing links
            const link = g.selectAll('path.link')
                .data(links, d => d.id); // Bind data to links

            // Enter any new links at the parent's previous position.
            const linkEnter = link.enter().insert('path', "g") // Insert before 'g' elements (nodes)
                .attr("class", "link")
                .attr('d', d => {
                    const o = {x: source.x0, y: source.y0}; // Start from source's old position
                    return diagonal(o, o); // Draw a zero-length path initially
                });

            // UPDATE section: Merge entering links with existing links
            const linkUpdate = linkEnter.merge(link);

            // Transition links to their new position
            linkUpdate.transition()
                .duration(duration)
                .attr('d', d => diagonal(d, d.parent)); // Draw path from node to its parent

            // Remove any exiting links
            link.exit().transition()
                .duration(duration)
                .attr('d', d => {
                    const o = {x: source.x, y: source.y}; // End at source's current position
                    return diagonal(o, o); // Shrink to zero-length path
                })
                .remove();

            // Store the current positions for the next transition.
            nodes.forEach(d => {
                d.x0 = d.x;
                d.y0 = d.y;
            });
        }

        /**
         * Generates a curved (diagonal) path string for a link.
         * @param {object} s - Source node data (child).
         * @param {object} d - Destination node data (parent).
         * @returns {string} SVG path string.
         */
        function diagonal(s, d) {
            return `M ${s.y} ${s.x}
                    C ${(s.y + d.y) / 2} ${s.x},
                      ${(s.y + d.y) / 2} ${d.x},
                      ${d.y} ${d.x}`;
        }

        /**
         * Toggles children of a node on click (expand/collapse) and displays its description.
         * @param {Event} event - The click event.
         * @param {object} d - The data of the clicked node.
         */
        function click(event, d) {
            if (d.children) { // If node has visible children, collapse them
                d._children = d.children;
                d.children = null;
            } else { // If node has hidden children, expand them
                d.children = d._children;
                d._children = null;
            }
            update(d); // Re-render the tree
            displayDescription(d.data.name, d.data.description); // Update description panel
            updateBreadcrumbs(d); // Update breadcrumbs on node click
            lastClickedNode = d; // Store the last clicked node
        }

        /**
         * Recursively collapses a node and all its descendants.
         * @param {object} d - The node to collapse.
         */
        function collapse(d) {
            if (d.children) {
                d._children = d.children; // Store children in _children property
                d._children.forEach(collapse); // Recursively collapse descendants
                d.children = null; // Hide children
            }
        }

        /**
         * Function to show the modal
         * @param {string} title - The title for the modal.
         * @param {string} content - The full content to display in the modal.
         */
        function showModal(title, content) {
            modalTitle.textContent = title;
            modalBody.innerHTML = marked.parse(content); // Parse Markdown for modal content
            fullContentModal.classList.remove('hidden');
            // Prevent body scrolling when modal is open
            document.body.style.overflow = 'hidden';
        }

        /**
         * Function to hide the modal
         */
        function hideModal() {
            fullContentModal.classList.add('hidden');
            // Restore body scrolling
            document.body.style.overflow = '';
        }

        /**
         * Displays a custom message box.
         * @param {string} message - The message to display.
         */
        function showMessageBox(message) {
            messageContent.textContent = message;
            messageBox.classList.remove('hidden');
            document.body.style.overflow = 'hidden'; // Prevent scrolling behind message box
        }

        /**
         * Hides the custom message box.
         */
        function hideMessageBox() {
            messageBox.classList.add('hidden');
            document.body.style.overflow = ''; // Restore scrolling
        }

        // Event listener for the "Read More" button
        readMoreButton.addEventListener('click', () => {
            // Retrieve the full content from the data attribute on descriptionContent
            const currentTitle = descriptionTitle.textContent;
            const currentFullContent = descriptionContent.dataset.fullContent;
            if (currentFullContent) {
                showModal(currentTitle, currentFullContent);
            }
        });

        // Event listener for the "Copy Markdown" button
        copyMarkdownButton.addEventListener('click', () => {
            const currentFullContent = descriptionContent.dataset.fullContent;
            if (currentFullContent) {
                // Create a temporary textarea element
                const tempTextArea = document.createElement('textarea');
                tempTextArea.value = currentFullContent;
                document.body.appendChild(tempTextArea);

                // Select the text and copy it to the clipboard
                tempTextArea.select();
                document.execCommand('copy');

                // Remove the temporary textarea
                document.body.removeChild(tempTextArea);

                copyMarkdownButton.textContent = 'Copied!';
                setTimeout(() => {
                    copyMarkdownButton.textContent = 'Copy Markdown';
                }, 2000);
            }
        });

        // Event listener for the close modal button
        closeModalButton.addEventListener('click', hideModal);

        // Event listener for the message box OK button
        messageOkButton.addEventListener('click', hideMessageBox);

        /**
         * Displays the name and description of the selected node in the panel.
         * @param {string} title - The name of the node.
         * @param {string} content - The detailed description of the node.
         */
        function displayDescription(title, content) {
            descriptionTitle.textContent = title;
            descriptionContent.dataset.fullContent = content; // Store the full content in a data attribute

            const maxDisplayLength = 500; // Character limit for direct display in the panel
            let displayedContent = content;
            if (content.length > maxDisplayLength) {
                // Find a natural break point (e.g., end of a sentence or paragraph)
                const truncated = content.substring(0, maxDisplayLength);
                const lastPeriod = truncated.lastIndexOf('.');
                const lastExclamation = truncated.lastIndexOf('!');
                const lastQuestion = truncated.lastIndexOf('?');
                const lastNewline = truncated.lastIndexOf('\n');

                let cutoff = maxDisplayLength;
                if (lastPeriod > -1) cutoff = lastPeriod + 1;
                else if (lastExclamation > -1) cutoff = lastExclamation + 1;
                else if (lastQuestion > -1) cutoff = lastQuestion + 1;
                else if (lastNewline > -1) cutoff = lastNewline + 1;

                displayedContent = content.substring(0, cutoff) + '\n\n...'; // Add ellipsis and newlines for better readability
                readMoreButton.classList.remove('hidden'); // Show the "Read More" button
                copyMarkdownButton.classList.remove('hidden'); // Show the "Copy Markdown" button
            } else {
                readMoreButton.classList.add('hidden'); // Hide the button if content is short
                copyMarkdownButton.classList.add('hidden'); // Hide the button if content is short
            }

            // Use innerHTML to render Markdown content
            descriptionContent.innerHTML = marked.parse(displayedContent);
            descriptionPanel.classList.add('active'); // Show the description panel
        }

        /**
         * Updates the breadcrumb navigation based on the currently selected node.
         * @param {object} node - The D3 node object that is currently selected.
         */
        function updateBreadcrumbs(node) {
            breadcrumbsContainer.innerHTML = ''; // Clear existing breadcrumbs
            const ancestors = node.ancestors().reverse(); // Get path from root to current node

            ancestors.forEach((d, index) => {
                const span = document.createElement('span');
                if (index < ancestors.length - 1) {
                    // Not the current node, make it a clickable link
                    const link = document.createElement('a');
                    link.href = '#'; // Prevent default link behavior
                    link.textContent = d.data.name;
                    link.addEventListener('click', (event) => {
                        event.preventDefault();
                        // Collapse all children of the root first to ensure clean expansion
                        root.children.forEach(collapse);
                        // Expand the path to the clicked ancestor
                        d.ancestors().reverse().forEach(ancestor => {
                            if (ancestor._children) {
                                ancestor.children = ancestor._children;
                                ancestor._children = null;
                            }
                        });
                        update(d); // Update the chart to show the path
                        displayDescription(d.data.name, d.data.description); // Update description
                        updateBreadcrumbs(d); // Update breadcrumbs
                        lastClickedNode = d; // Set last clicked node for centering
                        centerNode(d); // Center the view on the clicked node
                    });
                    span.appendChild(link);
                } else {
                    // Current node, just display its name
                    span.textContent = d.data.name;
                    span.classList.add('current-node');
                }
                breadcrumbsContainer.appendChild(span);
                if (index < ancestors.length - 1) {
                    const separator = document.createElement('span');
                    separator.textContent = ' > ';
                    breadcrumbsContainer.appendChild(separator);
                }
            });
        }

        /**
         * Centers the view on a specific node.
         * @param {object} source - The node to center on.
         */
        function centerNode(source) {
            const scale = zoomListener.transform().k;
            const x = -source.y * scale + width / 4; // Adjust x to leave space for description panel
            const y = -source.x * scale + height / 2;
            svg.transition()
                .duration(duration)
                .call(zoomListener.transform, d3.zoomIdentity.translate(x, y).scale(scale));
        }

        /**
         * Searches for a node by name, expands its path, and centers the view.
         * @param {string} query - The name of the node to search for.
         */
        function searchNode(query) {
            const lowerCaseQuery = query.toLowerCase();
            let foundNode = null;

            // Find the node
            root.each(d => {
                if (d.data.name.toLowerCase().includes(lowerCaseQuery)) {
                    foundNode = d;
                    return true; // Break loop
                }
            });

            if (foundNode) {
                // Collapse all nodes first
                root.children.forEach(collapse);

                // Expand ancestors of the found node
                foundNode.ancestors().reverse().forEach(d => {
                    if (d._children) {
                        d.children = d._children;
                        d._children = null;
                    }
                });

                // Remove highlight from previously highlighted nodes
                g.selectAll('g.node').classed('highlight', false);

                // Update the tree and highlight the found node
                update(foundNode);
                g.selectAll('g.node').filter(d => d.id === foundNode.id).classed('highlight', true);


                // Display description and update breadcrumbs
                displayDescription(foundNode.data.name, foundNode.data.description);
                updateBreadcrumbs(foundNode);
                lastClickedNode = foundNode;

                // Center the view on the found node
                centerNode(foundNode);
            } else {
                showMessageBox('Node not found!');
            }
        }

        // Event listener for search button click
        searchButton.addEventListener('click', () => {
            searchNode(searchInput.value);
        });

        // Event listener for Enter key in search input
        searchInput.addEventListener('keypress', (event) => {
            if (event.key === 'Enter') {
                searchNode(searchInput.value);
            }
        });

        /**
         * Exports the current SVG chart as an SVG file.
         */
        function exportSvg() {
            const svgElement = document.getElementById('tree-chart');
            const svgString = new XMLSerializer().serializeToString(svgElement);
            const blob = new Blob([svgString], { type: 'image/svg+xml;charset=utf-8' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'aws-ai-ml-hierarchy.svg';
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }

        // Event listener for export SVG button
        exportSvgButton.addEventListener('click', exportSvg);


        // Initialize the chart when the window loads
        window.onload = initializeChart;
        // Re-initialize the chart when the window is resized to ensure responsiveness
        window.addEventListener('resize', initializeChart);
    </script>
    <!-- Marked.js for Markdown parsing in the description panel and modal -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</body>
</html>
